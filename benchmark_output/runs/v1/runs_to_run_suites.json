{
  "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=anatomy,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=anatomy,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=anatomy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=anatomy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=anatomy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=astronomy,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=astronomy,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=astronomy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=astronomy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=astronomy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=business_ethics,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=business_ethics,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=business_ethics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=business_ethics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=business_ethics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=college_biology,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=college_biology,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=college_biology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=college_biology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=college_biology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=college_medicine,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=college_medicine,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=college_medicine,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=college_medicine,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=college_medicine,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=college_physics,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=college_physics,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=college_physics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=college_physics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=college_physics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=computer_security,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=computer_security,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=computer_security,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=computer_security,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=computer_security,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=econometrics,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=econometrics,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=econometrics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=econometrics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=econometrics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=formal_logic,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=formal_logic,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=formal_logic,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=formal_logic,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=formal_logic,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=global_facts,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=global_facts,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=global_facts,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=global_facts,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=global_facts,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=human_aging,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=human_aging,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=human_aging,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=human_aging,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=human_aging,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=international_law,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=international_law,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=international_law,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=international_law,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=international_law,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=machine_learning,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=machine_learning,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=machine_learning,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=machine_learning,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=machine_learning,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=management,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=management,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=management,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=management,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=management,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=marketing,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=marketing,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=marketing,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=marketing,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=marketing,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=nutrition,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=nutrition,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=nutrition,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=nutrition,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=nutrition,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=philosophy,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=philosophy,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=philosophy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=philosophy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=philosophy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=prehistory,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=prehistory,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=prehistory,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=prehistory,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=prehistory,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=professional_law,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=professional_law,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=professional_law,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=professional_law,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=professional_law,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=public_relations,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=public_relations,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=public_relations,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=public_relations,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=public_relations,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=security_studies,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=security_studies,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=security_studies,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=security_studies,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=security_studies,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=sociology,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=sociology,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=sociology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=sociology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=sociology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=virology,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=virology,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=virology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=virology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=virology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1",
  "mmlu:subject=world_religions,method=multiple_choice_joint,model=groq_llama3-70b-8192": "v1",
  "mmlu:subject=world_religions,method=multiple_choice_joint,model=groq_llama3-8b-8192": "v1",
  "mmlu:subject=world_religions,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B": "v1",
  "mmlu:subject=world_religions,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf": "v1",
  "mmlu:subject=world_religions,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf": "v1"
}