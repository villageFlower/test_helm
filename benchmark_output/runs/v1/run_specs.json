[
  {
    "name": "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "abstract_algebra"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about abstract algebra.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "abstract_algebra"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about abstract algebra.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "abstract_algebra"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about abstract algebra.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "abstract_algebra"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about abstract algebra.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "abstract_algebra"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about abstract algebra.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=anatomy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "anatomy"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about anatomy.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=anatomy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "anatomy"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about anatomy.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=anatomy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "anatomy"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about anatomy.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=anatomy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "anatomy"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about anatomy.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=anatomy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "anatomy"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about anatomy.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=astronomy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "astronomy"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about astronomy.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=astronomy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "astronomy"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about astronomy.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=astronomy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "astronomy"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about astronomy.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=astronomy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "astronomy"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about astronomy.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=astronomy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "astronomy"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about astronomy.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=business_ethics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "business_ethics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about business ethics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=business_ethics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "business_ethics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about business ethics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=business_ethics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "business_ethics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about business ethics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=business_ethics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "business_ethics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about business ethics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=business_ethics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "business_ethics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about business ethics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "clinical_knowledge"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about clinical knowledge.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "clinical_knowledge"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about clinical knowledge.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "clinical_knowledge"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about clinical knowledge.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "clinical_knowledge"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about clinical knowledge.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "clinical_knowledge"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about clinical knowledge.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=college_biology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "college_biology"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about college biology.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=college_biology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "college_biology"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about college biology.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=college_biology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "college_biology"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about college biology.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=college_biology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "college_biology"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about college biology.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=college_biology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "college_biology"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about college biology.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "college_chemistry"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about college chemistry.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "college_chemistry"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about college chemistry.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "college_chemistry"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about college chemistry.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "college_chemistry"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about college chemistry.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "college_chemistry"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about college chemistry.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "college_computer_science"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about college computer science.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "college_computer_science"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about college computer science.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "college_computer_science"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about college computer science.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "college_computer_science"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about college computer science.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "college_computer_science"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about college computer science.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "college_mathematics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about college mathematics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "college_mathematics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about college mathematics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "college_mathematics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about college mathematics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "college_mathematics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about college mathematics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "college_mathematics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about college mathematics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=college_medicine,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "college_medicine"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about college medicine.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=college_medicine,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "college_medicine"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about college medicine.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=college_medicine,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "college_medicine"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about college medicine.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=college_medicine,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "college_medicine"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about college medicine.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=college_medicine,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "college_medicine"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about college medicine.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=college_physics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "college_physics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about college physics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=college_physics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "college_physics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about college physics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=college_physics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "college_physics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about college physics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=college_physics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "college_physics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about college physics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=college_physics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "college_physics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about college physics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=computer_security,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "computer_security"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about computer security.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=computer_security,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "computer_security"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about computer security.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=computer_security,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "computer_security"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about computer security.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=computer_security,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "computer_security"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about computer security.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=computer_security,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "computer_security"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about computer security.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "conceptual_physics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about conceptual physics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "conceptual_physics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about conceptual physics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "conceptual_physics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about conceptual physics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "conceptual_physics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about conceptual physics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "conceptual_physics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about conceptual physics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=econometrics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "econometrics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about econometrics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=econometrics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "econometrics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about econometrics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=econometrics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "econometrics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about econometrics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=econometrics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "econometrics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about econometrics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=econometrics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "econometrics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about econometrics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "electrical_engineering"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about electrical engineering.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "electrical_engineering"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about electrical engineering.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "electrical_engineering"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about electrical engineering.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "electrical_engineering"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about electrical engineering.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "electrical_engineering"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about electrical engineering.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "elementary_mathematics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about elementary mathematics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "elementary_mathematics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about elementary mathematics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "elementary_mathematics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about elementary mathematics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "elementary_mathematics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about elementary mathematics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "elementary_mathematics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about elementary mathematics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=formal_logic,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "formal_logic"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about formal logic.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=formal_logic,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "formal_logic"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about formal logic.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=formal_logic,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "formal_logic"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about formal logic.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=formal_logic,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "formal_logic"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about formal logic.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=formal_logic,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "formal_logic"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about formal logic.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=global_facts,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "global_facts"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about global facts.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=global_facts,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "global_facts"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about global facts.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=global_facts,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "global_facts"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about global facts.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=global_facts,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "global_facts"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about global facts.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=global_facts,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "global_facts"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about global facts.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_biology"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school biology.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_biology"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school biology.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_biology"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school biology.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_biology"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school biology.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_biology"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school biology.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_chemistry"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school chemistry.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_chemistry"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school chemistry.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_chemistry"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school chemistry.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_chemistry"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school chemistry.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_chemistry"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school chemistry.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_computer_science"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school computer science.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_computer_science"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school computer science.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_computer_science"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school computer science.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_computer_science"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school computer science.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_computer_science"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school computer science.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_european_history"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school european history.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_european_history"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school european history.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_european_history"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school european history.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_european_history"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school european history.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_geography"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school geography.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_geography"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school geography.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_geography"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school geography.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_geography"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school geography.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_geography"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school geography.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_government_and_politics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school government and politics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_government_and_politics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school government and politics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_government_and_politics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school government and politics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_government_and_politics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school government and politics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_government_and_politics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school government and politics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_macroeconomics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school macroeconomics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_macroeconomics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school macroeconomics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_macroeconomics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school macroeconomics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_macroeconomics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school macroeconomics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_macroeconomics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school macroeconomics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_mathematics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school mathematics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_mathematics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school mathematics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_mathematics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school mathematics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_mathematics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school mathematics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_mathematics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school mathematics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_microeconomics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school microeconomics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_microeconomics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school microeconomics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_microeconomics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school microeconomics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_microeconomics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school microeconomics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_microeconomics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school microeconomics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_physics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school physics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_physics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school physics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_physics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school physics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_physics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school physics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_physics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school physics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_psychology"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school psychology.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_psychology"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school psychology.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_psychology"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school psychology.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_psychology"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school psychology.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_psychology"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school psychology.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_statistics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school statistics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_statistics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school statistics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_statistics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school statistics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_statistics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school statistics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_statistics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school statistics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_us_history"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school us history.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_us_history"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school us history.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_us_history"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school us history.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_us_history"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school us history.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_us_history"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school us history.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_world_history"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school world history.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_world_history"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school world history.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_world_history"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school world history.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "high_school_world_history"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about high school world history.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=human_aging,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "human_aging"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about human aging.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=human_aging,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "human_aging"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about human aging.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=human_aging,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "human_aging"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about human aging.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=human_aging,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "human_aging"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about human aging.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=human_aging,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "human_aging"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about human aging.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "human_sexuality"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about human sexuality.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "human_sexuality"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about human sexuality.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "human_sexuality"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about human sexuality.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "human_sexuality"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about human sexuality.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "human_sexuality"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about human sexuality.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=international_law,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "international_law"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about international law.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=international_law,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "international_law"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about international law.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=international_law,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "international_law"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about international law.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=international_law,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "international_law"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about international law.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=international_law,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "international_law"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about international law.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "jurisprudence"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about jurisprudence.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "jurisprudence"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about jurisprudence.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "jurisprudence"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about jurisprudence.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "jurisprudence"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about jurisprudence.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "jurisprudence"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about jurisprudence.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "logical_fallacies"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about logical fallacies.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "logical_fallacies"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about logical fallacies.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "logical_fallacies"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about logical fallacies.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "logical_fallacies"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about logical fallacies.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "logical_fallacies"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about logical fallacies.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=machine_learning,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "machine_learning"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about machine learning.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=machine_learning,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "machine_learning"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about machine learning.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=machine_learning,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "machine_learning"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about machine learning.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=machine_learning,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "machine_learning"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about machine learning.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=machine_learning,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "machine_learning"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about machine learning.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=management,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "management"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about management.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=management,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "management"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about management.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=management,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "management"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about management.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=management,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "management"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about management.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=management,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "management"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about management.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=marketing,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "marketing"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about marketing.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=marketing,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "marketing"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about marketing.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=marketing,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "marketing"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about marketing.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=marketing,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "marketing"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about marketing.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=marketing,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "marketing"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about marketing.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "medical_genetics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about medical genetics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "medical_genetics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about medical genetics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "medical_genetics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about medical genetics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "medical_genetics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about medical genetics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "medical_genetics"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about medical genetics.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "miscellaneous"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about miscellaneous.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "miscellaneous"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about miscellaneous.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "miscellaneous"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about miscellaneous.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "miscellaneous"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about miscellaneous.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "miscellaneous"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about miscellaneous.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "moral_disputes"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about moral disputes.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "moral_disputes"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about moral disputes.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "moral_disputes"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about moral disputes.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "moral_disputes"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about moral disputes.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "moral_disputes"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about moral disputes.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "moral_scenarios"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about moral scenarios.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "moral_scenarios"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about moral scenarios.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "moral_scenarios"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about moral scenarios.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "moral_scenarios"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about moral scenarios.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "moral_scenarios"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about moral scenarios.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=nutrition,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "nutrition"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about nutrition.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=nutrition,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "nutrition"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about nutrition.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=nutrition,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "nutrition"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about nutrition.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=nutrition,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "nutrition"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about nutrition.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=nutrition,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "nutrition"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about nutrition.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=philosophy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "philosophy"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about philosophy.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=philosophy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "philosophy"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about philosophy.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=philosophy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "philosophy"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about philosophy.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=philosophy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "philosophy"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about philosophy.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=philosophy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "philosophy"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about philosophy.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=prehistory,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "prehistory"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about prehistory.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=prehistory,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "prehistory"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about prehistory.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=prehistory,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "prehistory"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about prehistory.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=prehistory,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "prehistory"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about prehistory.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=prehistory,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "prehistory"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about prehistory.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "professional_accounting"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about professional accounting.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "professional_accounting"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about professional accounting.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "professional_accounting"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about professional accounting.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "professional_accounting"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about professional accounting.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "professional_accounting"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about professional accounting.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=professional_law,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "professional_law"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about professional law.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=professional_law,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "professional_law"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about professional law.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=professional_law,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "professional_law"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about professional law.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=professional_law,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "professional_law"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about professional law.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=professional_law,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "professional_law"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about professional law.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "professional_medicine"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about professional medicine.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "professional_medicine"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about professional medicine.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "professional_medicine"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about professional medicine.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "professional_medicine"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about professional medicine.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "professional_medicine"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about professional medicine.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "professional_psychology"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about professional psychology.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "professional_psychology"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about professional psychology.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "professional_psychology"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about professional psychology.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "professional_psychology"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about professional psychology.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "professional_psychology"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about professional psychology.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=public_relations,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "public_relations"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about public relations.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=public_relations,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "public_relations"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about public relations.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=public_relations,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "public_relations"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about public relations.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=public_relations,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "public_relations"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about public relations.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=public_relations,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "public_relations"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about public relations.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=security_studies,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "security_studies"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about security studies.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=security_studies,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "security_studies"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about security studies.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=security_studies,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "security_studies"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about security studies.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=security_studies,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "security_studies"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about security studies.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=security_studies,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "security_studies"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about security studies.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=sociology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "sociology"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about sociology.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=sociology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "sociology"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about sociology.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=sociology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "sociology"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about sociology.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=sociology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "sociology"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about sociology.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=sociology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "sociology"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about sociology.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "us_foreign_policy"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about us foreign policy.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "us_foreign_policy"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about us foreign policy.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "us_foreign_policy"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about us foreign policy.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "us_foreign_policy"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about us foreign policy.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "us_foreign_policy"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about us foreign policy.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=virology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "virology"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about virology.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=virology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "virology"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about virology.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=virology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "virology"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about virology.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=virology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "virology"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about virology.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=virology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "virology"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about virology.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=world_religions,method=multiple_choice_joint,model=groq_llama3-70b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "world_religions"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about world religions.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-70b-8192",
      "model": "groq/llama3-70b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=world_religions,method=multiple_choice_joint,model=groq_llama3-8b-8192",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "world_religions"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about world religions.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "groq/llama3-8b-8192",
      "model": "groq/llama3-8b-8192",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=world_religions,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "world_religions"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about world religions.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "huggingface/Meta-Llama-3-8B",
      "model": "huggingface/Meta-Llama-3-8B",
      "temperature": 0.0,
      "max_tokens": 1,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=world_religions,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "world_religions"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about world religions.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-70b-chat-hf",
      "model": "together/Llama-3-70b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  },
  {
    "name": "mmlu:subject=world_religions,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
    "scenario_spec": {
      "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
      "args": {
        "subject": "world_religions"
      }
    },
    "adapter_spec": {
      "method": "multiple_choice_joint",
      "global_prefix": "",
      "global_suffix": "",
      "instructions": "The following are multiple choice questions (with answers) about world religions.\n",
      "input_prefix": "Question: ",
      "input_suffix": "\n",
      "reference_prefix": "A. ",
      "reference_suffix": "\n",
      "output_prefix": "Answer: ",
      "output_suffix": "\n",
      "instance_prefix": "\n",
      "substitutions": [],
      "max_train_instances": 5,
      "max_eval_instances": 20,
      "num_outputs": 5,
      "num_train_trials": 1,
      "num_trials": 1,
      "sample_train": true,
      "model_deployment": "together/Llama-3-8b-chat-hf",
      "model": "together/Llama-3-8b-chat-hf",
      "temperature": 0.0,
      "max_tokens": 20,
      "stop_sequences": [
        "\n"
      ],
      "multi_label": false
    },
    "metric_specs": [
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
        "args": {
          "names": [
            "exact_match",
            "quasi_exact_match",
            "prefix_exact_match",
            "quasi_prefix_exact_match"
          ]
        }
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
        "args": {}
      },
      {
        "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
        "args": {}
      }
    ],
    "data_augmenter_spec": {
      "perturbation_specs": [],
      "should_augment_train_instances": false,
      "should_include_original_train": false,
      "should_skip_unchanged_train": false,
      "should_augment_eval_instances": false,
      "should_include_original_eval": false,
      "should_skip_unchanged_eval": false,
      "seeds_per_instance": 1
    },
    "groups": [
      "mmlu"
    ]
  }
]