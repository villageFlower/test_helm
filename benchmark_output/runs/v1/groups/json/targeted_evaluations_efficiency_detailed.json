{
  "title": "Efficiency (Detailed)",
  "header": [
    {
      "value": "Model/adapter",
      "markdown": false,
      "metadata": {}
    },
    {
      "value": "Mean win rate",
      "description": "How many models this model outperform on average (over columns).",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {}
    },
    {
      "value": "The Pile - Observed inference time (s)",
      "description": "The Pile corpus for measuring lanugage model performance across various domains [(Gao et al., 2020)](https://arxiv.org/pdf/2101.00027.pdf).\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Observed inference time (s)",
        "run_group": "The Pile"
      }
    },
    {
      "value": "The Pile - Idealized inference time (s)",
      "description": "The Pile corpus for measuring lanugage model performance across various domains [(Gao et al., 2020)](https://arxiv.org/pdf/2101.00027.pdf).\n\nIdealized inference runtime (s): Average time to process a request to the model based solely on the model architecture (using Megatron-LM).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Idealized inference time (s)",
        "run_group": "The Pile"
      }
    },
    {
      "value": "The Pile - Denoised inference time (s)",
      "description": "The Pile corpus for measuring lanugage model performance across various domains [(Gao et al., 2020)](https://arxiv.org/pdf/2101.00027.pdf).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Denoised inference time (s)",
        "run_group": "The Pile"
      }
    },
    {
      "value": "The Pile - Training emissions (kg CO2)",
      "description": "The Pile corpus for measuring lanugage model performance across various domains [(Gao et al., 2020)](https://arxiv.org/pdf/2101.00027.pdf).\n\nEstimated training emissions (kg CO2): Estimate of the CO2 emissions from training the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training emissions (kg CO2)",
        "run_group": "The Pile"
      }
    },
    {
      "value": "The Pile - Training energy (MWh)",
      "description": "The Pile corpus for measuring lanugage model performance across various domains [(Gao et al., 2020)](https://arxiv.org/pdf/2101.00027.pdf).\n\nEstimated training energy cost (MWh): Estimate of the amount of energy used to train the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training energy (MWh)",
        "run_group": "The Pile"
      }
    },
    {
      "value": "TwitterAAE - Observed inference time (s)",
      "description": "The TwitterAAE corpus of [Blodgett et al. (2016)](https://aclanthology.org/D16-1120/) for measuring language model performance in tweets as a function of speaker dialect.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Observed inference time (s)",
        "run_group": "TwitterAAE"
      }
    },
    {
      "value": "TwitterAAE - Idealized inference time (s)",
      "description": "The TwitterAAE corpus of [Blodgett et al. (2016)](https://aclanthology.org/D16-1120/) for measuring language model performance in tweets as a function of speaker dialect.\n\nIdealized inference runtime (s): Average time to process a request to the model based solely on the model architecture (using Megatron-LM).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Idealized inference time (s)",
        "run_group": "TwitterAAE"
      }
    },
    {
      "value": "TwitterAAE - Denoised inference time (s)",
      "description": "The TwitterAAE corpus of [Blodgett et al. (2016)](https://aclanthology.org/D16-1120/) for measuring language model performance in tweets as a function of speaker dialect.\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Denoised inference time (s)",
        "run_group": "TwitterAAE"
      }
    },
    {
      "value": "TwitterAAE - Training emissions (kg CO2)",
      "description": "The TwitterAAE corpus of [Blodgett et al. (2016)](https://aclanthology.org/D16-1120/) for measuring language model performance in tweets as a function of speaker dialect.\n\nEstimated training emissions (kg CO2): Estimate of the CO2 emissions from training the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training emissions (kg CO2)",
        "run_group": "TwitterAAE"
      }
    },
    {
      "value": "TwitterAAE - Training energy (MWh)",
      "description": "The TwitterAAE corpus of [Blodgett et al. (2016)](https://aclanthology.org/D16-1120/) for measuring language model performance in tweets as a function of speaker dialect.\n\nEstimated training energy cost (MWh): Estimate of the amount of energy used to train the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training energy (MWh)",
        "run_group": "TwitterAAE"
      }
    },
    {
      "value": "ICE - Observed inference time (s)",
      "description": "The International Corpus of English (ICE) drawn from English speakers from various places in the world, initiated by [Greenbaum (1991)](https://www.cambridge.org/core/journals/english-today/article/abs/ice-the-international-corpus-of-english/47808205394C538393C3FD8E62E5E701).\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Observed inference time (s)",
        "run_group": "ICE"
      }
    },
    {
      "value": "ICE - Idealized inference time (s)",
      "description": "The International Corpus of English (ICE) drawn from English speakers from various places in the world, initiated by [Greenbaum (1991)](https://www.cambridge.org/core/journals/english-today/article/abs/ice-the-international-corpus-of-english/47808205394C538393C3FD8E62E5E701).\n\nIdealized inference runtime (s): Average time to process a request to the model based solely on the model architecture (using Megatron-LM).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Idealized inference time (s)",
        "run_group": "ICE"
      }
    },
    {
      "value": "ICE - Denoised inference time (s)",
      "description": "The International Corpus of English (ICE) drawn from English speakers from various places in the world, initiated by [Greenbaum (1991)](https://www.cambridge.org/core/journals/english-today/article/abs/ice-the-international-corpus-of-english/47808205394C538393C3FD8E62E5E701).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Denoised inference time (s)",
        "run_group": "ICE"
      }
    },
    {
      "value": "ICE - Training emissions (kg CO2)",
      "description": "The International Corpus of English (ICE) drawn from English speakers from various places in the world, initiated by [Greenbaum (1991)](https://www.cambridge.org/core/journals/english-today/article/abs/ice-the-international-corpus-of-english/47808205394C538393C3FD8E62E5E701).\n\nEstimated training emissions (kg CO2): Estimate of the CO2 emissions from training the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training emissions (kg CO2)",
        "run_group": "ICE"
      }
    },
    {
      "value": "ICE - Training energy (MWh)",
      "description": "The International Corpus of English (ICE) drawn from English speakers from various places in the world, initiated by [Greenbaum (1991)](https://www.cambridge.org/core/journals/english-today/article/abs/ice-the-international-corpus-of-english/47808205394C538393C3FD8E62E5E701).\n\nEstimated training energy cost (MWh): Estimate of the amount of energy used to train the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training energy (MWh)",
        "run_group": "ICE"
      }
    },
    {
      "value": "BLiMP - Observed inference time (s)",
      "description": "The Benchmark of Linguistic Minimal Pairs for English (BLiMP) for measuring performance on linguistic phenomena using minimal pair design [(Warstadt et al., 2020)](https://aclanthology.org/2020.tacl-1.25/).\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Observed inference time (s)",
        "run_group": "BLiMP"
      }
    },
    {
      "value": "BLiMP - Idealized inference time (s)",
      "description": "The Benchmark of Linguistic Minimal Pairs for English (BLiMP) for measuring performance on linguistic phenomena using minimal pair design [(Warstadt et al., 2020)](https://aclanthology.org/2020.tacl-1.25/).\n\nIdealized inference runtime (s): Average time to process a request to the model based solely on the model architecture (using Megatron-LM).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Idealized inference time (s)",
        "run_group": "BLiMP"
      }
    },
    {
      "value": "BLiMP - Denoised inference time (s)",
      "description": "The Benchmark of Linguistic Minimal Pairs for English (BLiMP) for measuring performance on linguistic phenomena using minimal pair design [(Warstadt et al., 2020)](https://aclanthology.org/2020.tacl-1.25/).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Denoised inference time (s)",
        "run_group": "BLiMP"
      }
    },
    {
      "value": "BLiMP - Training emissions (kg CO2)",
      "description": "The Benchmark of Linguistic Minimal Pairs for English (BLiMP) for measuring performance on linguistic phenomena using minimal pair design [(Warstadt et al., 2020)](https://aclanthology.org/2020.tacl-1.25/).\n\nEstimated training emissions (kg CO2): Estimate of the CO2 emissions from training the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training emissions (kg CO2)",
        "run_group": "BLiMP"
      }
    },
    {
      "value": "BLiMP - Training energy (MWh)",
      "description": "The Benchmark of Linguistic Minimal Pairs for English (BLiMP) for measuring performance on linguistic phenomena using minimal pair design [(Warstadt et al., 2020)](https://aclanthology.org/2020.tacl-1.25/).\n\nEstimated training energy cost (MWh): Estimate of the amount of energy used to train the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training energy (MWh)",
        "run_group": "BLiMP"
      }
    },
    {
      "value": "NaturalQuestions (closed-book) - Observed inference time (s)",
      "description": "The NaturalQuestions [(Kwiatkowski et al., 2019)](https://aclanthology.org/Q19-1026/) benchmark for question answering based on naturally-occurring queries through Google Search. The input does not include the Wikipedia page with the answer.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Observed inference time (s)",
        "run_group": "NaturalQuestions (closed-book)"
      }
    },
    {
      "value": "NaturalQuestions (closed-book) - Idealized inference time (s)",
      "description": "The NaturalQuestions [(Kwiatkowski et al., 2019)](https://aclanthology.org/Q19-1026/) benchmark for question answering based on naturally-occurring queries through Google Search. The input does not include the Wikipedia page with the answer.\n\nIdealized inference runtime (s): Average time to process a request to the model based solely on the model architecture (using Megatron-LM).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Idealized inference time (s)",
        "run_group": "NaturalQuestions (closed-book)"
      }
    },
    {
      "value": "NaturalQuestions (closed-book) - Denoised inference time (s)",
      "description": "The NaturalQuestions [(Kwiatkowski et al., 2019)](https://aclanthology.org/Q19-1026/) benchmark for question answering based on naturally-occurring queries through Google Search. The input does not include the Wikipedia page with the answer.\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Denoised inference time (s)",
        "run_group": "NaturalQuestions (closed-book)"
      }
    },
    {
      "value": "NaturalQuestions (closed-book) - Training emissions (kg CO2)",
      "description": "The NaturalQuestions [(Kwiatkowski et al., 2019)](https://aclanthology.org/Q19-1026/) benchmark for question answering based on naturally-occurring queries through Google Search. The input does not include the Wikipedia page with the answer.\n\nEstimated training emissions (kg CO2): Estimate of the CO2 emissions from training the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training emissions (kg CO2)",
        "run_group": "NaturalQuestions (closed-book)"
      }
    },
    {
      "value": "NaturalQuestions (closed-book) - Training energy (MWh)",
      "description": "The NaturalQuestions [(Kwiatkowski et al., 2019)](https://aclanthology.org/Q19-1026/) benchmark for question answering based on naturally-occurring queries through Google Search. The input does not include the Wikipedia page with the answer.\n\nEstimated training energy cost (MWh): Estimate of the amount of energy used to train the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training energy (MWh)",
        "run_group": "NaturalQuestions (closed-book)"
      }
    },
    {
      "value": "HellaSwag - Observed inference time (s)",
      "description": "The HellaSwag benchmark for commonsense reasoning in question answering [(Zellers et al., 2019)](https://aclanthology.org/P19-1472/).\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Observed inference time (s)",
        "run_group": "HellaSwag"
      }
    },
    {
      "value": "HellaSwag - Idealized inference time (s)",
      "description": "The HellaSwag benchmark for commonsense reasoning in question answering [(Zellers et al., 2019)](https://aclanthology.org/P19-1472/).\n\nIdealized inference runtime (s): Average time to process a request to the model based solely on the model architecture (using Megatron-LM).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Idealized inference time (s)",
        "run_group": "HellaSwag"
      }
    },
    {
      "value": "HellaSwag - Denoised inference time (s)",
      "description": "The HellaSwag benchmark for commonsense reasoning in question answering [(Zellers et al., 2019)](https://aclanthology.org/P19-1472/).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Denoised inference time (s)",
        "run_group": "HellaSwag"
      }
    },
    {
      "value": "HellaSwag - Training emissions (kg CO2)",
      "description": "The HellaSwag benchmark for commonsense reasoning in question answering [(Zellers et al., 2019)](https://aclanthology.org/P19-1472/).\n\nEstimated training emissions (kg CO2): Estimate of the CO2 emissions from training the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training emissions (kg CO2)",
        "run_group": "HellaSwag"
      }
    },
    {
      "value": "HellaSwag - Training energy (MWh)",
      "description": "The HellaSwag benchmark for commonsense reasoning in question answering [(Zellers et al., 2019)](https://aclanthology.org/P19-1472/).\n\nEstimated training energy cost (MWh): Estimate of the amount of energy used to train the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training energy (MWh)",
        "run_group": "HellaSwag"
      }
    },
    {
      "value": "OpenbookQA - Observed inference time (s)",
      "description": "The OpenbookQA benchmark for commonsense-intensive open book question answering [(Mihaylov et al., 2018)](https://aclanthology.org/D18-1260/).\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Observed inference time (s)",
        "run_group": "OpenbookQA"
      }
    },
    {
      "value": "OpenbookQA - Idealized inference time (s)",
      "description": "The OpenbookQA benchmark for commonsense-intensive open book question answering [(Mihaylov et al., 2018)](https://aclanthology.org/D18-1260/).\n\nIdealized inference runtime (s): Average time to process a request to the model based solely on the model architecture (using Megatron-LM).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Idealized inference time (s)",
        "run_group": "OpenbookQA"
      }
    },
    {
      "value": "OpenbookQA - Denoised inference time (s)",
      "description": "The OpenbookQA benchmark for commonsense-intensive open book question answering [(Mihaylov et al., 2018)](https://aclanthology.org/D18-1260/).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Denoised inference time (s)",
        "run_group": "OpenbookQA"
      }
    },
    {
      "value": "OpenbookQA - Training emissions (kg CO2)",
      "description": "The OpenbookQA benchmark for commonsense-intensive open book question answering [(Mihaylov et al., 2018)](https://aclanthology.org/D18-1260/).\n\nEstimated training emissions (kg CO2): Estimate of the CO2 emissions from training the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training emissions (kg CO2)",
        "run_group": "OpenbookQA"
      }
    },
    {
      "value": "OpenbookQA - Training energy (MWh)",
      "description": "The OpenbookQA benchmark for commonsense-intensive open book question answering [(Mihaylov et al., 2018)](https://aclanthology.org/D18-1260/).\n\nEstimated training energy cost (MWh): Estimate of the amount of energy used to train the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training energy (MWh)",
        "run_group": "OpenbookQA"
      }
    },
    {
      "value": "TruthfulQA - Observed inference time (s)",
      "description": "The TruthfulQA benchmarking for measuring model truthfulness and commonsense knowledge in question answering [(Lin et al., 2022)](https://aclanthology.org/2022.acl-long.229/).\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Observed inference time (s)",
        "run_group": "TruthfulQA"
      }
    },
    {
      "value": "TruthfulQA - Idealized inference time (s)",
      "description": "The TruthfulQA benchmarking for measuring model truthfulness and commonsense knowledge in question answering [(Lin et al., 2022)](https://aclanthology.org/2022.acl-long.229/).\n\nIdealized inference runtime (s): Average time to process a request to the model based solely on the model architecture (using Megatron-LM).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Idealized inference time (s)",
        "run_group": "TruthfulQA"
      }
    },
    {
      "value": "TruthfulQA - Denoised inference time (s)",
      "description": "The TruthfulQA benchmarking for measuring model truthfulness and commonsense knowledge in question answering [(Lin et al., 2022)](https://aclanthology.org/2022.acl-long.229/).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Denoised inference time (s)",
        "run_group": "TruthfulQA"
      }
    },
    {
      "value": "TruthfulQA - Training emissions (kg CO2)",
      "description": "The TruthfulQA benchmarking for measuring model truthfulness and commonsense knowledge in question answering [(Lin et al., 2022)](https://aclanthology.org/2022.acl-long.229/).\n\nEstimated training emissions (kg CO2): Estimate of the CO2 emissions from training the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training emissions (kg CO2)",
        "run_group": "TruthfulQA"
      }
    },
    {
      "value": "TruthfulQA - Training energy (MWh)",
      "description": "The TruthfulQA benchmarking for measuring model truthfulness and commonsense knowledge in question answering [(Lin et al., 2022)](https://aclanthology.org/2022.acl-long.229/).\n\nEstimated training energy cost (MWh): Estimate of the amount of energy used to train the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training energy (MWh)",
        "run_group": "TruthfulQA"
      }
    },
    {
      "value": "MMLU - Observed inference time (s)",
      "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Observed inference time (s)",
        "run_group": "MMLU"
      }
    },
    {
      "value": "MMLU - Idealized inference time (s)",
      "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nIdealized inference runtime (s): Average time to process a request to the model based solely on the model architecture (using Megatron-LM).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Idealized inference time (s)",
        "run_group": "MMLU"
      }
    },
    {
      "value": "MMLU - Denoised inference time (s)",
      "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Denoised inference time (s)",
        "run_group": "MMLU"
      }
    },
    {
      "value": "MMLU - Training emissions (kg CO2)",
      "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nEstimated training emissions (kg CO2): Estimate of the CO2 emissions from training the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training emissions (kg CO2)",
        "run_group": "MMLU"
      }
    },
    {
      "value": "MMLU - Training energy (MWh)",
      "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nEstimated training energy cost (MWh): Estimate of the amount of energy used to train the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training energy (MWh)",
        "run_group": "MMLU"
      }
    },
    {
      "value": "WikiFact - Observed inference time (s)",
      "description": "Scenario introduced in this work, inspired by [Petroni et al. (2019)](https://aclanthology.org/D19-1250/), to more extensively test factual knowledge.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Observed inference time (s)",
        "run_group": "WikiFact"
      }
    },
    {
      "value": "WikiFact - Idealized inference time (s)",
      "description": "Scenario introduced in this work, inspired by [Petroni et al. (2019)](https://aclanthology.org/D19-1250/), to more extensively test factual knowledge.\n\nIdealized inference runtime (s): Average time to process a request to the model based solely on the model architecture (using Megatron-LM).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Idealized inference time (s)",
        "run_group": "WikiFact"
      }
    },
    {
      "value": "WikiFact - Denoised inference time (s)",
      "description": "Scenario introduced in this work, inspired by [Petroni et al. (2019)](https://aclanthology.org/D19-1250/), to more extensively test factual knowledge.\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Denoised inference time (s)",
        "run_group": "WikiFact"
      }
    },
    {
      "value": "WikiFact - Training emissions (kg CO2)",
      "description": "Scenario introduced in this work, inspired by [Petroni et al. (2019)](https://aclanthology.org/D19-1250/), to more extensively test factual knowledge.\n\nEstimated training emissions (kg CO2): Estimate of the CO2 emissions from training the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training emissions (kg CO2)",
        "run_group": "WikiFact"
      }
    },
    {
      "value": "WikiFact - Training energy (MWh)",
      "description": "Scenario introduced in this work, inspired by [Petroni et al. (2019)](https://aclanthology.org/D19-1250/), to more extensively test factual knowledge.\n\nEstimated training energy cost (MWh): Estimate of the amount of energy used to train the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training energy (MWh)",
        "run_group": "WikiFact"
      }
    },
    {
      "value": "Synthetic reasoning (abstract symbols) - Observed inference time (s)",
      "description": "Synthetic reasoning tasks defined using abstract symbols based on LIME [(Wu et al., 2021)](https://proceedings.mlr.press/v139/wu21c.html).\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Observed inference time (s)",
        "run_group": "Synthetic reasoning (abstract symbols)"
      }
    },
    {
      "value": "Synthetic reasoning (abstract symbols) - Idealized inference time (s)",
      "description": "Synthetic reasoning tasks defined using abstract symbols based on LIME [(Wu et al., 2021)](https://proceedings.mlr.press/v139/wu21c.html).\n\nIdealized inference runtime (s): Average time to process a request to the model based solely on the model architecture (using Megatron-LM).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Idealized inference time (s)",
        "run_group": "Synthetic reasoning (abstract symbols)"
      }
    },
    {
      "value": "Synthetic reasoning (abstract symbols) - Denoised inference time (s)",
      "description": "Synthetic reasoning tasks defined using abstract symbols based on LIME [(Wu et al., 2021)](https://proceedings.mlr.press/v139/wu21c.html).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Denoised inference time (s)",
        "run_group": "Synthetic reasoning (abstract symbols)"
      }
    },
    {
      "value": "Synthetic reasoning (abstract symbols) - Training emissions (kg CO2)",
      "description": "Synthetic reasoning tasks defined using abstract symbols based on LIME [(Wu et al., 2021)](https://proceedings.mlr.press/v139/wu21c.html).\n\nEstimated training emissions (kg CO2): Estimate of the CO2 emissions from training the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training emissions (kg CO2)",
        "run_group": "Synthetic reasoning (abstract symbols)"
      }
    },
    {
      "value": "Synthetic reasoning (abstract symbols) - Training energy (MWh)",
      "description": "Synthetic reasoning tasks defined using abstract symbols based on LIME [(Wu et al., 2021)](https://proceedings.mlr.press/v139/wu21c.html).\n\nEstimated training energy cost (MWh): Estimate of the amount of energy used to train the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training energy (MWh)",
        "run_group": "Synthetic reasoning (abstract symbols)"
      }
    },
    {
      "value": "Synthetic reasoning (natural language) - Observed inference time (s)",
      "description": "Synthetic reasoning tasks defined using simple natural language based on LIME [(Wu et al., 2021)](https://proceedings.mlr.press/v139/wu21c.html).\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Observed inference time (s)",
        "run_group": "Synthetic reasoning (natural language)"
      }
    },
    {
      "value": "Synthetic reasoning (natural language) - Idealized inference time (s)",
      "description": "Synthetic reasoning tasks defined using simple natural language based on LIME [(Wu et al., 2021)](https://proceedings.mlr.press/v139/wu21c.html).\n\nIdealized inference runtime (s): Average time to process a request to the model based solely on the model architecture (using Megatron-LM).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Idealized inference time (s)",
        "run_group": "Synthetic reasoning (natural language)"
      }
    },
    {
      "value": "Synthetic reasoning (natural language) - Denoised inference time (s)",
      "description": "Synthetic reasoning tasks defined using simple natural language based on LIME [(Wu et al., 2021)](https://proceedings.mlr.press/v139/wu21c.html).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Denoised inference time (s)",
        "run_group": "Synthetic reasoning (natural language)"
      }
    },
    {
      "value": "Synthetic reasoning (natural language) - Training emissions (kg CO2)",
      "description": "Synthetic reasoning tasks defined using simple natural language based on LIME [(Wu et al., 2021)](https://proceedings.mlr.press/v139/wu21c.html).\n\nEstimated training emissions (kg CO2): Estimate of the CO2 emissions from training the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training emissions (kg CO2)",
        "run_group": "Synthetic reasoning (natural language)"
      }
    },
    {
      "value": "Synthetic reasoning (natural language) - Training energy (MWh)",
      "description": "Synthetic reasoning tasks defined using simple natural language based on LIME [(Wu et al., 2021)](https://proceedings.mlr.press/v139/wu21c.html).\n\nEstimated training energy cost (MWh): Estimate of the amount of energy used to train the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training energy (MWh)",
        "run_group": "Synthetic reasoning (natural language)"
      }
    },
    {
      "value": "bAbI - Observed inference time (s)",
      "description": "The bAbI benchmark for measuring understanding and reasoning [(Weston et al., 2015)](https://arxiv.org/pdf/1502.05698.pdf).\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Observed inference time (s)",
        "run_group": "bAbI"
      }
    },
    {
      "value": "bAbI - Idealized inference time (s)",
      "description": "The bAbI benchmark for measuring understanding and reasoning [(Weston et al., 2015)](https://arxiv.org/pdf/1502.05698.pdf).\n\nIdealized inference runtime (s): Average time to process a request to the model based solely on the model architecture (using Megatron-LM).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Idealized inference time (s)",
        "run_group": "bAbI"
      }
    },
    {
      "value": "bAbI - Denoised inference time (s)",
      "description": "The bAbI benchmark for measuring understanding and reasoning [(Weston et al., 2015)](https://arxiv.org/pdf/1502.05698.pdf).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Denoised inference time (s)",
        "run_group": "bAbI"
      }
    },
    {
      "value": "bAbI - Training emissions (kg CO2)",
      "description": "The bAbI benchmark for measuring understanding and reasoning [(Weston et al., 2015)](https://arxiv.org/pdf/1502.05698.pdf).\n\nEstimated training emissions (kg CO2): Estimate of the CO2 emissions from training the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training emissions (kg CO2)",
        "run_group": "bAbI"
      }
    },
    {
      "value": "bAbI - Training energy (MWh)",
      "description": "The bAbI benchmark for measuring understanding and reasoning [(Weston et al., 2015)](https://arxiv.org/pdf/1502.05698.pdf).\n\nEstimated training energy cost (MWh): Estimate of the amount of energy used to train the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training energy (MWh)",
        "run_group": "bAbI"
      }
    },
    {
      "value": "Dyck - Observed inference time (s)",
      "description": "Scenario testing hierarchical reasoning through the Dyck formal languages [(Suzgun et al., 2019)](https://aclanthology.org/W19-3905/).\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Observed inference time (s)",
        "run_group": "Dyck"
      }
    },
    {
      "value": "Dyck - Idealized inference time (s)",
      "description": "Scenario testing hierarchical reasoning through the Dyck formal languages [(Suzgun et al., 2019)](https://aclanthology.org/W19-3905/).\n\nIdealized inference runtime (s): Average time to process a request to the model based solely on the model architecture (using Megatron-LM).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Idealized inference time (s)",
        "run_group": "Dyck"
      }
    },
    {
      "value": "Dyck - Denoised inference time (s)",
      "description": "Scenario testing hierarchical reasoning through the Dyck formal languages [(Suzgun et al., 2019)](https://aclanthology.org/W19-3905/).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Denoised inference time (s)",
        "run_group": "Dyck"
      }
    },
    {
      "value": "Dyck - Training emissions (kg CO2)",
      "description": "Scenario testing hierarchical reasoning through the Dyck formal languages [(Suzgun et al., 2019)](https://aclanthology.org/W19-3905/).\n\nEstimated training emissions (kg CO2): Estimate of the CO2 emissions from training the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training emissions (kg CO2)",
        "run_group": "Dyck"
      }
    },
    {
      "value": "Dyck - Training energy (MWh)",
      "description": "Scenario testing hierarchical reasoning through the Dyck formal languages [(Suzgun et al., 2019)](https://aclanthology.org/W19-3905/).\n\nEstimated training energy cost (MWh): Estimate of the amount of energy used to train the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training energy (MWh)",
        "run_group": "Dyck"
      }
    },
    {
      "value": "GSM8K - Observed inference time (s)",
      "description": "The grade school math word problems dataset (GSM8K) for testing mathematical reasoning on grade-school math problems [(Cobbe et al., 2021)](https://arxiv.org/pdf/2110.14168.pdf).\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Observed inference time (s)",
        "run_group": "GSM8K"
      }
    },
    {
      "value": "GSM8K - Idealized inference time (s)",
      "description": "The grade school math word problems dataset (GSM8K) for testing mathematical reasoning on grade-school math problems [(Cobbe et al., 2021)](https://arxiv.org/pdf/2110.14168.pdf).\n\nIdealized inference runtime (s): Average time to process a request to the model based solely on the model architecture (using Megatron-LM).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Idealized inference time (s)",
        "run_group": "GSM8K"
      }
    },
    {
      "value": "GSM8K - Denoised inference time (s)",
      "description": "The grade school math word problems dataset (GSM8K) for testing mathematical reasoning on grade-school math problems [(Cobbe et al., 2021)](https://arxiv.org/pdf/2110.14168.pdf).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Denoised inference time (s)",
        "run_group": "GSM8K"
      }
    },
    {
      "value": "GSM8K - Training emissions (kg CO2)",
      "description": "The grade school math word problems dataset (GSM8K) for testing mathematical reasoning on grade-school math problems [(Cobbe et al., 2021)](https://arxiv.org/pdf/2110.14168.pdf).\n\nEstimated training emissions (kg CO2): Estimate of the CO2 emissions from training the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training emissions (kg CO2)",
        "run_group": "GSM8K"
      }
    },
    {
      "value": "GSM8K - Training energy (MWh)",
      "description": "The grade school math word problems dataset (GSM8K) for testing mathematical reasoning on grade-school math problems [(Cobbe et al., 2021)](https://arxiv.org/pdf/2110.14168.pdf).\n\nEstimated training energy cost (MWh): Estimate of the amount of energy used to train the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training energy (MWh)",
        "run_group": "GSM8K"
      }
    },
    {
      "value": "MATH - Observed inference time (s)",
      "description": "The MATH benchmark for measuring mathematical problem solving on competition math problems [(Hendrycks et al., 2021)](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html).\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Observed inference time (s)",
        "run_group": "MATH"
      }
    },
    {
      "value": "MATH - Idealized inference time (s)",
      "description": "The MATH benchmark for measuring mathematical problem solving on competition math problems [(Hendrycks et al., 2021)](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html).\n\nIdealized inference runtime (s): Average time to process a request to the model based solely on the model architecture (using Megatron-LM).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Idealized inference time (s)",
        "run_group": "MATH"
      }
    },
    {
      "value": "MATH - Denoised inference time (s)",
      "description": "The MATH benchmark for measuring mathematical problem solving on competition math problems [(Hendrycks et al., 2021)](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Denoised inference time (s)",
        "run_group": "MATH"
      }
    },
    {
      "value": "MATH - Training emissions (kg CO2)",
      "description": "The MATH benchmark for measuring mathematical problem solving on competition math problems [(Hendrycks et al., 2021)](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html).\n\nEstimated training emissions (kg CO2): Estimate of the CO2 emissions from training the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training emissions (kg CO2)",
        "run_group": "MATH"
      }
    },
    {
      "value": "MATH - Training energy (MWh)",
      "description": "The MATH benchmark for measuring mathematical problem solving on competition math problems [(Hendrycks et al., 2021)](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html).\n\nEstimated training energy cost (MWh): Estimate of the amount of energy used to train the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training energy (MWh)",
        "run_group": "MATH"
      }
    },
    {
      "value": "MATH (chain-of-thought) - Observed inference time (s)",
      "description": "The MATH benchmark for measuring mathematical problem solving on competition math problems with chain-of-thought style reasoning [(Hendrycks et al., 2021)](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html).\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Observed inference time (s)",
        "run_group": "MATH (chain-of-thought)"
      }
    },
    {
      "value": "MATH (chain-of-thought) - Idealized inference time (s)",
      "description": "The MATH benchmark for measuring mathematical problem solving on competition math problems with chain-of-thought style reasoning [(Hendrycks et al., 2021)](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html).\n\nIdealized inference runtime (s): Average time to process a request to the model based solely on the model architecture (using Megatron-LM).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Idealized inference time (s)",
        "run_group": "MATH (chain-of-thought)"
      }
    },
    {
      "value": "MATH (chain-of-thought) - Denoised inference time (s)",
      "description": "The MATH benchmark for measuring mathematical problem solving on competition math problems with chain-of-thought style reasoning [(Hendrycks et al., 2021)](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Denoised inference time (s)",
        "run_group": "MATH (chain-of-thought)"
      }
    },
    {
      "value": "MATH (chain-of-thought) - Training emissions (kg CO2)",
      "description": "The MATH benchmark for measuring mathematical problem solving on competition math problems with chain-of-thought style reasoning [(Hendrycks et al., 2021)](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html).\n\nEstimated training emissions (kg CO2): Estimate of the CO2 emissions from training the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training emissions (kg CO2)",
        "run_group": "MATH (chain-of-thought)"
      }
    },
    {
      "value": "MATH (chain-of-thought) - Training energy (MWh)",
      "description": "The MATH benchmark for measuring mathematical problem solving on competition math problems with chain-of-thought style reasoning [(Hendrycks et al., 2021)](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html).\n\nEstimated training energy cost (MWh): Estimate of the amount of energy used to train the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training energy (MWh)",
        "run_group": "MATH (chain-of-thought)"
      }
    },
    {
      "value": "APPS (Code) - Observed inference time (s)",
      "description": "The APPS benchmark for measuring competence on code challenges [(Hendrycks et al., 2021)](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/c24cd76e1ce41366a4bbe8a49b02a028-Abstract-round2.html).\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Observed inference time (s)",
        "run_group": "APPS (Code)"
      }
    },
    {
      "value": "APPS (Code) - Idealized inference time (s)",
      "description": "The APPS benchmark for measuring competence on code challenges [(Hendrycks et al., 2021)](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/c24cd76e1ce41366a4bbe8a49b02a028-Abstract-round2.html).\n\nIdealized inference runtime (s): Average time to process a request to the model based solely on the model architecture (using Megatron-LM).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Idealized inference time (s)",
        "run_group": "APPS (Code)"
      }
    },
    {
      "value": "APPS (Code) - Denoised inference time (s)",
      "description": "The APPS benchmark for measuring competence on code challenges [(Hendrycks et al., 2021)](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/c24cd76e1ce41366a4bbe8a49b02a028-Abstract-round2.html).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Denoised inference time (s)",
        "run_group": "APPS (Code)"
      }
    },
    {
      "value": "APPS (Code) - Training emissions (kg CO2)",
      "description": "The APPS benchmark for measuring competence on code challenges [(Hendrycks et al., 2021)](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/c24cd76e1ce41366a4bbe8a49b02a028-Abstract-round2.html).\n\nEstimated training emissions (kg CO2): Estimate of the CO2 emissions from training the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training emissions (kg CO2)",
        "run_group": "APPS (Code)"
      }
    },
    {
      "value": "APPS (Code) - Training energy (MWh)",
      "description": "The APPS benchmark for measuring competence on code challenges [(Hendrycks et al., 2021)](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/c24cd76e1ce41366a4bbe8a49b02a028-Abstract-round2.html).\n\nEstimated training energy cost (MWh): Estimate of the amount of energy used to train the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training energy (MWh)",
        "run_group": "APPS (Code)"
      }
    },
    {
      "value": "HumanEval (Code) - Observed inference time (s)",
      "description": "The HumanEval benchmark for measuring functional correctness for synthesizing programs from docstrings [(Chen et al., 2021)](https://arxiv.org/pdf/2107.03374.pdf).\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Observed inference time (s)",
        "run_group": "HumanEval (Code)"
      }
    },
    {
      "value": "HumanEval (Code) - Idealized inference time (s)",
      "description": "The HumanEval benchmark for measuring functional correctness for synthesizing programs from docstrings [(Chen et al., 2021)](https://arxiv.org/pdf/2107.03374.pdf).\n\nIdealized inference runtime (s): Average time to process a request to the model based solely on the model architecture (using Megatron-LM).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Idealized inference time (s)",
        "run_group": "HumanEval (Code)"
      }
    },
    {
      "value": "HumanEval (Code) - Denoised inference time (s)",
      "description": "The HumanEval benchmark for measuring functional correctness for synthesizing programs from docstrings [(Chen et al., 2021)](https://arxiv.org/pdf/2107.03374.pdf).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Denoised inference time (s)",
        "run_group": "HumanEval (Code)"
      }
    },
    {
      "value": "HumanEval (Code) - Training emissions (kg CO2)",
      "description": "The HumanEval benchmark for measuring functional correctness for synthesizing programs from docstrings [(Chen et al., 2021)](https://arxiv.org/pdf/2107.03374.pdf).\n\nEstimated training emissions (kg CO2): Estimate of the CO2 emissions from training the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training emissions (kg CO2)",
        "run_group": "HumanEval (Code)"
      }
    },
    {
      "value": "HumanEval (Code) - Training energy (MWh)",
      "description": "The HumanEval benchmark for measuring functional correctness for synthesizing programs from docstrings [(Chen et al., 2021)](https://arxiv.org/pdf/2107.03374.pdf).\n\nEstimated training energy cost (MWh): Estimate of the amount of energy used to train the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training energy (MWh)",
        "run_group": "HumanEval (Code)"
      }
    },
    {
      "value": "LSAT - Observed inference time (s)",
      "description": "The LSAT benchmark for measuring analytical reasoning on the Law School Admission Test (LSAT; [Zhong et al., 2021](https://arxiv.org/pdf/2104.06598.pdf)).\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Observed inference time (s)",
        "run_group": "LSAT"
      }
    },
    {
      "value": "LSAT - Idealized inference time (s)",
      "description": "The LSAT benchmark for measuring analytical reasoning on the Law School Admission Test (LSAT; [Zhong et al., 2021](https://arxiv.org/pdf/2104.06598.pdf)).\n\nIdealized inference runtime (s): Average time to process a request to the model based solely on the model architecture (using Megatron-LM).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Idealized inference time (s)",
        "run_group": "LSAT"
      }
    },
    {
      "value": "LSAT - Denoised inference time (s)",
      "description": "The LSAT benchmark for measuring analytical reasoning on the Law School Admission Test (LSAT; [Zhong et al., 2021](https://arxiv.org/pdf/2104.06598.pdf)).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Denoised inference time (s)",
        "run_group": "LSAT"
      }
    },
    {
      "value": "LSAT - Training emissions (kg CO2)",
      "description": "The LSAT benchmark for measuring analytical reasoning on the Law School Admission Test (LSAT; [Zhong et al., 2021](https://arxiv.org/pdf/2104.06598.pdf)).\n\nEstimated training emissions (kg CO2): Estimate of the CO2 emissions from training the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training emissions (kg CO2)",
        "run_group": "LSAT"
      }
    },
    {
      "value": "LSAT - Training energy (MWh)",
      "description": "The LSAT benchmark for measuring analytical reasoning on the Law School Admission Test (LSAT; [Zhong et al., 2021](https://arxiv.org/pdf/2104.06598.pdf)).\n\nEstimated training energy cost (MWh): Estimate of the amount of energy used to train the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training energy (MWh)",
        "run_group": "LSAT"
      }
    },
    {
      "value": "LegalSupport - Observed inference time (s)",
      "description": "Scenario introduced in this work to measure fine-grained legal reasoning through reverse entailment.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Observed inference time (s)",
        "run_group": "LegalSupport"
      }
    },
    {
      "value": "LegalSupport - Idealized inference time (s)",
      "description": "Scenario introduced in this work to measure fine-grained legal reasoning through reverse entailment.\n\nIdealized inference runtime (s): Average time to process a request to the model based solely on the model architecture (using Megatron-LM).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Idealized inference time (s)",
        "run_group": "LegalSupport"
      }
    },
    {
      "value": "LegalSupport - Denoised inference time (s)",
      "description": "Scenario introduced in this work to measure fine-grained legal reasoning through reverse entailment.\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Denoised inference time (s)",
        "run_group": "LegalSupport"
      }
    },
    {
      "value": "LegalSupport - Training emissions (kg CO2)",
      "description": "Scenario introduced in this work to measure fine-grained legal reasoning through reverse entailment.\n\nEstimated training emissions (kg CO2): Estimate of the CO2 emissions from training the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training emissions (kg CO2)",
        "run_group": "LegalSupport"
      }
    },
    {
      "value": "LegalSupport - Training energy (MWh)",
      "description": "Scenario introduced in this work to measure fine-grained legal reasoning through reverse entailment.\n\nEstimated training energy cost (MWh): Estimate of the amount of energy used to train the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training energy (MWh)",
        "run_group": "LegalSupport"
      }
    },
    {
      "value": "Data imputation - Observed inference time (s)",
      "description": "Scenario from [Mei et al. (2021)](https://ieeexplore.ieee.org/document/9458712/) that tests the ability to impute missing entities in a data table.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Observed inference time (s)",
        "run_group": "Data imputation"
      }
    },
    {
      "value": "Data imputation - Idealized inference time (s)",
      "description": "Scenario from [Mei et al. (2021)](https://ieeexplore.ieee.org/document/9458712/) that tests the ability to impute missing entities in a data table.\n\nIdealized inference runtime (s): Average time to process a request to the model based solely on the model architecture (using Megatron-LM).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Idealized inference time (s)",
        "run_group": "Data imputation"
      }
    },
    {
      "value": "Data imputation - Denoised inference time (s)",
      "description": "Scenario from [Mei et al. (2021)](https://ieeexplore.ieee.org/document/9458712/) that tests the ability to impute missing entities in a data table.\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Denoised inference time (s)",
        "run_group": "Data imputation"
      }
    },
    {
      "value": "Data imputation - Training emissions (kg CO2)",
      "description": "Scenario from [Mei et al. (2021)](https://ieeexplore.ieee.org/document/9458712/) that tests the ability to impute missing entities in a data table.\n\nEstimated training emissions (kg CO2): Estimate of the CO2 emissions from training the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training emissions (kg CO2)",
        "run_group": "Data imputation"
      }
    },
    {
      "value": "Data imputation - Training energy (MWh)",
      "description": "Scenario from [Mei et al. (2021)](https://ieeexplore.ieee.org/document/9458712/) that tests the ability to impute missing entities in a data table.\n\nEstimated training energy cost (MWh): Estimate of the amount of energy used to train the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training energy (MWh)",
        "run_group": "Data imputation"
      }
    },
    {
      "value": "Entity matching - Observed inference time (s)",
      "description": "Scenario from Magellan [(Konda et al., 2016)](https://dl.acm.org/doi/10.14778/3007263.3007314) that tests the ability to determine if two entities match.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Observed inference time (s)",
        "run_group": "Entity matching"
      }
    },
    {
      "value": "Entity matching - Idealized inference time (s)",
      "description": "Scenario from Magellan [(Konda et al., 2016)](https://dl.acm.org/doi/10.14778/3007263.3007314) that tests the ability to determine if two entities match.\n\nIdealized inference runtime (s): Average time to process a request to the model based solely on the model architecture (using Megatron-LM).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Idealized inference time (s)",
        "run_group": "Entity matching"
      }
    },
    {
      "value": "Entity matching - Denoised inference time (s)",
      "description": "Scenario from Magellan [(Konda et al., 2016)](https://dl.acm.org/doi/10.14778/3007263.3007314) that tests the ability to determine if two entities match.\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Denoised inference time (s)",
        "run_group": "Entity matching"
      }
    },
    {
      "value": "Entity matching - Training emissions (kg CO2)",
      "description": "Scenario from Magellan [(Konda et al., 2016)](https://dl.acm.org/doi/10.14778/3007263.3007314) that tests the ability to determine if two entities match.\n\nEstimated training emissions (kg CO2): Estimate of the CO2 emissions from training the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training emissions (kg CO2)",
        "run_group": "Entity matching"
      }
    },
    {
      "value": "Entity matching - Training energy (MWh)",
      "description": "Scenario from Magellan [(Konda et al., 2016)](https://dl.acm.org/doi/10.14778/3007263.3007314) that tests the ability to determine if two entities match.\n\nEstimated training energy cost (MWh): Estimate of the amount of energy used to train the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training energy (MWh)",
        "run_group": "Entity matching"
      }
    },
    {
      "value": "Copyright (text) - Observed inference time (s)",
      "description": "Scenario introduced in this work to measure copyright and memorization behavior for books, based off of [Carlini et al. (2021)](https://www.usenix.org/biblio-11958).\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Observed inference time (s)",
        "run_group": "Copyright (text)"
      }
    },
    {
      "value": "Copyright (text) - Idealized inference time (s)",
      "description": "Scenario introduced in this work to measure copyright and memorization behavior for books, based off of [Carlini et al. (2021)](https://www.usenix.org/biblio-11958).\n\nIdealized inference runtime (s): Average time to process a request to the model based solely on the model architecture (using Megatron-LM).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Idealized inference time (s)",
        "run_group": "Copyright (text)"
      }
    },
    {
      "value": "Copyright (text) - Denoised inference time (s)",
      "description": "Scenario introduced in this work to measure copyright and memorization behavior for books, based off of [Carlini et al. (2021)](https://www.usenix.org/biblio-11958).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Denoised inference time (s)",
        "run_group": "Copyright (text)"
      }
    },
    {
      "value": "Copyright (text) - Training emissions (kg CO2)",
      "description": "Scenario introduced in this work to measure copyright and memorization behavior for books, based off of [Carlini et al. (2021)](https://www.usenix.org/biblio-11958).\n\nEstimated training emissions (kg CO2): Estimate of the CO2 emissions from training the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training emissions (kg CO2)",
        "run_group": "Copyright (text)"
      }
    },
    {
      "value": "Copyright (text) - Training energy (MWh)",
      "description": "Scenario introduced in this work to measure copyright and memorization behavior for books, based off of [Carlini et al. (2021)](https://www.usenix.org/biblio-11958).\n\nEstimated training energy cost (MWh): Estimate of the amount of energy used to train the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training energy (MWh)",
        "run_group": "Copyright (text)"
      }
    },
    {
      "value": "Copyright (code) - Observed inference time (s)",
      "description": "Scenario introduced in this work to measure copyright and memorization behavior for code, based off of [Carlini et al. (2021)](https://www.usenix.org/biblio-11958).\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Observed inference time (s)",
        "run_group": "Copyright (code)"
      }
    },
    {
      "value": "Copyright (code) - Idealized inference time (s)",
      "description": "Scenario introduced in this work to measure copyright and memorization behavior for code, based off of [Carlini et al. (2021)](https://www.usenix.org/biblio-11958).\n\nIdealized inference runtime (s): Average time to process a request to the model based solely on the model architecture (using Megatron-LM).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Idealized inference time (s)",
        "run_group": "Copyright (code)"
      }
    },
    {
      "value": "Copyright (code) - Denoised inference time (s)",
      "description": "Scenario introduced in this work to measure copyright and memorization behavior for code, based off of [Carlini et al. (2021)](https://www.usenix.org/biblio-11958).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Denoised inference time (s)",
        "run_group": "Copyright (code)"
      }
    },
    {
      "value": "Copyright (code) - Training emissions (kg CO2)",
      "description": "Scenario introduced in this work to measure copyright and memorization behavior for code, based off of [Carlini et al. (2021)](https://www.usenix.org/biblio-11958).\n\nEstimated training emissions (kg CO2): Estimate of the CO2 emissions from training the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training emissions (kg CO2)",
        "run_group": "Copyright (code)"
      }
    },
    {
      "value": "Copyright (code) - Training energy (MWh)",
      "description": "Scenario introduced in this work to measure copyright and memorization behavior for code, based off of [Carlini et al. (2021)](https://www.usenix.org/biblio-11958).\n\nEstimated training energy cost (MWh): Estimate of the amount of energy used to train the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training energy (MWh)",
        "run_group": "Copyright (code)"
      }
    },
    {
      "value": "Disinformation (reiteration) - Observed inference time (s)",
      "description": "Scenario from [Buchanan et al. (2021)](https://cset.georgetown.edu/publication/truth-lies-and-automation/) that tests the ability to reiterate disinformation content.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Observed inference time (s)",
        "run_group": "Disinformation (reiteration)"
      }
    },
    {
      "value": "Disinformation (reiteration) - Idealized inference time (s)",
      "description": "Scenario from [Buchanan et al. (2021)](https://cset.georgetown.edu/publication/truth-lies-and-automation/) that tests the ability to reiterate disinformation content.\n\nIdealized inference runtime (s): Average time to process a request to the model based solely on the model architecture (using Megatron-LM).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Idealized inference time (s)",
        "run_group": "Disinformation (reiteration)"
      }
    },
    {
      "value": "Disinformation (reiteration) - Denoised inference time (s)",
      "description": "Scenario from [Buchanan et al. (2021)](https://cset.georgetown.edu/publication/truth-lies-and-automation/) that tests the ability to reiterate disinformation content.\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Denoised inference time (s)",
        "run_group": "Disinformation (reiteration)"
      }
    },
    {
      "value": "Disinformation (reiteration) - Training emissions (kg CO2)",
      "description": "Scenario from [Buchanan et al. (2021)](https://cset.georgetown.edu/publication/truth-lies-and-automation/) that tests the ability to reiterate disinformation content.\n\nEstimated training emissions (kg CO2): Estimate of the CO2 emissions from training the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training emissions (kg CO2)",
        "run_group": "Disinformation (reiteration)"
      }
    },
    {
      "value": "Disinformation (reiteration) - Training energy (MWh)",
      "description": "Scenario from [Buchanan et al. (2021)](https://cset.georgetown.edu/publication/truth-lies-and-automation/) that tests the ability to reiterate disinformation content.\n\nEstimated training energy cost (MWh): Estimate of the amount of energy used to train the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training energy (MWh)",
        "run_group": "Disinformation (reiteration)"
      }
    },
    {
      "value": "Disinformation (wedging) - Observed inference time (s)",
      "description": "Scenario from [Buchanan et al. (2021)](https://cset.georgetown.edu/publication/truth-lies-and-automation/) that tests the ability to generate divisive and wedging content.\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Observed inference time (s)",
        "run_group": "Disinformation (wedging)"
      }
    },
    {
      "value": "Disinformation (wedging) - Idealized inference time (s)",
      "description": "Scenario from [Buchanan et al. (2021)](https://cset.georgetown.edu/publication/truth-lies-and-automation/) that tests the ability to generate divisive and wedging content.\n\nIdealized inference runtime (s): Average time to process a request to the model based solely on the model architecture (using Megatron-LM).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Idealized inference time (s)",
        "run_group": "Disinformation (wedging)"
      }
    },
    {
      "value": "Disinformation (wedging) - Denoised inference time (s)",
      "description": "Scenario from [Buchanan et al. (2021)](https://cset.georgetown.edu/publication/truth-lies-and-automation/) that tests the ability to generate divisive and wedging content.\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Denoised inference time (s)",
        "run_group": "Disinformation (wedging)"
      }
    },
    {
      "value": "Disinformation (wedging) - Training emissions (kg CO2)",
      "description": "Scenario from [Buchanan et al. (2021)](https://cset.georgetown.edu/publication/truth-lies-and-automation/) that tests the ability to generate divisive and wedging content.\n\nEstimated training emissions (kg CO2): Estimate of the CO2 emissions from training the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training emissions (kg CO2)",
        "run_group": "Disinformation (wedging)"
      }
    },
    {
      "value": "Disinformation (wedging) - Training energy (MWh)",
      "description": "Scenario from [Buchanan et al. (2021)](https://cset.georgetown.edu/publication/truth-lies-and-automation/) that tests the ability to generate divisive and wedging content.\n\nEstimated training energy cost (MWh): Estimate of the amount of energy used to train the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training energy (MWh)",
        "run_group": "Disinformation (wedging)"
      }
    },
    {
      "value": "BBQ - Observed inference time (s)",
      "description": "The Bias Benchmark for Question Answering (BBQ) for measuring social bias in question answering in ambiguous and unambigous context [(Parrish et al., 2022)](https://aclanthology.org/2022.findings-acl.165/).\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Observed inference time (s)",
        "run_group": "BBQ"
      }
    },
    {
      "value": "BBQ - Idealized inference time (s)",
      "description": "The Bias Benchmark for Question Answering (BBQ) for measuring social bias in question answering in ambiguous and unambigous context [(Parrish et al., 2022)](https://aclanthology.org/2022.findings-acl.165/).\n\nIdealized inference runtime (s): Average time to process a request to the model based solely on the model architecture (using Megatron-LM).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Idealized inference time (s)",
        "run_group": "BBQ"
      }
    },
    {
      "value": "BBQ - Denoised inference time (s)",
      "description": "The Bias Benchmark for Question Answering (BBQ) for measuring social bias in question answering in ambiguous and unambigous context [(Parrish et al., 2022)](https://aclanthology.org/2022.findings-acl.165/).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Denoised inference time (s)",
        "run_group": "BBQ"
      }
    },
    {
      "value": "BBQ - Training emissions (kg CO2)",
      "description": "The Bias Benchmark for Question Answering (BBQ) for measuring social bias in question answering in ambiguous and unambigous context [(Parrish et al., 2022)](https://aclanthology.org/2022.findings-acl.165/).\n\nEstimated training emissions (kg CO2): Estimate of the CO2 emissions from training the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training emissions (kg CO2)",
        "run_group": "BBQ"
      }
    },
    {
      "value": "BBQ - Training energy (MWh)",
      "description": "The Bias Benchmark for Question Answering (BBQ) for measuring social bias in question answering in ambiguous and unambigous context [(Parrish et al., 2022)](https://aclanthology.org/2022.findings-acl.165/).\n\nEstimated training energy cost (MWh): Estimate of the amount of energy used to train the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training energy (MWh)",
        "run_group": "BBQ"
      }
    },
    {
      "value": "BOLD - Observed inference time (s)",
      "description": "The Bias in Open-Ended Language Generation Dataset (BOLD) for measuring biases and toxicity in open-ended language generation [(Dhamala et al., 2021)](https://dl.acm.org/doi/10.1145/3442188.3445924).\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Observed inference time (s)",
        "run_group": "BOLD"
      }
    },
    {
      "value": "BOLD - Idealized inference time (s)",
      "description": "The Bias in Open-Ended Language Generation Dataset (BOLD) for measuring biases and toxicity in open-ended language generation [(Dhamala et al., 2021)](https://dl.acm.org/doi/10.1145/3442188.3445924).\n\nIdealized inference runtime (s): Average time to process a request to the model based solely on the model architecture (using Megatron-LM).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Idealized inference time (s)",
        "run_group": "BOLD"
      }
    },
    {
      "value": "BOLD - Denoised inference time (s)",
      "description": "The Bias in Open-Ended Language Generation Dataset (BOLD) for measuring biases and toxicity in open-ended language generation [(Dhamala et al., 2021)](https://dl.acm.org/doi/10.1145/3442188.3445924).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Denoised inference time (s)",
        "run_group": "BOLD"
      }
    },
    {
      "value": "BOLD - Training emissions (kg CO2)",
      "description": "The Bias in Open-Ended Language Generation Dataset (BOLD) for measuring biases and toxicity in open-ended language generation [(Dhamala et al., 2021)](https://dl.acm.org/doi/10.1145/3442188.3445924).\n\nEstimated training emissions (kg CO2): Estimate of the CO2 emissions from training the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training emissions (kg CO2)",
        "run_group": "BOLD"
      }
    },
    {
      "value": "BOLD - Training energy (MWh)",
      "description": "The Bias in Open-Ended Language Generation Dataset (BOLD) for measuring biases and toxicity in open-ended language generation [(Dhamala et al., 2021)](https://dl.acm.org/doi/10.1145/3442188.3445924).\n\nEstimated training energy cost (MWh): Estimate of the amount of energy used to train the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training energy (MWh)",
        "run_group": "BOLD"
      }
    },
    {
      "value": "RealToxicityPrompts - Observed inference time (s)",
      "description": "The RealToxicityPrompts dataset for measuring toxicity in prompted model generations [(Gehman et al., 2020)](https://aclanthology.org/2020.findings-emnlp.301/).\n\nObserved inference runtime (s): Average observed time to process a request to the model (via an API, and thus depends on particular deployment).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Observed inference time (s)",
        "run_group": "RealToxicityPrompts"
      }
    },
    {
      "value": "RealToxicityPrompts - Idealized inference time (s)",
      "description": "The RealToxicityPrompts dataset for measuring toxicity in prompted model generations [(Gehman et al., 2020)](https://aclanthology.org/2020.findings-emnlp.301/).\n\nIdealized inference runtime (s): Average time to process a request to the model based solely on the model architecture (using Megatron-LM).",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Idealized inference time (s)",
        "run_group": "RealToxicityPrompts"
      }
    },
    {
      "value": "RealToxicityPrompts - Denoised inference time (s)",
      "description": "The RealToxicityPrompts dataset for measuring toxicity in prompted model generations [(Gehman et al., 2020)](https://aclanthology.org/2020.findings-emnlp.301/).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Denoised inference time (s)",
        "run_group": "RealToxicityPrompts"
      }
    },
    {
      "value": "RealToxicityPrompts - Training emissions (kg CO2)",
      "description": "The RealToxicityPrompts dataset for measuring toxicity in prompted model generations [(Gehman et al., 2020)](https://aclanthology.org/2020.findings-emnlp.301/).\n\nEstimated training emissions (kg CO2): Estimate of the CO2 emissions from training the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training emissions (kg CO2)",
        "run_group": "RealToxicityPrompts"
      }
    },
    {
      "value": "RealToxicityPrompts - Training energy (MWh)",
      "description": "The RealToxicityPrompts dataset for measuring toxicity in prompted model generations [(Gehman et al., 2020)](https://aclanthology.org/2020.findings-emnlp.301/).\n\nEstimated training energy cost (MWh): Estimate of the amount of energy used to train the model.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "Training energy (MWh)",
        "run_group": "RealToxicityPrompts"
      }
    }
  ],
  "rows": [
    [
      {
        "value": "Together AI Llama 3 (8B)",
        "description": "",
        "markdown": false
      },
      {
        "value": 1.0,
        "style": {
          "font-weight": "bold"
        },
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "value": 0.8968552043934309,
        "description": "min=0.659, mean=0.897, max=1.452, sum=51.121 (57)",
        "style": {
          "font-weight": "bold"
        },
        "markdown": false,
        "run_spec_names": [
          "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=anatomy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=astronomy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=business_ethics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=college_biology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=college_medicine,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=college_physics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=computer_security,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=econometrics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=formal_logic,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=global_facts,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=human_aging,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=international_law,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=machine_learning,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=management,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=marketing,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=nutrition,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=philosophy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=prehistory,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=professional_law,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=public_relations,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=security_studies,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=sociology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=virology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=world_religions,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
        ]
      },
      {
        "description": "57 matching runs, but no matching metrics",
        "markdown": false
      },
      {
        "description": "57 matching runs, but no matching metrics",
        "markdown": false
      },
      {
        "description": "(0)",
        "style": {},
        "markdown": false,
        "run_spec_names": [
          "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=anatomy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=astronomy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=business_ethics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=college_biology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=college_medicine,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=college_physics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=computer_security,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=econometrics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=formal_logic,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=global_facts,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=human_aging,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=international_law,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=machine_learning,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=management,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=marketing,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=nutrition,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=philosophy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=prehistory,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=professional_law,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=public_relations,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=security_studies,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=sociology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=virology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=world_religions,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
        ]
      },
      {
        "description": "(0)",
        "style": {},
        "markdown": false,
        "run_spec_names": [
          "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=anatomy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=astronomy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=business_ethics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=college_biology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=college_medicine,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=college_physics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=computer_security,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=econometrics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=formal_logic,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=global_facts,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=human_aging,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=international_law,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=machine_learning,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=management,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=marketing,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=nutrition,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=philosophy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=prehistory,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=professional_law,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=public_relations,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=security_studies,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=sociology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=virology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=world_religions,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
        ]
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      }
    ],
    [
      {
        "value": "Together AI Llama 3 (70B)",
        "description": "",
        "markdown": false
      },
      {
        "value": 0.75,
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "value": 0.9152035797913505,
        "description": "min=0.779, mean=0.915, max=1.573, sum=52.167 (57)",
        "style": {},
        "markdown": false,
        "run_spec_names": [
          "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=anatomy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=astronomy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=business_ethics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=college_biology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=college_medicine,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=college_physics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=computer_security,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=econometrics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=formal_logic,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=global_facts,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=human_aging,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=international_law,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=machine_learning,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=management,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=marketing,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=nutrition,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=philosophy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=prehistory,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=professional_law,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=public_relations,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=security_studies,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=sociology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=virology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=world_religions,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
        ]
      },
      {
        "description": "57 matching runs, but no matching metrics",
        "markdown": false
      },
      {
        "description": "57 matching runs, but no matching metrics",
        "markdown": false
      },
      {
        "description": "(0)",
        "style": {},
        "markdown": false,
        "run_spec_names": [
          "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=anatomy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=astronomy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=business_ethics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=college_biology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=college_medicine,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=college_physics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=computer_security,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=econometrics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=formal_logic,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=global_facts,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=human_aging,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=international_law,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=machine_learning,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=management,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=marketing,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=nutrition,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=philosophy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=prehistory,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=professional_law,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=public_relations,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=security_studies,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=sociology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=virology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=world_religions,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
        ]
      },
      {
        "description": "(0)",
        "style": {},
        "markdown": false,
        "run_spec_names": [
          "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=anatomy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=astronomy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=business_ethics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=college_biology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=college_medicine,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=college_physics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=computer_security,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=econometrics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=formal_logic,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=global_facts,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=human_aging,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=international_law,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=machine_learning,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=management,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=marketing,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=nutrition,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=philosophy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=prehistory,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=professional_law,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=public_relations,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=security_studies,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=sociology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=virology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=world_religions,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
        ]
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      }
    ],
    [
      {
        "value": "Groq LLaMA 3 (8B)",
        "description": "",
        "markdown": false
      },
      {
        "value": 0.5,
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "value": 3.0843004897548245,
        "description": "min=0.519, mean=3.084, max=9.862, sum=175.805 (57)",
        "style": {},
        "markdown": false,
        "run_spec_names": [
          "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=anatomy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=astronomy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=business_ethics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=college_biology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=college_medicine,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=college_physics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=computer_security,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=econometrics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=formal_logic,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=global_facts,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=human_aging,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=international_law,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=machine_learning,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=management,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=marketing,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=nutrition,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=philosophy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=prehistory,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=professional_law,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=public_relations,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=security_studies,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=sociology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=virology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=world_religions,method=multiple_choice_joint,model=groq_llama3-8b-8192"
        ]
      },
      {
        "description": "57 matching runs, but no matching metrics",
        "markdown": false
      },
      {
        "description": "57 matching runs, but no matching metrics",
        "markdown": false
      },
      {
        "description": "(0)",
        "style": {},
        "markdown": false,
        "run_spec_names": [
          "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=anatomy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=astronomy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=business_ethics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=college_biology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=college_medicine,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=college_physics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=computer_security,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=econometrics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=formal_logic,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=global_facts,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=human_aging,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=international_law,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=machine_learning,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=management,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=marketing,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=nutrition,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=philosophy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=prehistory,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=professional_law,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=public_relations,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=security_studies,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=sociology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=virology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=world_religions,method=multiple_choice_joint,model=groq_llama3-8b-8192"
        ]
      },
      {
        "description": "(0)",
        "style": {},
        "markdown": false,
        "run_spec_names": [
          "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=anatomy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=astronomy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=business_ethics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=college_biology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=college_medicine,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=college_physics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=computer_security,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=econometrics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=formal_logic,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=global_facts,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=human_aging,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=international_law,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=machine_learning,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=management,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=marketing,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=nutrition,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=philosophy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=prehistory,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=professional_law,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=public_relations,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=security_studies,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=sociology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=virology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=world_religions,method=multiple_choice_joint,model=groq_llama3-8b-8192"
        ]
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      }
    ],
    [
      {
        "value": "Groq LLaMA 3 (70B)",
        "description": "",
        "markdown": false
      },
      {
        "value": 0.25,
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "value": 14.847715817846877,
        "description": "min=2.019, mean=14.848, max=56.382, sum=816.624 (55)",
        "style": {},
        "markdown": false,
        "run_spec_names": [
          "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=anatomy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=astronomy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=business_ethics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=college_biology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=college_medicine,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=college_physics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=computer_security,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=econometrics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=formal_logic,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=global_facts,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=human_aging,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=international_law,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=machine_learning,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=management,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=marketing,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=nutrition,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=philosophy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=prehistory,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=professional_law,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=public_relations,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=security_studies,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=sociology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=virology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=world_religions,method=multiple_choice_joint,model=groq_llama3-70b-8192"
        ]
      },
      {
        "description": "55 matching runs, but no matching metrics",
        "markdown": false
      },
      {
        "description": "55 matching runs, but no matching metrics",
        "markdown": false
      },
      {
        "description": "(0)",
        "style": {},
        "markdown": false,
        "run_spec_names": [
          "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=anatomy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=astronomy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=business_ethics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=college_biology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=college_medicine,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=college_physics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=computer_security,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=econometrics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=formal_logic,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=global_facts,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=human_aging,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=international_law,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=machine_learning,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=management,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=marketing,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=nutrition,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=philosophy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=prehistory,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=professional_law,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=public_relations,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=security_studies,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=sociology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=virology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=world_religions,method=multiple_choice_joint,model=groq_llama3-70b-8192"
        ]
      },
      {
        "description": "(0)",
        "style": {},
        "markdown": false,
        "run_spec_names": [
          "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=anatomy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=astronomy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=business_ethics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=college_biology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=college_medicine,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=college_physics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=computer_security,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=econometrics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=formal_logic,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=global_facts,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=human_aging,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=international_law,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=machine_learning,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=management,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=marketing,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=nutrition,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=philosophy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=prehistory,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=professional_law,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=public_relations,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=security_studies,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=sociology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=virology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=world_religions,method=multiple_choice_joint,model=groq_llama3-70b-8192"
        ]
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      }
    ],
    [
      {
        "value": "Hugging face LLaMA 3 (8B)(original)",
        "description": "",
        "markdown": false
      },
      {
        "value": 0.0,
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "value": 25.47382867102776,
        "description": "min=16.83, mean=25.474, max=48.006, sum=1452.008 (57)",
        "style": {},
        "markdown": false,
        "run_spec_names": [
          "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=anatomy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=astronomy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=business_ethics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=college_biology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=college_medicine,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=college_physics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=computer_security,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=econometrics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=formal_logic,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=global_facts,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=human_aging,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=international_law,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=machine_learning,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=management,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=marketing,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=nutrition,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=philosophy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=prehistory,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=professional_law,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=public_relations,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=security_studies,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=sociology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=virology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=world_religions,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
        ]
      },
      {
        "description": "57 matching runs, but no matching metrics",
        "markdown": false
      },
      {
        "description": "57 matching runs, but no matching metrics",
        "markdown": false
      },
      {
        "description": "(0)",
        "style": {},
        "markdown": false,
        "run_spec_names": [
          "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=anatomy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=astronomy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=business_ethics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=college_biology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=college_medicine,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=college_physics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=computer_security,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=econometrics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=formal_logic,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=global_facts,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=human_aging,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=international_law,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=machine_learning,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=management,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=marketing,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=nutrition,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=philosophy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=prehistory,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=professional_law,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=public_relations,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=security_studies,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=sociology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=virology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=world_religions,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
        ]
      },
      {
        "description": "(0)",
        "style": {},
        "markdown": false,
        "run_spec_names": [
          "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=anatomy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=astronomy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=business_ethics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=college_biology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=college_medicine,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=college_physics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=computer_security,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=econometrics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=formal_logic,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=global_facts,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=human_aging,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=international_law,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=machine_learning,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=management,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=marketing,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=nutrition,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=philosophy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=prehistory,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=professional_law,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=public_relations,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=security_studies,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=sociology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=virology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=world_religions,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
        ]
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      }
    ]
  ],
  "links": [
    {
      "text": "LaTeX",
      "href": "benchmark_output/runs/v1/groups/latex/targeted_evaluations_efficiency_detailed.tex"
    },
    {
      "text": "JSON",
      "href": "benchmark_output/runs/v1/groups/json/targeted_evaluations_efficiency_detailed.json"
    }
  ],
  "name": "efficiency_detailed"
}