{
  "title": "Accuracy",
  "header": [
    {
      "value": "Model/adapter",
      "markdown": false,
      "metadata": {}
    },
    {
      "value": "Mean win rate",
      "description": "How many models this model outperform on average (over columns).",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {}
    },
    {
      "value": "The Pile - BPB",
      "description": "The Pile corpus for measuring lanugage model performance across various domains [(Gao et al., 2020)](https://arxiv.org/pdf/2101.00027.pdf).\n\nBits/byte: Average number of bits per byte according to model probabilities.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "BPB",
        "run_group": "The Pile"
      }
    },
    {
      "value": "TwitterAAE - BPB",
      "description": "The TwitterAAE corpus of [Blodgett et al. (2016)](https://aclanthology.org/D16-1120/) for measuring language model performance in tweets as a function of speaker dialect.\n\nBits/byte: Average number of bits per byte according to model probabilities.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "BPB",
        "run_group": "TwitterAAE"
      }
    },
    {
      "value": "ICE - BPB",
      "description": "The International Corpus of English (ICE) drawn from English speakers from various places in the world, initiated by [Greenbaum (1991)](https://www.cambridge.org/core/journals/english-today/article/abs/ice-the-international-corpus-of-english/47808205394C538393C3FD8E62E5E701).\n\nBits/byte: Average number of bits per byte according to model probabilities.",
      "markdown": false,
      "lower_is_better": true,
      "metadata": {
        "metric": "BPB",
        "run_group": "ICE"
      }
    },
    {
      "value": "BLiMP - EM",
      "description": "The Benchmark of Linguistic Minimal Pairs for English (BLiMP) for measuring performance on linguistic phenomena using minimal pair design [(Warstadt et al., 2020)](https://aclanthology.org/2020.tacl-1.25/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "EM",
        "run_group": "BLiMP"
      }
    },
    {
      "value": "NaturalQuestions (closed-book) - F1",
      "description": "The NaturalQuestions [(Kwiatkowski et al., 2019)](https://aclanthology.org/Q19-1026/) benchmark for question answering based on naturally-occurring queries through Google Search. The input does not include the Wikipedia page with the answer.\n\nF1: Average F1 score in terms of word overlap between the model output and correct reference.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "F1",
        "run_group": "NaturalQuestions (closed-book)"
      }
    },
    {
      "value": "HellaSwag - EM",
      "description": "The HellaSwag benchmark for commonsense reasoning in question answering [(Zellers et al., 2019)](https://aclanthology.org/P19-1472/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "EM",
        "run_group": "HellaSwag"
      }
    },
    {
      "value": "OpenbookQA - EM",
      "description": "The OpenbookQA benchmark for commonsense-intensive open book question answering [(Mihaylov et al., 2018)](https://aclanthology.org/D18-1260/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "EM",
        "run_group": "OpenbookQA"
      }
    },
    {
      "value": "TruthfulQA - EM",
      "description": "The TruthfulQA benchmarking for measuring model truthfulness and commonsense knowledge in question answering [(Lin et al., 2022)](https://aclanthology.org/2022.acl-long.229/).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "EM",
        "run_group": "TruthfulQA"
      }
    },
    {
      "value": "MMLU - EM",
      "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "EM",
        "run_group": "MMLU"
      }
    },
    {
      "value": "WikiFact - EM",
      "description": "Scenario introduced in this work, inspired by [Petroni et al. (2019)](https://aclanthology.org/D19-1250/), to more extensively test factual knowledge.\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "EM",
        "run_group": "WikiFact"
      }
    },
    {
      "value": "Synthetic reasoning (abstract symbols) - EM",
      "description": "Synthetic reasoning tasks defined using abstract symbols based on LIME [(Wu et al., 2021)](https://proceedings.mlr.press/v139/wu21c.html).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "EM",
        "run_group": "Synthetic reasoning (abstract symbols)"
      }
    },
    {
      "value": "Synthetic reasoning (natural language) - F1",
      "description": "Synthetic reasoning tasks defined using simple natural language based on LIME [(Wu et al., 2021)](https://proceedings.mlr.press/v139/wu21c.html).\n\nF1 (set match): Average F1 score in terms of set overlap between the model predicted set and correct reference set.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "F1",
        "run_group": "Synthetic reasoning (natural language)"
      }
    },
    {
      "value": "bAbI - EM",
      "description": "The bAbI benchmark for measuring understanding and reasoning [(Weston et al., 2015)](https://arxiv.org/pdf/1502.05698.pdf).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "EM",
        "run_group": "bAbI"
      }
    },
    {
      "value": "Dyck - EM",
      "description": "Scenario testing hierarchical reasoning through the Dyck formal languages [(Suzgun et al., 2019)](https://aclanthology.org/W19-3905/).\n\nExact match (final): Fraction of instances that the predicted output matches a correct reference exactly, ignoring text preceding the specified indicator (e.g., space).",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "EM",
        "run_group": "Dyck"
      }
    },
    {
      "value": "GSM8K - EM",
      "description": "The grade school math word problems dataset (GSM8K) for testing mathematical reasoning on grade-school math problems [(Cobbe et al., 2021)](https://arxiv.org/pdf/2110.14168.pdf).\n\nExact match (final): Fraction of instances that the predicted output matches a correct reference exactly, ignoring text preceding the specified indicator (e.g., space).",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "EM",
        "run_group": "GSM8K"
      }
    },
    {
      "value": "MATH - Equivalent",
      "description": "The MATH benchmark for measuring mathematical problem solving on competition math problems [(Hendrycks et al., 2021)](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html).\n\nEquivalent: Fraction of model outputs that are mathematically equivalent to the correct reference.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "Equivalent",
        "run_group": "MATH"
      }
    },
    {
      "value": "MATH (chain-of-thought) - Equivalent (chain of thought)",
      "description": "The MATH benchmark for measuring mathematical problem solving on competition math problems with chain-of-thought style reasoning [(Hendrycks et al., 2021)](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html).\n\nEquivalent (chain of thought): Fraction of model outputs that are mathematically equivalent to the correct reference when using chain-of-thought prompting.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "Equivalent (chain of thought)",
        "run_group": "MATH (chain-of-thought)"
      }
    },
    {
      "value": "HumanEval (Code) - pass@1",
      "description": "The HumanEval benchmark for measuring functional correctness for synthesizing programs from docstrings [(Chen et al., 2021)](https://arxiv.org/pdf/2107.03374.pdf).\n\npass@1: Fraction of model outputs that pass the associated test cases.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "pass@1",
        "run_group": "HumanEval (Code)"
      }
    },
    {
      "value": "LSAT - EM",
      "description": "The LSAT benchmark for measuring analytical reasoning on the Law School Admission Test (LSAT; [Zhong et al., 2021](https://arxiv.org/pdf/2104.06598.pdf)).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "EM",
        "run_group": "LSAT"
      }
    },
    {
      "value": "LegalSupport - EM",
      "description": "Scenario introduced in this work to measure fine-grained legal reasoning through reverse entailment.\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "EM",
        "run_group": "LegalSupport"
      }
    },
    {
      "value": "Data imputation - EM",
      "description": "Scenario from [Mei et al. (2021)](https://ieeexplore.ieee.org/document/9458712/) that tests the ability to impute missing entities in a data table.\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "EM",
        "run_group": "Data imputation"
      }
    },
    {
      "value": "Entity matching - EM",
      "description": "Scenario from Magellan [(Konda et al., 2016)](https://dl.acm.org/doi/10.14778/3007263.3007314) that tests the ability to determine if two entities match.\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "EM",
        "run_group": "Entity matching"
      }
    },
    {
      "value": "BBQ - EM",
      "description": "The Bias Benchmark for Question Answering (BBQ) for measuring social bias in question answering in ambiguous and unambigous context [(Parrish et al., 2022)](https://aclanthology.org/2022.findings-acl.165/).\n\nQuasi-exact match: Fraction of instances that the predicted output matches a correct reference up to light processing.",
      "markdown": false,
      "lower_is_better": false,
      "metadata": {
        "metric": "EM",
        "run_group": "BBQ"
      }
    }
  ],
  "rows": [
    [
      {
        "value": "Together AI Llama 3 (8B)",
        "description": "",
        "markdown": false
      },
      {
        "value": 0.25,
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "value": 0.5457712478651048,
        "description": "min=0.222, mean=0.546, max=0.944, sum=31.109 (57)",
        "style": {},
        "markdown": false,
        "run_spec_names": [
          "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=anatomy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=astronomy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=business_ethics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=college_biology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=college_medicine,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=college_physics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=computer_security,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=econometrics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=formal_logic,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=global_facts,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=human_aging,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=international_law,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=machine_learning,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=management,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=marketing,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=nutrition,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=philosophy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=prehistory,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=professional_law,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=public_relations,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=security_studies,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=sociology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=virology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
          "mmlu:subject=world_religions,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
        ]
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      }
    ],
    [
      {
        "value": "Together AI Llama 3 (70B)",
        "description": "",
        "markdown": false
      },
      {
        "value": 1.0,
        "style": {
          "font-weight": "bold"
        },
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "value": 0.7306329549363606,
        "description": "min=0.278, mean=0.731, max=1, sum=41.646 (57)",
        "style": {
          "font-weight": "bold"
        },
        "markdown": false,
        "run_spec_names": [
          "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=anatomy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=astronomy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=business_ethics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=college_biology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=college_medicine,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=college_physics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=computer_security,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=econometrics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=formal_logic,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=global_facts,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=human_aging,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=international_law,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=machine_learning,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=management,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=marketing,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=nutrition,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=philosophy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=prehistory,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=professional_law,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=public_relations,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=security_studies,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=sociology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=virology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
          "mmlu:subject=world_religions,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
        ]
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      }
    ],
    [
      {
        "value": "Groq LLaMA 3 (8B)",
        "description": "",
        "markdown": false
      },
      {
        "value": 0.0,
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "value": 0.38060987091050635,
        "description": "min=0.056, mean=0.381, max=0.706, sum=21.695 (57)",
        "style": {},
        "markdown": false,
        "run_spec_names": [
          "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=anatomy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=astronomy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=business_ethics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=college_biology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=college_medicine,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=college_physics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=computer_security,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=econometrics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=formal_logic,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=global_facts,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=human_aging,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=international_law,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=machine_learning,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=management,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=marketing,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=nutrition,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=philosophy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=prehistory,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=professional_law,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=public_relations,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=security_studies,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=sociology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=virology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
          "mmlu:subject=world_religions,method=multiple_choice_joint,model=groq_llama3-8b-8192"
        ]
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      }
    ],
    [
      {
        "value": "Groq LLaMA 3 (70B)",
        "description": "",
        "markdown": false
      },
      {
        "value": 0.75,
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "value": 0.703082215342277,
        "description": "min=0.294, mean=0.703, max=0.944, sum=38.67 (55)",
        "style": {},
        "markdown": false,
        "run_spec_names": [
          "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=anatomy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=astronomy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=business_ethics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=college_biology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=college_medicine,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=college_physics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=computer_security,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=econometrics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=formal_logic,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=global_facts,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=human_aging,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=international_law,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=machine_learning,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=management,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=marketing,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=nutrition,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=philosophy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=prehistory,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=professional_law,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=public_relations,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=security_studies,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=sociology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=virology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
          "mmlu:subject=world_religions,method=multiple_choice_joint,model=groq_llama3-70b-8192"
        ]
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      }
    ],
    [
      {
        "value": "Hugging face LLaMA 3 (8B)(original)",
        "description": "",
        "markdown": false
      },
      {
        "value": 0.5,
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "value": 0.6461130574113303,
        "description": "min=0.176, mean=0.646, max=0.895, sum=36.828 (57)",
        "style": {},
        "markdown": false,
        "run_spec_names": [
          "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=anatomy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=astronomy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=business_ethics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=college_biology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=college_medicine,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=college_physics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=computer_security,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=econometrics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=formal_logic,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=global_facts,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=human_aging,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=international_law,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=machine_learning,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=management,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=marketing,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=nutrition,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=philosophy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=prehistory,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=professional_law,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=public_relations,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=security_studies,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=sociology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=virology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
          "mmlu:subject=world_religions,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
        ]
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      },
      {
        "description": "No matching runs",
        "markdown": false
      }
    ]
  ],
  "links": [
    {
      "text": "LaTeX",
      "href": "benchmark_output/runs/v1/groups/latex/targeted_evaluations_accuracy.tex"
    },
    {
      "text": "JSON",
      "href": "benchmark_output/runs/v1/groups/json/targeted_evaluations_accuracy.json"
    }
  ],
  "name": "accuracy"
}