[
  {
    "title": "MMLU (Massive Multitask Language Understanding)",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "markdown": false
        },
        {
          "value": 0.5457712478651048,
          "description": "min=0.222, mean=0.546, max=0.944, sum=31.109 (57)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=anatomy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=astronomy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=business_ethics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_biology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_medicine,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_physics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=computer_security,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=econometrics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=formal_logic,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=global_facts,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=human_aging,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=international_law,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=machine_learning,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=management,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=marketing,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=nutrition,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=philosophy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=prehistory,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=professional_law,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=public_relations,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=security_studies,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=sociology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=virology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=world_religions,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "description": "57 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.5457712478651048,
          "description": "min=0.222, mean=0.546, max=0.944, sum=31.109 (57)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=anatomy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=astronomy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=business_ethics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_biology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_medicine,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_physics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=computer_security,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=econometrics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=formal_logic,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=global_facts,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=human_aging,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=international_law,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=machine_learning,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=management,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=marketing,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=nutrition,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=philosophy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=prehistory,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=professional_law,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=public_relations,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=security_studies,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=sociology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=virology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=world_religions,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.5457712478651048,
          "description": "min=0.222, mean=0.546, max=0.944, sum=31.109 (57)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=anatomy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=astronomy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=business_ethics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_biology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_medicine,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_physics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=computer_security,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=econometrics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=formal_logic,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=global_facts,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=human_aging,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=international_law,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=machine_learning,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=management,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=marketing,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=nutrition,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=philosophy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=prehistory,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=professional_law,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=public_relations,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=security_studies,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=sociology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=virology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=world_religions,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "description": "57 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.859649122807017,
          "description": "min=16, mean=17.86, max=20, sum=1018 (57)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=anatomy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=astronomy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=business_ethics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_biology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_medicine,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_physics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=computer_security,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=econometrics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=formal_logic,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=global_facts,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=human_aging,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=international_law,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=machine_learning,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=management,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=marketing,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=nutrition,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=philosophy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=prehistory,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=professional_law,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=public_relations,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=security_studies,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=sociology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=virology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=world_religions,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 4.946393762183236,
          "description": "min=2.944, mean=4.946, max=5, sum=281.944 (57)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=anatomy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=astronomy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=business_ethics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_biology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_medicine,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_physics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=computer_security,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=econometrics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=formal_logic,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=global_facts,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=human_aging,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=international_law,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=machine_learning,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=management,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=marketing,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=nutrition,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=philosophy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=prehistory,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=professional_law,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=public_relations,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=security_studies,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=sociology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=virology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=world_religions,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (57)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=anatomy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=astronomy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=business_ethics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_biology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_medicine,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_physics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=computer_security,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=econometrics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=formal_logic,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=global_facts,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=human_aging,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=international_law,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=machine_learning,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=management,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=marketing,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=nutrition,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=philosophy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=prehistory,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=professional_law,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=public_relations,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=security_studies,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=sociology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=virology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=world_religions,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 624.5755288203309,
          "description": "min=309.059, mean=624.576, max=1769.444, sum=35600.805 (57)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=anatomy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=astronomy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=business_ethics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_biology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_medicine,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_physics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=computer_security,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=econometrics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=formal_logic,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=global_facts,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=human_aging,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=international_law,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=machine_learning,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=management,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=marketing,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=nutrition,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=philosophy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=prehistory,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=professional_law,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=public_relations,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=security_studies,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=sociology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=virology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=world_religions,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=57 (57)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=anatomy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=astronomy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=business_ethics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_biology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_medicine,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_physics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=computer_security,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=econometrics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=formal_logic,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=global_facts,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=human_aging,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=international_law,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=machine_learning,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=management,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=marketing,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=nutrition,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=philosophy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=prehistory,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=professional_law,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=public_relations,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=security_studies,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=sociology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=virology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=world_religions,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=57 (57)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=anatomy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=astronomy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=business_ethics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_biology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_medicine,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=college_physics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=computer_security,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=econometrics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=formal_logic,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=global_facts,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=human_aging,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=international_law,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=machine_learning,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=management,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=marketing,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=nutrition,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=philosophy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=prehistory,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=professional_law,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=public_relations,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=security_studies,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=sociology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=virology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf",
            "mmlu:subject=world_religions,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "markdown": false
        },
        {
          "value": 0.7306329549363606,
          "description": "min=0.278, mean=0.731, max=1, sum=41.646 (57)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=anatomy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=astronomy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=business_ethics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_biology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_medicine,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_physics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=computer_security,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=econometrics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=formal_logic,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=global_facts,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=human_aging,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=international_law,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=machine_learning,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=management,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=marketing,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=nutrition,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=philosophy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=prehistory,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=professional_law,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=public_relations,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=security_studies,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=sociology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=virology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=world_religions,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "description": "57 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.7306329549363606,
          "description": "min=0.278, mean=0.731, max=1, sum=41.646 (57)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=anatomy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=astronomy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=business_ethics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_biology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_medicine,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_physics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=computer_security,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=econometrics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=formal_logic,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=global_facts,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=human_aging,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=international_law,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=machine_learning,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=management,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=marketing,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=nutrition,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=philosophy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=prehistory,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=professional_law,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=public_relations,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=security_studies,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=sociology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=virology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=world_religions,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.7306329549363606,
          "description": "min=0.278, mean=0.731, max=1, sum=41.646 (57)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=anatomy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=astronomy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=business_ethics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_biology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_medicine,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_physics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=computer_security,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=econometrics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=formal_logic,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=global_facts,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=human_aging,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=international_law,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=machine_learning,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=management,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=marketing,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=nutrition,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=philosophy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=prehistory,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=professional_law,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=public_relations,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=security_studies,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=sociology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=virology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=world_religions,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "description": "57 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.859649122807017,
          "description": "min=16, mean=17.86, max=20, sum=1018 (57)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=anatomy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=astronomy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=business_ethics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_biology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_medicine,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_physics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=computer_security,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=econometrics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=formal_logic,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=global_facts,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=human_aging,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=international_law,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=machine_learning,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=management,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=marketing,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=nutrition,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=philosophy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=prehistory,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=professional_law,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=public_relations,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=security_studies,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=sociology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=virology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=world_religions,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 4.946393762183236,
          "description": "min=2.944, mean=4.946, max=5, sum=281.944 (57)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=anatomy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=astronomy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=business_ethics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_biology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_medicine,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_physics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=computer_security,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=econometrics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=formal_logic,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=global_facts,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=human_aging,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=international_law,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=machine_learning,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=management,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=marketing,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=nutrition,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=philosophy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=prehistory,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=professional_law,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=public_relations,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=security_studies,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=sociology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=virology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=world_religions,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (57)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=anatomy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=astronomy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=business_ethics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_biology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_medicine,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_physics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=computer_security,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=econometrics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=formal_logic,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=global_facts,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=human_aging,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=international_law,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=machine_learning,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=management,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=marketing,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=nutrition,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=philosophy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=prehistory,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=professional_law,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=public_relations,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=security_studies,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=sociology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=virology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=world_religions,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 624.5755288203309,
          "description": "min=309.059, mean=624.576, max=1769.444, sum=35600.805 (57)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=anatomy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=astronomy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=business_ethics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_biology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_medicine,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_physics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=computer_security,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=econometrics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=formal_logic,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=global_facts,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=human_aging,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=international_law,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=machine_learning,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=management,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=marketing,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=nutrition,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=philosophy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=prehistory,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=professional_law,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=public_relations,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=security_studies,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=sociology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=virology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=world_religions,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=57 (57)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=anatomy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=astronomy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=business_ethics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_biology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_medicine,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_physics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=computer_security,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=econometrics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=formal_logic,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=global_facts,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=human_aging,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=international_law,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=machine_learning,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=management,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=marketing,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=nutrition,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=philosophy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=prehistory,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=professional_law,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=public_relations,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=security_studies,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=sociology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=virology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=world_religions,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=57 (57)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=anatomy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=astronomy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=business_ethics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_biology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_medicine,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=college_physics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=computer_security,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=econometrics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=formal_logic,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=global_facts,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=human_aging,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=international_law,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=machine_learning,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=management,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=marketing,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=nutrition,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=philosophy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=prehistory,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=professional_law,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=public_relations,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=security_studies,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=sociology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=virology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
            "mmlu:subject=world_religions,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "markdown": false
        },
        {
          "value": 0.38060987091050635,
          "description": "min=0.056, mean=0.381, max=0.706, sum=21.695 (57)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=anatomy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=astronomy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=business_ethics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_biology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_medicine,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_physics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=computer_security,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=econometrics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=formal_logic,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=global_facts,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=human_aging,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=international_law,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=machine_learning,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=management,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=marketing,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=nutrition,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=philosophy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=prehistory,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=professional_law,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=public_relations,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=security_studies,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=sociology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=virology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=world_religions,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "description": "57 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.38060987091050635,
          "description": "min=0.056, mean=0.381, max=0.706, sum=21.695 (57)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=anatomy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=astronomy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=business_ethics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_biology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_medicine,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_physics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=computer_security,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=econometrics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=formal_logic,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=global_facts,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=human_aging,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=international_law,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=machine_learning,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=management,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=marketing,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=nutrition,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=philosophy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=prehistory,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=professional_law,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=public_relations,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=security_studies,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=sociology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=virology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=world_religions,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.38060987091050635,
          "description": "min=0.056, mean=0.381, max=0.706, sum=21.695 (57)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=anatomy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=astronomy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=business_ethics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_biology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_medicine,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_physics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=computer_security,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=econometrics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=formal_logic,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=global_facts,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=human_aging,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=international_law,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=machine_learning,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=management,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=marketing,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=nutrition,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=philosophy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=prehistory,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=professional_law,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=public_relations,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=security_studies,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=sociology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=virology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=world_religions,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "description": "57 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.859649122807017,
          "description": "min=16, mean=17.86, max=20, sum=1018 (57)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=anatomy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=astronomy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=business_ethics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_biology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_medicine,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_physics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=computer_security,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=econometrics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=formal_logic,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=global_facts,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=human_aging,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=international_law,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=machine_learning,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=management,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=marketing,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=nutrition,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=philosophy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=prehistory,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=professional_law,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=public_relations,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=security_studies,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=sociology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=virology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=world_religions,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=285 (57)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=anatomy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=astronomy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=business_ethics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_biology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_medicine,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_physics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=computer_security,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=econometrics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=formal_logic,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=global_facts,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=human_aging,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=international_law,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=machine_learning,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=management,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=marketing,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=nutrition,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=philosophy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=prehistory,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=professional_law,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=public_relations,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=security_studies,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=sociology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=virology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=world_religions,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (57)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=anatomy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=astronomy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=business_ethics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_biology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_medicine,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_physics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=computer_security,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=econometrics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=formal_logic,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=global_facts,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=human_aging,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=international_law,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=machine_learning,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=management,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=marketing,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=nutrition,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=philosophy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=prehistory,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=professional_law,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=public_relations,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=security_studies,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=sociology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=virology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=world_religions,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 650.7295249216954,
          "description": "min=309.059, mean=650.73, max=2784.222, sum=37091.583 (57)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=anatomy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=astronomy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=business_ethics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_biology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_medicine,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_physics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=computer_security,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=econometrics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=formal_logic,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=global_facts,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=human_aging,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=international_law,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=machine_learning,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=management,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=marketing,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=nutrition,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=philosophy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=prehistory,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=professional_law,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=public_relations,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=security_studies,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=sociology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=virology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=world_religions,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 1.4342029825164908,
          "description": "min=1, mean=1.434, max=8.389, sum=81.75 (57)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=anatomy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=astronomy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=business_ethics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_biology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_medicine,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_physics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=computer_security,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=econometrics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=formal_logic,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=global_facts,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=human_aging,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=international_law,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=machine_learning,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=management,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=marketing,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=nutrition,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=philosophy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=prehistory,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=professional_law,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=public_relations,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=security_studies,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=sociology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=virology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=world_religions,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=57 (57)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=anatomy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=astronomy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=business_ethics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_biology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_medicine,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=college_physics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=computer_security,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=econometrics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=formal_logic,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=global_facts,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=human_aging,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=international_law,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=machine_learning,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=management,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=marketing,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=nutrition,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=philosophy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=prehistory,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=professional_law,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=public_relations,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=security_studies,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=sociology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=virology,method=multiple_choice_joint,model=groq_llama3-8b-8192",
            "mmlu:subject=world_religions,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "markdown": false
        },
        {
          "value": 0.703082215342277,
          "description": "min=0.294, mean=0.703, max=0.944, sum=38.67 (55)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=anatomy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=astronomy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=business_ethics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_biology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_medicine,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_physics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=computer_security,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=econometrics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=formal_logic,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=global_facts,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=human_aging,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=international_law,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=machine_learning,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=management,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=marketing,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=nutrition,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=philosophy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=prehistory,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=professional_law,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=public_relations,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=security_studies,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=sociology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=virology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=world_religions,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "description": "55 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.703082215342277,
          "description": "min=0.294, mean=0.703, max=0.944, sum=38.67 (55)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=anatomy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=astronomy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=business_ethics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_biology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_medicine,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_physics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=computer_security,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=econometrics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=formal_logic,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=global_facts,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=human_aging,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=international_law,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=machine_learning,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=management,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=marketing,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=nutrition,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=philosophy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=prehistory,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=professional_law,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=public_relations,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=security_studies,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=sociology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=virology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=world_religions,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.703082215342277,
          "description": "min=0.294, mean=0.703, max=0.944, sum=38.67 (55)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=anatomy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=astronomy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=business_ethics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_biology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_medicine,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_physics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=computer_security,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=econometrics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=formal_logic,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=global_facts,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=human_aging,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=international_law,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=machine_learning,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=management,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=marketing,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=nutrition,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=philosophy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=prehistory,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=professional_law,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=public_relations,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=security_studies,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=sociology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=virology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=world_religions,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "description": "55 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.836363636363636,
          "description": "min=16, mean=17.836, max=20, sum=981 (55)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=anatomy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=astronomy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=business_ethics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_biology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_medicine,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_physics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=computer_security,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=econometrics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=formal_logic,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=global_facts,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=human_aging,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=international_law,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=machine_learning,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=management,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=marketing,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=nutrition,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=philosophy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=prehistory,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=professional_law,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=public_relations,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=security_studies,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=sociology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=virology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=world_religions,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=275 (55)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=anatomy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=astronomy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=business_ethics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_biology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_medicine,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_physics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=computer_security,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=econometrics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=formal_logic,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=global_facts,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=human_aging,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=international_law,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=machine_learning,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=management,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=marketing,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=nutrition,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=philosophy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=prehistory,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=professional_law,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=public_relations,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=security_studies,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=sociology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=virology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=world_religions,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (55)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=anatomy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=astronomy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=business_ethics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_biology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_medicine,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_physics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=computer_security,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=econometrics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=formal_logic,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=global_facts,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=human_aging,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=international_law,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=machine_learning,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=management,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=marketing,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=nutrition,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=philosophy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=prehistory,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=professional_law,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=public_relations,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=security_studies,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=sociology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=virology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=world_religions,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 596.9089504956688,
          "description": "min=309.059, mean=596.909, max=2209.278, sum=32829.992 (55)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=anatomy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=astronomy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=business_ethics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_biology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_medicine,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_physics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=computer_security,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=econometrics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=formal_logic,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=global_facts,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=human_aging,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=international_law,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=machine_learning,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=management,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=marketing,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=nutrition,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=philosophy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=prehistory,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=professional_law,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=public_relations,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=security_studies,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=sociology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=virology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=world_religions,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 1.0010101010101011,
          "description": "min=1, mean=1.001, max=1.056, sum=55.056 (55)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=anatomy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=astronomy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=business_ethics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_biology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_medicine,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_physics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=computer_security,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=econometrics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=formal_logic,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=global_facts,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=human_aging,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=international_law,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=machine_learning,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=management,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=marketing,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=nutrition,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=philosophy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=prehistory,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=professional_law,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=public_relations,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=security_studies,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=sociology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=virology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=world_religions,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=55 (55)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=anatomy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=astronomy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=business_ethics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_biology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_medicine,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=college_physics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=computer_security,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=econometrics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=formal_logic,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=global_facts,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=human_aging,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=international_law,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=machine_learning,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=management,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=marketing,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=nutrition,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=philosophy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=prehistory,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=professional_law,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=public_relations,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=security_studies,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=sociology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=virology,method=multiple_choice_joint,model=groq_llama3-70b-8192",
            "mmlu:subject=world_religions,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "markdown": false
        },
        {
          "value": 0.6461130574113303,
          "description": "min=0.176, mean=0.646, max=0.895, sum=36.828 (57)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=anatomy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=astronomy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=business_ethics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_biology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_medicine,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_physics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=computer_security,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=econometrics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=formal_logic,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=global_facts,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=human_aging,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=international_law,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=machine_learning,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=management,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=marketing,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=nutrition,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=philosophy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=prehistory,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=professional_law,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=public_relations,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=security_studies,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=sociology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=virology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=world_religions,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "description": "57 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.6461130574113303,
          "description": "min=0.176, mean=0.646, max=0.895, sum=36.828 (57)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=anatomy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=astronomy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=business_ethics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_biology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_medicine,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_physics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=computer_security,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=econometrics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=formal_logic,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=global_facts,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=human_aging,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=international_law,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=machine_learning,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=management,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=marketing,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=nutrition,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=philosophy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=prehistory,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=professional_law,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=public_relations,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=security_studies,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=sociology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=virology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=world_religions,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.6461130574113303,
          "description": "min=0.176, mean=0.646, max=0.895, sum=36.828 (57)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=anatomy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=astronomy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=business_ethics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_biology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_medicine,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_physics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=computer_security,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=econometrics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=formal_logic,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=global_facts,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=human_aging,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=international_law,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=machine_learning,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=management,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=marketing,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=nutrition,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=philosophy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=prehistory,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=professional_law,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=public_relations,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=security_studies,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=sociology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=virology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=world_religions,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "description": "57 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.859649122807017,
          "description": "min=16, mean=17.86, max=20, sum=1018 (57)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=anatomy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=astronomy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=business_ethics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_biology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_medicine,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_physics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=computer_security,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=econometrics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=formal_logic,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=global_facts,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=human_aging,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=international_law,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=machine_learning,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=management,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=marketing,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=nutrition,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=philosophy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=prehistory,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=professional_law,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=public_relations,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=security_studies,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=sociology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=virology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=world_religions,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 4.948343079922027,
          "description": "min=3, mean=4.948, max=5, sum=282.056 (57)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=anatomy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=astronomy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=business_ethics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_biology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_medicine,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_physics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=computer_security,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=econometrics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=formal_logic,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=global_facts,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=human_aging,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=international_law,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=machine_learning,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=management,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=marketing,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=nutrition,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=philosophy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=prehistory,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=professional_law,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=public_relations,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=security_studies,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=sociology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=virology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=world_religions,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (57)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=anatomy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=astronomy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=business_ethics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_biology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_medicine,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_physics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=computer_security,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=econometrics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=formal_logic,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=global_facts,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=human_aging,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=international_law,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=machine_learning,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=management,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=marketing,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=nutrition,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=philosophy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=prehistory,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=professional_law,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=public_relations,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=security_studies,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=sociology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=virology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=world_religions,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 581.4741642979137,
          "description": "min=265.059, mean=581.474, max=1750.222, sum=33144.027 (57)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=anatomy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=astronomy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=business_ethics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_biology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_medicine,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_physics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=computer_security,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=econometrics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=formal_logic,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=global_facts,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=human_aging,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=international_law,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=machine_learning,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=management,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=marketing,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=nutrition,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=philosophy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=prehistory,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=professional_law,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=public_relations,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=security_studies,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=sociology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=virology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=world_religions,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=57 (57)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=anatomy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=astronomy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=business_ethics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_biology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_medicine,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_physics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=computer_security,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=econometrics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=formal_logic,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=global_facts,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=human_aging,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=international_law,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=machine_learning,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=management,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=marketing,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=nutrition,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=philosophy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=prehistory,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=professional_law,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=public_relations,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=security_studies,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=sociology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=virology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=world_religions,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=57 (57)",
          "style": {},
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=anatomy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=astronomy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=business_ethics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_biology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_medicine,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=college_physics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=computer_security,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=econometrics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=formal_logic,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=global_facts,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=human_aging,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=international_law,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=machine_learning,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=management,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=marketing,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=nutrition,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=philosophy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=prehistory,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=professional_law,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=public_relations,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=security_studies,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=sociology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=virology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B",
            "mmlu:subject=world_religions,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        }
      ]
    ],
    "links": [
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu.json"
      }
    ],
    "name": "mmlu"
  },
  {
    "title": "subject: abstract_algebra",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20abstract_algebra&runSpecs=%5B%22mmlu%3Asubject%3Dabstract_algebra%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.4117647058823529,
          "description": "min=0.412, mean=0.412, max=0.412, sum=0.412 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.4117647058823529,
          "description": "min=0.412, mean=0.412, max=0.412, sum=0.412 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.4117647058823529,
          "description": "min=0.412, mean=0.412, max=0.412, sum=0.412 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 413.3529411764706,
          "description": "min=413.353, mean=413.353, max=413.353, sum=413.353 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20abstract_algebra&runSpecs=%5B%22mmlu%3Asubject%3Dabstract_algebra%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.5294117647058824,
          "description": "min=0.529, mean=0.529, max=0.529, sum=0.529 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.5294117647058824,
          "description": "min=0.529, mean=0.529, max=0.529, sum=0.529 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.5294117647058824,
          "description": "min=0.529, mean=0.529, max=0.529, sum=0.529 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 413.3529411764706,
          "description": "min=413.353, mean=413.353, max=413.353, sum=413.353 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20abstract_algebra&runSpecs=%5B%22mmlu%3Asubject%3Dabstract_algebra%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.29411764705882354,
          "description": "min=0.294, mean=0.294, max=0.294, sum=0.294 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.29411764705882354,
          "description": "min=0.294, mean=0.294, max=0.294, sum=0.294 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.29411764705882354,
          "description": "min=0.294, mean=0.294, max=0.294, sum=0.294 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 413.3529411764706,
          "description": "min=413.353, mean=413.353, max=413.353, sum=413.353 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 2.1176470588235294,
          "description": "min=2.118, mean=2.118, max=2.118, sum=2.118 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20abstract_algebra&runSpecs=%5B%22mmlu%3Asubject%3Dabstract_algebra%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.35294117647058826,
          "description": "min=0.353, mean=0.353, max=0.353, sum=0.353 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.35294117647058826,
          "description": "min=0.353, mean=0.353, max=0.353, sum=0.353 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.35294117647058826,
          "description": "min=0.353, mean=0.353, max=0.353, sum=0.353 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 413.3529411764706,
          "description": "min=413.353, mean=413.353, max=413.353, sum=413.353 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20abstract_algebra&runSpecs=%5B%22mmlu%3Asubject%3Dabstract_algebra%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.35294117647058826,
          "description": "min=0.353, mean=0.353, max=0.353, sum=0.353 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.35294117647058826,
          "description": "min=0.353, mean=0.353, max=0.353, sum=0.353 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.35294117647058826,
          "description": "min=0.353, mean=0.353, max=0.353, sum=0.353 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 369.3529411764706,
          "description": "min=369.353, mean=369.353, max=369.353, sum=369.353 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20abstract_algebra&runSpecs=%5B%22mmlu%3Asubject%3Dabstract_algebra%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Dabstract_algebra%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Dabstract_algebra%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Dabstract_algebra%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Dabstract_algebra%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:abstract_algebra.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:abstract_algebra.json"
      }
    ],
    "name": "mmlu_subject:abstract_algebra"
  },
  {
    "title": "subject: anatomy",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20anatomy&runSpecs=%5B%22mmlu%3Asubject%3Danatomy%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=anatomy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.631578947368421,
          "description": "min=0.632, mean=0.632, max=0.632, sum=0.632 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.631578947368421,
          "description": "min=0.632, mean=0.632, max=0.632, sum=0.632 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.631578947368421,
          "description": "min=0.632, mean=0.632, max=0.632, sum=0.632 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 392.2631578947368,
          "description": "min=392.263, mean=392.263, max=392.263, sum=392.263 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20anatomy&runSpecs=%5B%22mmlu%3Asubject%3Danatomy%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=anatomy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.5789473684210527,
          "description": "min=0.579, mean=0.579, max=0.579, sum=0.579 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.5789473684210527,
          "description": "min=0.579, mean=0.579, max=0.579, sum=0.579 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.5789473684210527,
          "description": "min=0.579, mean=0.579, max=0.579, sum=0.579 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 392.2631578947368,
          "description": "min=392.263, mean=392.263, max=392.263, sum=392.263 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20anatomy&runSpecs=%5B%22mmlu%3Asubject%3Danatomy%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=anatomy,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.3684210526315789,
          "description": "min=0.368, mean=0.368, max=0.368, sum=0.368 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.3684210526315789,
          "description": "min=0.368, mean=0.368, max=0.368, sum=0.368 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.3684210526315789,
          "description": "min=0.368, mean=0.368, max=0.368, sum=0.368 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 392.2631578947368,
          "description": "min=392.263, mean=392.263, max=392.263, sum=392.263 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20anatomy&runSpecs=%5B%22mmlu%3Asubject%3Danatomy%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=anatomy,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.6842105263157895,
          "description": "min=0.684, mean=0.684, max=0.684, sum=0.684 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.6842105263157895,
          "description": "min=0.684, mean=0.684, max=0.684, sum=0.684 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.6842105263157895,
          "description": "min=0.684, mean=0.684, max=0.684, sum=0.684 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 392.2631578947368,
          "description": "min=392.263, mean=392.263, max=392.263, sum=392.263 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20anatomy&runSpecs=%5B%22mmlu%3Asubject%3Danatomy%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=anatomy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.5789473684210527,
          "description": "min=0.579, mean=0.579, max=0.579, sum=0.579 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.5789473684210527,
          "description": "min=0.579, mean=0.579, max=0.579, sum=0.579 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.5789473684210527,
          "description": "min=0.579, mean=0.579, max=0.579, sum=0.579 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 348.2631578947368,
          "description": "min=348.263, mean=348.263, max=348.263, sum=348.263 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20anatomy&runSpecs=%5B%22mmlu%3Asubject%3Danatomy%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Danatomy%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Danatomy%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Danatomy%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Danatomy%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:anatomy.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:anatomy.json"
      }
    ],
    "name": "mmlu_subject:anatomy"
  },
  {
    "title": "subject: astronomy",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20astronomy&runSpecs=%5B%22mmlu%3Asubject%3Dastronomy%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=astronomy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.5,
          "description": "min=0.5, mean=0.5, max=0.5, sum=0.5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.5,
          "description": "min=0.5, mean=0.5, max=0.5, sum=0.5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.5,
          "description": "min=0.5, mean=0.5, max=0.5, sum=0.5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 623.1666666666666,
          "description": "min=623.167, mean=623.167, max=623.167, sum=623.167 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20astronomy&runSpecs=%5B%22mmlu%3Asubject%3Dastronomy%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=astronomy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.8333333333333334,
          "description": "min=0.833, mean=0.833, max=0.833, sum=0.833 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.8333333333333334,
          "description": "min=0.833, mean=0.833, max=0.833, sum=0.833 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.8333333333333334,
          "description": "min=0.833, mean=0.833, max=0.833, sum=0.833 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 623.1666666666666,
          "description": "min=623.167, mean=623.167, max=623.167, sum=623.167 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20astronomy&runSpecs=%5B%22mmlu%3Asubject%3Dastronomy%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=astronomy,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.3888888888888889,
          "description": "min=0.389, mean=0.389, max=0.389, sum=0.389 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.3888888888888889,
          "description": "min=0.389, mean=0.389, max=0.389, sum=0.389 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.3888888888888889,
          "description": "min=0.389, mean=0.389, max=0.389, sum=0.389 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 623.1666666666666,
          "description": "min=623.167, mean=623.167, max=623.167, sum=623.167 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.2222222222222223,
          "description": "min=1.222, mean=1.222, max=1.222, sum=1.222 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20astronomy&runSpecs=%5B%22mmlu%3Asubject%3Dastronomy%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=astronomy,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.7777777777777778,
          "description": "min=0.778, mean=0.778, max=0.778, sum=0.778 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.7777777777777778,
          "description": "min=0.778, mean=0.778, max=0.778, sum=0.778 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.7777777777777778,
          "description": "min=0.778, mean=0.778, max=0.778, sum=0.778 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 623.1666666666666,
          "description": "min=623.167, mean=623.167, max=623.167, sum=623.167 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20astronomy&runSpecs=%5B%22mmlu%3Asubject%3Dastronomy%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=astronomy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.7777777777777778,
          "description": "min=0.778, mean=0.778, max=0.778, sum=0.778 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.7777777777777778,
          "description": "min=0.778, mean=0.778, max=0.778, sum=0.778 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.7777777777777778,
          "description": "min=0.778, mean=0.778, max=0.778, sum=0.778 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 579.1666666666666,
          "description": "min=579.167, mean=579.167, max=579.167, sum=579.167 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20astronomy&runSpecs=%5B%22mmlu%3Asubject%3Dastronomy%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Dastronomy%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Dastronomy%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Dastronomy%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Dastronomy%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:astronomy.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:astronomy.json"
      }
    ],
    "name": "mmlu_subject:astronomy"
  },
  {
    "title": "subject: business_ethics",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20business_ethics&runSpecs=%5B%22mmlu%3Asubject%3Dbusiness_ethics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=business_ethics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.5294117647058824,
          "description": "min=0.529, mean=0.529, max=0.529, sum=0.529 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.5294117647058824,
          "description": "min=0.529, mean=0.529, max=0.529, sum=0.529 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.5294117647058824,
          "description": "min=0.529, mean=0.529, max=0.529, sum=0.529 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 604.0588235294117,
          "description": "min=604.059, mean=604.059, max=604.059, sum=604.059 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20business_ethics&runSpecs=%5B%22mmlu%3Asubject%3Dbusiness_ethics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=business_ethics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.8823529411764706,
          "description": "min=0.882, mean=0.882, max=0.882, sum=0.882 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.8823529411764706,
          "description": "min=0.882, mean=0.882, max=0.882, sum=0.882 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.8823529411764706,
          "description": "min=0.882, mean=0.882, max=0.882, sum=0.882 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 604.0588235294117,
          "description": "min=604.059, mean=604.059, max=604.059, sum=604.059 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20business_ethics&runSpecs=%5B%22mmlu%3Asubject%3Dbusiness_ethics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=business_ethics,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.35294117647058826,
          "description": "min=0.353, mean=0.353, max=0.353, sum=0.353 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.35294117647058826,
          "description": "min=0.353, mean=0.353, max=0.353, sum=0.353 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.35294117647058826,
          "description": "min=0.353, mean=0.353, max=0.353, sum=0.353 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 604.0588235294117,
          "description": "min=604.059, mean=604.059, max=604.059, sum=604.059 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.5294117647058822,
          "description": "min=1.529, mean=1.529, max=1.529, sum=1.529 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20business_ethics&runSpecs=%5B%22mmlu%3Asubject%3Dbusiness_ethics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=business_ethics,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.8235294117647058,
          "description": "min=0.824, mean=0.824, max=0.824, sum=0.824 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.8235294117647058,
          "description": "min=0.824, mean=0.824, max=0.824, sum=0.824 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.8235294117647058,
          "description": "min=0.824, mean=0.824, max=0.824, sum=0.824 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 604.0588235294117,
          "description": "min=604.059, mean=604.059, max=604.059, sum=604.059 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20business_ethics&runSpecs=%5B%22mmlu%3Asubject%3Dbusiness_ethics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=business_ethics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.5294117647058824,
          "description": "min=0.529, mean=0.529, max=0.529, sum=0.529 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.5294117647058824,
          "description": "min=0.529, mean=0.529, max=0.529, sum=0.529 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.5294117647058824,
          "description": "min=0.529, mean=0.529, max=0.529, sum=0.529 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 560.0588235294117,
          "description": "min=560.059, mean=560.059, max=560.059, sum=560.059 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20business_ethics&runSpecs=%5B%22mmlu%3Asubject%3Dbusiness_ethics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Dbusiness_ethics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Dbusiness_ethics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Dbusiness_ethics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Dbusiness_ethics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:business_ethics.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:business_ethics.json"
      }
    ],
    "name": "mmlu_subject:business_ethics"
  },
  {
    "title": "subject: clinical_knowledge",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20clinical_knowledge&runSpecs=%5B%22mmlu%3Asubject%3Dclinical_knowledge%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.47058823529411764,
          "description": "min=0.471, mean=0.471, max=0.471, sum=0.471 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.47058823529411764,
          "description": "min=0.471, mean=0.471, max=0.471, sum=0.471 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.47058823529411764,
          "description": "min=0.471, mean=0.471, max=0.471, sum=0.471 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 437.4117647058824,
          "description": "min=437.412, mean=437.412, max=437.412, sum=437.412 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20clinical_knowledge&runSpecs=%5B%22mmlu%3Asubject%3Dclinical_knowledge%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.8235294117647058,
          "description": "min=0.824, mean=0.824, max=0.824, sum=0.824 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.8235294117647058,
          "description": "min=0.824, mean=0.824, max=0.824, sum=0.824 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.8235294117647058,
          "description": "min=0.824, mean=0.824, max=0.824, sum=0.824 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 437.4117647058824,
          "description": "min=437.412, mean=437.412, max=437.412, sum=437.412 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20clinical_knowledge&runSpecs=%5B%22mmlu%3Asubject%3Dclinical_knowledge%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.4117647058823529,
          "description": "min=0.412, mean=0.412, max=0.412, sum=0.412 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.4117647058823529,
          "description": "min=0.412, mean=0.412, max=0.412, sum=0.412 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.4117647058823529,
          "description": "min=0.412, mean=0.412, max=0.412, sum=0.412 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 437.4117647058824,
          "description": "min=437.412, mean=437.412, max=437.412, sum=437.412 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.588235294117647,
          "description": "min=1.588, mean=1.588, max=1.588, sum=1.588 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20clinical_knowledge&runSpecs=%5B%22mmlu%3Asubject%3Dclinical_knowledge%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.6470588235294118,
          "description": "min=0.647, mean=0.647, max=0.647, sum=0.647 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.6470588235294118,
          "description": "min=0.647, mean=0.647, max=0.647, sum=0.647 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.6470588235294118,
          "description": "min=0.647, mean=0.647, max=0.647, sum=0.647 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 437.4117647058824,
          "description": "min=437.412, mean=437.412, max=437.412, sum=437.412 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20clinical_knowledge&runSpecs=%5B%22mmlu%3Asubject%3Dclinical_knowledge%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.6470588235294118,
          "description": "min=0.647, mean=0.647, max=0.647, sum=0.647 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.6470588235294118,
          "description": "min=0.647, mean=0.647, max=0.647, sum=0.647 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.6470588235294118,
          "description": "min=0.647, mean=0.647, max=0.647, sum=0.647 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 393.4117647058824,
          "description": "min=393.412, mean=393.412, max=393.412, sum=393.412 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20clinical_knowledge&runSpecs=%5B%22mmlu%3Asubject%3Dclinical_knowledge%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Dclinical_knowledge%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Dclinical_knowledge%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Dclinical_knowledge%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Dclinical_knowledge%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:clinical_knowledge.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:clinical_knowledge.json"
      }
    ],
    "name": "mmlu_subject:clinical_knowledge"
  },
  {
    "title": "subject: college_biology",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20college_biology&runSpecs=%5B%22mmlu%3Asubject%3Dcollege_biology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=college_biology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.631578947368421,
          "description": "min=0.632, mean=0.632, max=0.632, sum=0.632 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.631578947368421,
          "description": "min=0.632, mean=0.632, max=0.632, sum=0.632 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.631578947368421,
          "description": "min=0.632, mean=0.632, max=0.632, sum=0.632 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 509.2631578947368,
          "description": "min=509.263, mean=509.263, max=509.263, sum=509.263 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20college_biology&runSpecs=%5B%22mmlu%3Asubject%3Dcollege_biology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=college_biology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.7894736842105263,
          "description": "min=0.789, mean=0.789, max=0.789, sum=0.789 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.7894736842105263,
          "description": "min=0.789, mean=0.789, max=0.789, sum=0.789 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.7894736842105263,
          "description": "min=0.789, mean=0.789, max=0.789, sum=0.789 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 509.2631578947368,
          "description": "min=509.263, mean=509.263, max=509.263, sum=509.263 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20college_biology&runSpecs=%5B%22mmlu%3Asubject%3Dcollege_biology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=college_biology,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.2631578947368421,
          "description": "min=0.263, mean=0.263, max=0.263, sum=0.263 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.2631578947368421,
          "description": "min=0.263, mean=0.263, max=0.263, sum=0.263 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.2631578947368421,
          "description": "min=0.263, mean=0.263, max=0.263, sum=0.263 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 509.2631578947368,
          "description": "min=509.263, mean=509.263, max=509.263, sum=509.263 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20college_biology&runSpecs=%5B%22mmlu%3Asubject%3Dcollege_biology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=college_biology,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.7894736842105263,
          "description": "min=0.789, mean=0.789, max=0.789, sum=0.789 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.7894736842105263,
          "description": "min=0.789, mean=0.789, max=0.789, sum=0.789 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.7894736842105263,
          "description": "min=0.789, mean=0.789, max=0.789, sum=0.789 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 509.2631578947368,
          "description": "min=509.263, mean=509.263, max=509.263, sum=509.263 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20college_biology&runSpecs=%5B%22mmlu%3Asubject%3Dcollege_biology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=college_biology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.8947368421052632,
          "description": "min=0.895, mean=0.895, max=0.895, sum=0.895 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.8947368421052632,
          "description": "min=0.895, mean=0.895, max=0.895, sum=0.895 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.8947368421052632,
          "description": "min=0.895, mean=0.895, max=0.895, sum=0.895 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 465.2631578947368,
          "description": "min=465.263, mean=465.263, max=465.263, sum=465.263 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20college_biology&runSpecs=%5B%22mmlu%3Asubject%3Dcollege_biology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Dcollege_biology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Dcollege_biology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Dcollege_biology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Dcollege_biology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:college_biology.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:college_biology.json"
      }
    ],
    "name": "mmlu_subject:college_biology"
  },
  {
    "title": "subject: college_chemistry",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20college_chemistry&runSpecs=%5B%22mmlu%3Asubject%3Dcollege_chemistry%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.3888888888888889,
          "description": "min=0.389, mean=0.389, max=0.389, sum=0.389 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.3888888888888889,
          "description": "min=0.389, mean=0.389, max=0.389, sum=0.389 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.3888888888888889,
          "description": "min=0.389, mean=0.389, max=0.389, sum=0.389 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 590.0,
          "description": "min=590, mean=590, max=590, sum=590 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20college_chemistry&runSpecs=%5B%22mmlu%3Asubject%3Dcollege_chemistry%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.5,
          "description": "min=0.5, mean=0.5, max=0.5, sum=0.5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.5,
          "description": "min=0.5, mean=0.5, max=0.5, sum=0.5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.5,
          "description": "min=0.5, mean=0.5, max=0.5, sum=0.5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 590.0,
          "description": "min=590, mean=590, max=590, sum=590 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20college_chemistry&runSpecs=%5B%22mmlu%3Asubject%3Dcollege_chemistry%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.5555555555555556,
          "description": "min=0.556, mean=0.556, max=0.556, sum=0.556 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.5555555555555556,
          "description": "min=0.556, mean=0.556, max=0.556, sum=0.556 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.5555555555555556,
          "description": "min=0.556, mean=0.556, max=0.556, sum=0.556 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 590.0,
          "description": "min=590, mean=590, max=590, sum=590 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20college_chemistry&runSpecs=%5B%22mmlu%3Asubject%3Dcollege_chemistry%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.6111111111111112,
          "description": "min=0.611, mean=0.611, max=0.611, sum=0.611 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.6111111111111112,
          "description": "min=0.611, mean=0.611, max=0.611, sum=0.611 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.6111111111111112,
          "description": "min=0.611, mean=0.611, max=0.611, sum=0.611 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 590.0,
          "description": "min=590, mean=590, max=590, sum=590 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20college_chemistry&runSpecs=%5B%22mmlu%3Asubject%3Dcollege_chemistry%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=college_chemistry,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.5,
          "description": "min=0.5, mean=0.5, max=0.5, sum=0.5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.5,
          "description": "min=0.5, mean=0.5, max=0.5, sum=0.5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.5,
          "description": "min=0.5, mean=0.5, max=0.5, sum=0.5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 546.0,
          "description": "min=546, mean=546, max=546, sum=546 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20college_chemistry&runSpecs=%5B%22mmlu%3Asubject%3Dcollege_chemistry%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Dcollege_chemistry%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Dcollege_chemistry%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Dcollege_chemistry%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Dcollege_chemistry%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:college_chemistry.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:college_chemistry.json"
      }
    ],
    "name": "mmlu_subject:college_chemistry"
  },
  {
    "title": "subject: college_computer_science",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20college_computer_science&runSpecs=%5B%22mmlu%3Asubject%3Dcollege_computer_science%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.29411764705882354,
          "description": "min=0.294, mean=0.294, max=0.294, sum=0.294 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.29411764705882354,
          "description": "min=0.294, mean=0.294, max=0.294, sum=0.294 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.29411764705882354,
          "description": "min=0.294, mean=0.294, max=0.294, sum=0.294 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 856.5882352941177,
          "description": "min=856.588, mean=856.588, max=856.588, sum=856.588 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20college_computer_science&runSpecs=%5B%22mmlu%3Asubject%3Dcollege_computer_science%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.7058823529411765,
          "description": "min=0.706, mean=0.706, max=0.706, sum=0.706 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.7058823529411765,
          "description": "min=0.706, mean=0.706, max=0.706, sum=0.706 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.7058823529411765,
          "description": "min=0.706, mean=0.706, max=0.706, sum=0.706 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 856.5882352941177,
          "description": "min=856.588, mean=856.588, max=856.588, sum=856.588 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20college_computer_science&runSpecs=%5B%22mmlu%3Asubject%3Dcollege_computer_science%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.29411764705882354,
          "description": "min=0.294, mean=0.294, max=0.294, sum=0.294 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.29411764705882354,
          "description": "min=0.294, mean=0.294, max=0.294, sum=0.294 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.29411764705882354,
          "description": "min=0.294, mean=0.294, max=0.294, sum=0.294 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 856.5882352941177,
          "description": "min=856.588, mean=856.588, max=856.588, sum=856.588 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20college_computer_science&runSpecs=%5B%22mmlu%3Asubject%3Dcollege_computer_science%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.7058823529411765,
          "description": "min=0.706, mean=0.706, max=0.706, sum=0.706 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.7058823529411765,
          "description": "min=0.706, mean=0.706, max=0.706, sum=0.706 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.7058823529411765,
          "description": "min=0.706, mean=0.706, max=0.706, sum=0.706 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 856.5882352941177,
          "description": "min=856.588, mean=856.588, max=856.588, sum=856.588 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20college_computer_science&runSpecs=%5B%22mmlu%3Asubject%3Dcollege_computer_science%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=college_computer_science,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.5294117647058824,
          "description": "min=0.529, mean=0.529, max=0.529, sum=0.529 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.5294117647058824,
          "description": "min=0.529, mean=0.529, max=0.529, sum=0.529 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.5294117647058824,
          "description": "min=0.529, mean=0.529, max=0.529, sum=0.529 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 812.5882352941177,
          "description": "min=812.588, mean=812.588, max=812.588, sum=812.588 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20college_computer_science&runSpecs=%5B%22mmlu%3Asubject%3Dcollege_computer_science%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Dcollege_computer_science%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Dcollege_computer_science%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Dcollege_computer_science%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Dcollege_computer_science%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:college_computer_science.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:college_computer_science.json"
      }
    ],
    "name": "mmlu_subject:college_computer_science"
  },
  {
    "title": "subject: college_mathematics",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20college_mathematics&runSpecs=%5B%22mmlu%3Asubject%3Dcollege_mathematics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.35294117647058826,
          "description": "min=0.353, mean=0.353, max=0.353, sum=0.353 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.35294117647058826,
          "description": "min=0.353, mean=0.353, max=0.353, sum=0.353 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.35294117647058826,
          "description": "min=0.353, mean=0.353, max=0.353, sum=0.353 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 636.8235294117648,
          "description": "min=636.824, mean=636.824, max=636.824, sum=636.824 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20college_mathematics&runSpecs=%5B%22mmlu%3Asubject%3Dcollege_mathematics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.5294117647058824,
          "description": "min=0.529, mean=0.529, max=0.529, sum=0.529 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.5294117647058824,
          "description": "min=0.529, mean=0.529, max=0.529, sum=0.529 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.5294117647058824,
          "description": "min=0.529, mean=0.529, max=0.529, sum=0.529 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 636.8235294117648,
          "description": "min=636.824, mean=636.824, max=636.824, sum=636.824 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20college_mathematics&runSpecs=%5B%22mmlu%3Asubject%3Dcollege_mathematics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.35294117647058826,
          "description": "min=0.353, mean=0.353, max=0.353, sum=0.353 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.35294117647058826,
          "description": "min=0.353, mean=0.353, max=0.353, sum=0.353 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.35294117647058826,
          "description": "min=0.353, mean=0.353, max=0.353, sum=0.353 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 636.8235294117648,
          "description": "min=636.824, mean=636.824, max=636.824, sum=636.824 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.1764705882352942,
          "description": "min=1.176, mean=1.176, max=1.176, sum=1.176 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20college_mathematics&runSpecs=%5B%22mmlu%3Asubject%3Dcollege_mathematics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.35294117647058826,
          "description": "min=0.353, mean=0.353, max=0.353, sum=0.353 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.35294117647058826,
          "description": "min=0.353, mean=0.353, max=0.353, sum=0.353 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.35294117647058826,
          "description": "min=0.353, mean=0.353, max=0.353, sum=0.353 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 636.8235294117648,
          "description": "min=636.824, mean=636.824, max=636.824, sum=636.824 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20college_mathematics&runSpecs=%5B%22mmlu%3Asubject%3Dcollege_mathematics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=college_mathematics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.17647058823529413,
          "description": "min=0.176, mean=0.176, max=0.176, sum=0.176 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.17647058823529413,
          "description": "min=0.176, mean=0.176, max=0.176, sum=0.176 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.17647058823529413,
          "description": "min=0.176, mean=0.176, max=0.176, sum=0.176 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 592.8235294117648,
          "description": "min=592.824, mean=592.824, max=592.824, sum=592.824 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20college_mathematics&runSpecs=%5B%22mmlu%3Asubject%3Dcollege_mathematics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Dcollege_mathematics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Dcollege_mathematics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Dcollege_mathematics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Dcollege_mathematics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:college_mathematics.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:college_mathematics.json"
      }
    ],
    "name": "mmlu_subject:college_mathematics"
  },
  {
    "title": "subject: college_medicine",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20college_medicine&runSpecs=%5B%22mmlu%3Asubject%3Dcollege_medicine%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=college_medicine,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.8125,
          "description": "min=0.812, mean=0.812, max=0.812, sum=0.812 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.8125,
          "description": "min=0.812, mean=0.812, max=0.812, sum=0.812 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.8125,
          "description": "min=0.812, mean=0.812, max=0.812, sum=0.812 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 16.0,
          "description": "min=16, mean=16, max=16, sum=16 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 506.9375,
          "description": "min=506.938, mean=506.938, max=506.938, sum=506.938 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20college_medicine&runSpecs=%5B%22mmlu%3Asubject%3Dcollege_medicine%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=college_medicine,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.5625,
          "description": "min=0.562, mean=0.562, max=0.562, sum=0.562 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.5625,
          "description": "min=0.562, mean=0.562, max=0.562, sum=0.562 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.5625,
          "description": "min=0.562, mean=0.562, max=0.562, sum=0.562 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 16.0,
          "description": "min=16, mean=16, max=16, sum=16 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 506.9375,
          "description": "min=506.938, mean=506.938, max=506.938, sum=506.938 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20college_medicine&runSpecs=%5B%22mmlu%3Asubject%3Dcollege_medicine%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=college_medicine,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.5625,
          "description": "min=0.562, mean=0.562, max=0.562, sum=0.562 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.5625,
          "description": "min=0.562, mean=0.562, max=0.562, sum=0.562 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.5625,
          "description": "min=0.562, mean=0.562, max=0.562, sum=0.562 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 16.0,
          "description": "min=16, mean=16, max=16, sum=16 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 506.9375,
          "description": "min=506.938, mean=506.938, max=506.938, sum=506.938 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20college_medicine&runSpecs=%5B%22mmlu%3Asubject%3Dcollege_medicine%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=college_medicine,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.6875,
          "description": "min=0.688, mean=0.688, max=0.688, sum=0.688 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.6875,
          "description": "min=0.688, mean=0.688, max=0.688, sum=0.688 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.6875,
          "description": "min=0.688, mean=0.688, max=0.688, sum=0.688 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 16.0,
          "description": "min=16, mean=16, max=16, sum=16 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 506.9375,
          "description": "min=506.938, mean=506.938, max=506.938, sum=506.938 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20college_medicine&runSpecs=%5B%22mmlu%3Asubject%3Dcollege_medicine%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=college_medicine,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.75,
          "description": "min=0.75, mean=0.75, max=0.75, sum=0.75 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.75,
          "description": "min=0.75, mean=0.75, max=0.75, sum=0.75 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.75,
          "description": "min=0.75, mean=0.75, max=0.75, sum=0.75 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 16.0,
          "description": "min=16, mean=16, max=16, sum=16 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 462.9375,
          "description": "min=462.938, mean=462.938, max=462.938, sum=462.938 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20college_medicine&runSpecs=%5B%22mmlu%3Asubject%3Dcollege_medicine%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Dcollege_medicine%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Dcollege_medicine%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Dcollege_medicine%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Dcollege_medicine%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:college_medicine.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:college_medicine.json"
      }
    ],
    "name": "mmlu_subject:college_medicine"
  },
  {
    "title": "subject: college_physics",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20college_physics&runSpecs=%5B%22mmlu%3Asubject%3Dcollege_physics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=college_physics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.23529411764705882,
          "description": "min=0.235, mean=0.235, max=0.235, sum=0.235 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.23529411764705882,
          "description": "min=0.235, mean=0.235, max=0.235, sum=0.235 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.23529411764705882,
          "description": "min=0.235, mean=0.235, max=0.235, sum=0.235 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 530.5882352941177,
          "description": "min=530.588, mean=530.588, max=530.588, sum=530.588 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20college_physics&runSpecs=%5B%22mmlu%3Asubject%3Dcollege_physics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=college_physics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.6470588235294118,
          "description": "min=0.647, mean=0.647, max=0.647, sum=0.647 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.6470588235294118,
          "description": "min=0.647, mean=0.647, max=0.647, sum=0.647 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.6470588235294118,
          "description": "min=0.647, mean=0.647, max=0.647, sum=0.647 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 530.5882352941177,
          "description": "min=530.588, mean=530.588, max=530.588, sum=530.588 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20college_physics&runSpecs=%5B%22mmlu%3Asubject%3Dcollege_physics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=college_physics,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.23529411764705882,
          "description": "min=0.235, mean=0.235, max=0.235, sum=0.235 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.23529411764705882,
          "description": "min=0.235, mean=0.235, max=0.235, sum=0.235 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.23529411764705882,
          "description": "min=0.235, mean=0.235, max=0.235, sum=0.235 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 530.5882352941177,
          "description": "min=530.588, mean=530.588, max=530.588, sum=530.588 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 2.2941176470588234,
          "description": "min=2.294, mean=2.294, max=2.294, sum=2.294 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20college_physics&runSpecs=%5B%22mmlu%3Asubject%3Dcollege_physics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=college_physics,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.47058823529411764,
          "description": "min=0.471, mean=0.471, max=0.471, sum=0.471 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.47058823529411764,
          "description": "min=0.471, mean=0.471, max=0.471, sum=0.471 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.47058823529411764,
          "description": "min=0.471, mean=0.471, max=0.471, sum=0.471 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 530.5882352941177,
          "description": "min=530.588, mean=530.588, max=530.588, sum=530.588 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20college_physics&runSpecs=%5B%22mmlu%3Asubject%3Dcollege_physics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=college_physics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.5294117647058824,
          "description": "min=0.529, mean=0.529, max=0.529, sum=0.529 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.5294117647058824,
          "description": "min=0.529, mean=0.529, max=0.529, sum=0.529 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.5294117647058824,
          "description": "min=0.529, mean=0.529, max=0.529, sum=0.529 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 486.5882352941176,
          "description": "min=486.588, mean=486.588, max=486.588, sum=486.588 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20college_physics&runSpecs=%5B%22mmlu%3Asubject%3Dcollege_physics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Dcollege_physics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Dcollege_physics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Dcollege_physics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Dcollege_physics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:college_physics.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:college_physics.json"
      }
    ],
    "name": "mmlu_subject:college_physics"
  },
  {
    "title": "subject: computer_security",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20computer_security&runSpecs=%5B%22mmlu%3Asubject%3Dcomputer_security%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=computer_security,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.5294117647058824,
          "description": "min=0.529, mean=0.529, max=0.529, sum=0.529 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.5294117647058824,
          "description": "min=0.529, mean=0.529, max=0.529, sum=0.529 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.5294117647058824,
          "description": "min=0.529, mean=0.529, max=0.529, sum=0.529 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 421.52941176470586,
          "description": "min=421.529, mean=421.529, max=421.529, sum=421.529 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20computer_security&runSpecs=%5B%22mmlu%3Asubject%3Dcomputer_security%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=computer_security,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.7058823529411765,
          "description": "min=0.706, mean=0.706, max=0.706, sum=0.706 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.7058823529411765,
          "description": "min=0.706, mean=0.706, max=0.706, sum=0.706 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.7058823529411765,
          "description": "min=0.706, mean=0.706, max=0.706, sum=0.706 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 421.52941176470586,
          "description": "min=421.529, mean=421.529, max=421.529, sum=421.529 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20computer_security&runSpecs=%5B%22mmlu%3Asubject%3Dcomputer_security%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=computer_security,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.4117647058823529,
          "description": "min=0.412, mean=0.412, max=0.412, sum=0.412 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.4117647058823529,
          "description": "min=0.412, mean=0.412, max=0.412, sum=0.412 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.4117647058823529,
          "description": "min=0.412, mean=0.412, max=0.412, sum=0.412 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 421.52941176470586,
          "description": "min=421.529, mean=421.529, max=421.529, sum=421.529 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20computer_security&runSpecs=%5B%22mmlu%3Asubject%3Dcomputer_security%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=computer_security,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.7058823529411765,
          "description": "min=0.706, mean=0.706, max=0.706, sum=0.706 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.7058823529411765,
          "description": "min=0.706, mean=0.706, max=0.706, sum=0.706 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.7058823529411765,
          "description": "min=0.706, mean=0.706, max=0.706, sum=0.706 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 421.52941176470586,
          "description": "min=421.529, mean=421.529, max=421.529, sum=421.529 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20computer_security&runSpecs=%5B%22mmlu%3Asubject%3Dcomputer_security%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=computer_security,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.6470588235294118,
          "description": "min=0.647, mean=0.647, max=0.647, sum=0.647 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.6470588235294118,
          "description": "min=0.647, mean=0.647, max=0.647, sum=0.647 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.6470588235294118,
          "description": "min=0.647, mean=0.647, max=0.647, sum=0.647 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 377.52941176470586,
          "description": "min=377.529, mean=377.529, max=377.529, sum=377.529 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20computer_security&runSpecs=%5B%22mmlu%3Asubject%3Dcomputer_security%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Dcomputer_security%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Dcomputer_security%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Dcomputer_security%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Dcomputer_security%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:computer_security.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:computer_security.json"
      }
    ],
    "name": "mmlu_subject:computer_security"
  },
  {
    "title": "subject: conceptual_physics",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20conceptual_physics&runSpecs=%5B%22mmlu%3Asubject%3Dconceptual_physics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.8421052631578947,
          "description": "min=0.842, mean=0.842, max=0.842, sum=0.842 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.8421052631578947,
          "description": "min=0.842, mean=0.842, max=0.842, sum=0.842 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.8421052631578947,
          "description": "min=0.842, mean=0.842, max=0.842, sum=0.842 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 337.36842105263156,
          "description": "min=337.368, mean=337.368, max=337.368, sum=337.368 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20conceptual_physics&runSpecs=%5B%22mmlu%3Asubject%3Dconceptual_physics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.8421052631578947,
          "description": "min=0.842, mean=0.842, max=0.842, sum=0.842 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.8421052631578947,
          "description": "min=0.842, mean=0.842, max=0.842, sum=0.842 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.8421052631578947,
          "description": "min=0.842, mean=0.842, max=0.842, sum=0.842 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 337.36842105263156,
          "description": "min=337.368, mean=337.368, max=337.368, sum=337.368 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20conceptual_physics&runSpecs=%5B%22mmlu%3Asubject%3Dconceptual_physics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.3684210526315789,
          "description": "min=0.368, mean=0.368, max=0.368, sum=0.368 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.3684210526315789,
          "description": "min=0.368, mean=0.368, max=0.368, sum=0.368 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.3684210526315789,
          "description": "min=0.368, mean=0.368, max=0.368, sum=0.368 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 337.36842105263156,
          "description": "min=337.368, mean=337.368, max=337.368, sum=337.368 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.2105263157894737,
          "description": "min=1.211, mean=1.211, max=1.211, sum=1.211 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20conceptual_physics&runSpecs=%5B%22mmlu%3Asubject%3Dconceptual_physics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.8947368421052632,
          "description": "min=0.895, mean=0.895, max=0.895, sum=0.895 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.8947368421052632,
          "description": "min=0.895, mean=0.895, max=0.895, sum=0.895 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.8947368421052632,
          "description": "min=0.895, mean=0.895, max=0.895, sum=0.895 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 337.36842105263156,
          "description": "min=337.368, mean=337.368, max=337.368, sum=337.368 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20conceptual_physics&runSpecs=%5B%22mmlu%3Asubject%3Dconceptual_physics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=conceptual_physics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.631578947368421,
          "description": "min=0.632, mean=0.632, max=0.632, sum=0.632 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.631578947368421,
          "description": "min=0.632, mean=0.632, max=0.632, sum=0.632 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.631578947368421,
          "description": "min=0.632, mean=0.632, max=0.632, sum=0.632 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 293.36842105263156,
          "description": "min=293.368, mean=293.368, max=293.368, sum=293.368 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20conceptual_physics&runSpecs=%5B%22mmlu%3Asubject%3Dconceptual_physics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Dconceptual_physics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Dconceptual_physics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Dconceptual_physics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Dconceptual_physics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:conceptual_physics.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:conceptual_physics.json"
      }
    ],
    "name": "mmlu_subject:conceptual_physics"
  },
  {
    "title": "subject: econometrics",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20econometrics&runSpecs=%5B%22mmlu%3Asubject%3Deconometrics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=econometrics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.4375,
          "description": "min=0.438, mean=0.438, max=0.438, sum=0.438 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.4375,
          "description": "min=0.438, mean=0.438, max=0.438, sum=0.438 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.4375,
          "description": "min=0.438, mean=0.438, max=0.438, sum=0.438 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 16.0,
          "description": "min=16, mean=16, max=16, sum=16 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 647.0,
          "description": "min=647, mean=647, max=647, sum=647 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20econometrics&runSpecs=%5B%22mmlu%3Asubject%3Deconometrics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=econometrics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.8125,
          "description": "min=0.812, mean=0.812, max=0.812, sum=0.812 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.8125,
          "description": "min=0.812, mean=0.812, max=0.812, sum=0.812 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.8125,
          "description": "min=0.812, mean=0.812, max=0.812, sum=0.812 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 16.0,
          "description": "min=16, mean=16, max=16, sum=16 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 647.0,
          "description": "min=647, mean=647, max=647, sum=647 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20econometrics&runSpecs=%5B%22mmlu%3Asubject%3Deconometrics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=econometrics,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.3125,
          "description": "min=0.312, mean=0.312, max=0.312, sum=0.312 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.3125,
          "description": "min=0.312, mean=0.312, max=0.312, sum=0.312 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.3125,
          "description": "min=0.312, mean=0.312, max=0.312, sum=0.312 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 16.0,
          "description": "min=16, mean=16, max=16, sum=16 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 647.0,
          "description": "min=647, mean=647, max=647, sum=647 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20econometrics&runSpecs=%5B%22mmlu%3Asubject%3Deconometrics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=econometrics,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.75,
          "description": "min=0.75, mean=0.75, max=0.75, sum=0.75 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.75,
          "description": "min=0.75, mean=0.75, max=0.75, sum=0.75 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.75,
          "description": "min=0.75, mean=0.75, max=0.75, sum=0.75 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 16.0,
          "description": "min=16, mean=16, max=16, sum=16 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 647.0,
          "description": "min=647, mean=647, max=647, sum=647 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20econometrics&runSpecs=%5B%22mmlu%3Asubject%3Deconometrics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=econometrics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.625,
          "description": "min=0.625, mean=0.625, max=0.625, sum=0.625 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.625,
          "description": "min=0.625, mean=0.625, max=0.625, sum=0.625 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.625,
          "description": "min=0.625, mean=0.625, max=0.625, sum=0.625 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 16.0,
          "description": "min=16, mean=16, max=16, sum=16 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 603.0,
          "description": "min=603, mean=603, max=603, sum=603 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20econometrics&runSpecs=%5B%22mmlu%3Asubject%3Deconometrics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Deconometrics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Deconometrics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Deconometrics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Deconometrics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:econometrics.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:econometrics.json"
      }
    ],
    "name": "mmlu_subject:econometrics"
  },
  {
    "title": "subject: electrical_engineering",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20electrical_engineering&runSpecs=%5B%22mmlu%3Asubject%3Delectrical_engineering%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.3684210526315789,
          "description": "min=0.368, mean=0.368, max=0.368, sum=0.368 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.3684210526315789,
          "description": "min=0.368, mean=0.368, max=0.368, sum=0.368 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.3684210526315789,
          "description": "min=0.368, mean=0.368, max=0.368, sum=0.368 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 469.63157894736844,
          "description": "min=469.632, mean=469.632, max=469.632, sum=469.632 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20electrical_engineering&runSpecs=%5B%22mmlu%3Asubject%3Delectrical_engineering%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.7368421052631579,
          "description": "min=0.737, mean=0.737, max=0.737, sum=0.737 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.7368421052631579,
          "description": "min=0.737, mean=0.737, max=0.737, sum=0.737 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.7368421052631579,
          "description": "min=0.737, mean=0.737, max=0.737, sum=0.737 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 469.63157894736844,
          "description": "min=469.632, mean=469.632, max=469.632, sum=469.632 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20electrical_engineering&runSpecs=%5B%22mmlu%3Asubject%3Delectrical_engineering%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.2631578947368421,
          "description": "min=0.263, mean=0.263, max=0.263, sum=0.263 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.2631578947368421,
          "description": "min=0.263, mean=0.263, max=0.263, sum=0.263 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.2631578947368421,
          "description": "min=0.263, mean=0.263, max=0.263, sum=0.263 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 469.63157894736844,
          "description": "min=469.632, mean=469.632, max=469.632, sum=469.632 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20electrical_engineering&runSpecs=%5B%22mmlu%3Asubject%3Delectrical_engineering%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.5263157894736842,
          "description": "min=0.526, mean=0.526, max=0.526, sum=0.526 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.5263157894736842,
          "description": "min=0.526, mean=0.526, max=0.526, sum=0.526 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.5263157894736842,
          "description": "min=0.526, mean=0.526, max=0.526, sum=0.526 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 469.63157894736844,
          "description": "min=469.632, mean=469.632, max=469.632, sum=469.632 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20electrical_engineering&runSpecs=%5B%22mmlu%3Asubject%3Delectrical_engineering%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.5789473684210527,
          "description": "min=0.579, mean=0.579, max=0.579, sum=0.579 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.5789473684210527,
          "description": "min=0.579, mean=0.579, max=0.579, sum=0.579 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.5789473684210527,
          "description": "min=0.579, mean=0.579, max=0.579, sum=0.579 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 425.63157894736844,
          "description": "min=425.632, mean=425.632, max=425.632, sum=425.632 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20electrical_engineering&runSpecs=%5B%22mmlu%3Asubject%3Delectrical_engineering%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Delectrical_engineering%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Delectrical_engineering%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Delectrical_engineering%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Delectrical_engineering%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:electrical_engineering.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:electrical_engineering.json"
      }
    ],
    "name": "mmlu_subject:electrical_engineering"
  },
  {
    "title": "subject: elementary_mathematics",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20elementary_mathematics&runSpecs=%5B%22mmlu%3Asubject%3Delementary_mathematics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.3888888888888889,
          "description": "min=0.389, mean=0.389, max=0.389, sum=0.389 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.3888888888888889,
          "description": "min=0.389, mean=0.389, max=0.389, sum=0.389 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.3888888888888889,
          "description": "min=0.389, mean=0.389, max=0.389, sum=0.389 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 571.5555555555555,
          "description": "min=571.556, mean=571.556, max=571.556, sum=571.556 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20elementary_mathematics&runSpecs=%5B%22mmlu%3Asubject%3Delementary_mathematics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.7222222222222222,
          "description": "min=0.722, mean=0.722, max=0.722, sum=0.722 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.7222222222222222,
          "description": "min=0.722, mean=0.722, max=0.722, sum=0.722 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.7222222222222222,
          "description": "min=0.722, mean=0.722, max=0.722, sum=0.722 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 571.5555555555555,
          "description": "min=571.556, mean=571.556, max=571.556, sum=571.556 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20elementary_mathematics&runSpecs=%5B%22mmlu%3Asubject%3Delementary_mathematics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.2777777777777778,
          "description": "min=0.278, mean=0.278, max=0.278, sum=0.278 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.2777777777777778,
          "description": "min=0.278, mean=0.278, max=0.278, sum=0.278 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.2777777777777778,
          "description": "min=0.278, mean=0.278, max=0.278, sum=0.278 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 571.5555555555555,
          "description": "min=571.556, mean=571.556, max=571.556, sum=571.556 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.3888888888888888,
          "description": "min=1.389, mean=1.389, max=1.389, sum=1.389 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20elementary_mathematics&runSpecs=%5B%22mmlu%3Asubject%3Delementary_mathematics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.7222222222222222,
          "description": "min=0.722, mean=0.722, max=0.722, sum=0.722 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.7222222222222222,
          "description": "min=0.722, mean=0.722, max=0.722, sum=0.722 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.7222222222222222,
          "description": "min=0.722, mean=0.722, max=0.722, sum=0.722 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 571.5555555555555,
          "description": "min=571.556, mean=571.556, max=571.556, sum=571.556 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20elementary_mathematics&runSpecs=%5B%22mmlu%3Asubject%3Delementary_mathematics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=elementary_mathematics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.5555555555555556,
          "description": "min=0.556, mean=0.556, max=0.556, sum=0.556 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.5555555555555556,
          "description": "min=0.556, mean=0.556, max=0.556, sum=0.556 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.5555555555555556,
          "description": "min=0.556, mean=0.556, max=0.556, sum=0.556 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 527.5555555555555,
          "description": "min=527.556, mean=527.556, max=527.556, sum=527.556 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20elementary_mathematics&runSpecs=%5B%22mmlu%3Asubject%3Delementary_mathematics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Delementary_mathematics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Delementary_mathematics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Delementary_mathematics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Delementary_mathematics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:elementary_mathematics.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:elementary_mathematics.json"
      }
    ],
    "name": "mmlu_subject:elementary_mathematics"
  },
  {
    "title": "subject: formal_logic",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20formal_logic&runSpecs=%5B%22mmlu%3Asubject%3Dformal_logic%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=formal_logic,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.3333333333333333,
          "description": "min=0.333, mean=0.333, max=0.333, sum=0.333 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.3333333333333333,
          "description": "min=0.333, mean=0.333, max=0.333, sum=0.333 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.3333333333333333,
          "description": "min=0.333, mean=0.333, max=0.333, sum=0.333 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 654.2222222222222,
          "description": "min=654.222, mean=654.222, max=654.222, sum=654.222 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20formal_logic&runSpecs=%5B%22mmlu%3Asubject%3Dformal_logic%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=formal_logic,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.3888888888888889,
          "description": "min=0.389, mean=0.389, max=0.389, sum=0.389 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.3888888888888889,
          "description": "min=0.389, mean=0.389, max=0.389, sum=0.389 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.3888888888888889,
          "description": "min=0.389, mean=0.389, max=0.389, sum=0.389 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 654.2222222222222,
          "description": "min=654.222, mean=654.222, max=654.222, sum=654.222 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20formal_logic&runSpecs=%5B%22mmlu%3Asubject%3Dformal_logic%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=formal_logic,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.1111111111111111,
          "description": "min=0.111, mean=0.111, max=0.111, sum=0.111 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.1111111111111111,
          "description": "min=0.111, mean=0.111, max=0.111, sum=0.111 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.1111111111111111,
          "description": "min=0.111, mean=0.111, max=0.111, sum=0.111 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 654.2222222222222,
          "description": "min=654.222, mean=654.222, max=654.222, sum=654.222 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20formal_logic&runSpecs=%5B%22mmlu%3Asubject%3Dformal_logic%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=formal_logic,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.5,
          "description": "min=0.5, mean=0.5, max=0.5, sum=0.5 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.5,
          "description": "min=0.5, mean=0.5, max=0.5, sum=0.5 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.5,
          "description": "min=0.5, mean=0.5, max=0.5, sum=0.5 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 654.2222222222222,
          "description": "min=654.222, mean=654.222, max=654.222, sum=654.222 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20formal_logic&runSpecs=%5B%22mmlu%3Asubject%3Dformal_logic%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=formal_logic,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.2777777777777778,
          "description": "min=0.278, mean=0.278, max=0.278, sum=0.278 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.2777777777777778,
          "description": "min=0.278, mean=0.278, max=0.278, sum=0.278 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.2777777777777778,
          "description": "min=0.278, mean=0.278, max=0.278, sum=0.278 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 610.2222222222222,
          "description": "min=610.222, mean=610.222, max=610.222, sum=610.222 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20formal_logic&runSpecs=%5B%22mmlu%3Asubject%3Dformal_logic%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Dformal_logic%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Dformal_logic%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Dformal_logic%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Dformal_logic%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:formal_logic.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:formal_logic.json"
      }
    ],
    "name": "mmlu_subject:formal_logic"
  },
  {
    "title": "subject: global_facts",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20global_facts&runSpecs=%5B%22mmlu%3Asubject%3Dglobal_facts%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=global_facts,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.5555555555555556,
          "description": "min=0.556, mean=0.556, max=0.556, sum=0.556 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.5555555555555556,
          "description": "min=0.556, mean=0.556, max=0.556, sum=0.556 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.5555555555555556,
          "description": "min=0.556, mean=0.556, max=0.556, sum=0.556 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 429.6111111111111,
          "description": "min=429.611, mean=429.611, max=429.611, sum=429.611 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20global_facts&runSpecs=%5B%22mmlu%3Asubject%3Dglobal_facts%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=global_facts,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.6666666666666666,
          "description": "min=0.667, mean=0.667, max=0.667, sum=0.667 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.6666666666666666,
          "description": "min=0.667, mean=0.667, max=0.667, sum=0.667 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.6666666666666666,
          "description": "min=0.667, mean=0.667, max=0.667, sum=0.667 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 429.6111111111111,
          "description": "min=429.611, mean=429.611, max=429.611, sum=429.611 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20global_facts&runSpecs=%5B%22mmlu%3Asubject%3Dglobal_facts%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=global_facts,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.2777777777777778,
          "description": "min=0.278, mean=0.278, max=0.278, sum=0.278 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.2777777777777778,
          "description": "min=0.278, mean=0.278, max=0.278, sum=0.278 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.2777777777777778,
          "description": "min=0.278, mean=0.278, max=0.278, sum=0.278 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 429.6111111111111,
          "description": "min=429.611, mean=429.611, max=429.611, sum=429.611 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 2.888888888888889,
          "description": "min=2.889, mean=2.889, max=2.889, sum=2.889 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20global_facts&runSpecs=%5B%22mmlu%3Asubject%3Dglobal_facts%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=global_facts,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.6111111111111112,
          "description": "min=0.611, mean=0.611, max=0.611, sum=0.611 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.6111111111111112,
          "description": "min=0.611, mean=0.611, max=0.611, sum=0.611 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.6111111111111112,
          "description": "min=0.611, mean=0.611, max=0.611, sum=0.611 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 429.6111111111111,
          "description": "min=429.611, mean=429.611, max=429.611, sum=429.611 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20global_facts&runSpecs=%5B%22mmlu%3Asubject%3Dglobal_facts%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=global_facts,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.4444444444444444,
          "description": "min=0.444, mean=0.444, max=0.444, sum=0.444 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.4444444444444444,
          "description": "min=0.444, mean=0.444, max=0.444, sum=0.444 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.4444444444444444,
          "description": "min=0.444, mean=0.444, max=0.444, sum=0.444 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 385.6111111111111,
          "description": "min=385.611, mean=385.611, max=385.611, sum=385.611 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20global_facts&runSpecs=%5B%22mmlu%3Asubject%3Dglobal_facts%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Dglobal_facts%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Dglobal_facts%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Dglobal_facts%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Dglobal_facts%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:global_facts.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:global_facts.json"
      }
    ],
    "name": "mmlu_subject:global_facts"
  },
  {
    "title": "subject: high_school_biology",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_biology&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_biology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.7647058823529411,
          "description": "min=0.765, mean=0.765, max=0.765, sum=0.765 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.7647058823529411,
          "description": "min=0.765, mean=0.765, max=0.765, sum=0.765 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.7647058823529411,
          "description": "min=0.765, mean=0.765, max=0.765, sum=0.765 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 563.1764705882352,
          "description": "min=563.176, mean=563.176, max=563.176, sum=563.176 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_biology&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_biology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.9411764705882353,
          "description": "min=0.941, mean=0.941, max=0.941, sum=0.941 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.9411764705882353,
          "description": "min=0.941, mean=0.941, max=0.941, sum=0.941 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.9411764705882353,
          "description": "min=0.941, mean=0.941, max=0.941, sum=0.941 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 563.1764705882352,
          "description": "min=563.176, mean=563.176, max=563.176, sum=563.176 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_biology&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_biology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.47058823529411764,
          "description": "min=0.471, mean=0.471, max=0.471, sum=0.471 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.47058823529411764,
          "description": "min=0.471, mean=0.471, max=0.471, sum=0.471 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.47058823529411764,
          "description": "min=0.471, mean=0.471, max=0.471, sum=0.471 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 563.1764705882352,
          "description": "min=563.176, mean=563.176, max=563.176, sum=563.176 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_biology&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_biology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.8235294117647058,
          "description": "min=0.824, mean=0.824, max=0.824, sum=0.824 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.8235294117647058,
          "description": "min=0.824, mean=0.824, max=0.824, sum=0.824 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.8235294117647058,
          "description": "min=0.824, mean=0.824, max=0.824, sum=0.824 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 563.1764705882352,
          "description": "min=563.176, mean=563.176, max=563.176, sum=563.176 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_biology&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_biology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_biology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.8235294117647058,
          "description": "min=0.824, mean=0.824, max=0.824, sum=0.824 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.8235294117647058,
          "description": "min=0.824, mean=0.824, max=0.824, sum=0.824 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.8235294117647058,
          "description": "min=0.824, mean=0.824, max=0.824, sum=0.824 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 519.1764705882352,
          "description": "min=519.176, mean=519.176, max=519.176, sum=519.176 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20high_school_biology&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_biology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Dhigh_school_biology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Dhigh_school_biology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Dhigh_school_biology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Dhigh_school_biology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:high_school_biology.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:high_school_biology.json"
      }
    ],
    "name": "mmlu_subject:high_school_biology"
  },
  {
    "title": "subject: high_school_chemistry",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_chemistry&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_chemistry%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.3888888888888889,
          "description": "min=0.389, mean=0.389, max=0.389, sum=0.389 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.3888888888888889,
          "description": "min=0.389, mean=0.389, max=0.389, sum=0.389 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.3888888888888889,
          "description": "min=0.389, mean=0.389, max=0.389, sum=0.389 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 534.2777777777778,
          "description": "min=534.278, mean=534.278, max=534.278, sum=534.278 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_chemistry&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_chemistry%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.5555555555555556,
          "description": "min=0.556, mean=0.556, max=0.556, sum=0.556 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.5555555555555556,
          "description": "min=0.556, mean=0.556, max=0.556, sum=0.556 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.5555555555555556,
          "description": "min=0.556, mean=0.556, max=0.556, sum=0.556 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 534.2777777777778,
          "description": "min=534.278, mean=534.278, max=534.278, sum=534.278 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_chemistry&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_chemistry%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.2222222222222222,
          "description": "min=0.222, mean=0.222, max=0.222, sum=0.222 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.2222222222222222,
          "description": "min=0.222, mean=0.222, max=0.222, sum=0.222 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.2222222222222222,
          "description": "min=0.222, mean=0.222, max=0.222, sum=0.222 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 534.2777777777778,
          "description": "min=534.278, mean=534.278, max=534.278, sum=534.278 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.2222222222222223,
          "description": "min=1.222, mean=1.222, max=1.222, sum=1.222 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_chemistry&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_chemistry%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.4444444444444444,
          "description": "min=0.444, mean=0.444, max=0.444, sum=0.444 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.4444444444444444,
          "description": "min=0.444, mean=0.444, max=0.444, sum=0.444 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.4444444444444444,
          "description": "min=0.444, mean=0.444, max=0.444, sum=0.444 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 534.2777777777778,
          "description": "min=534.278, mean=534.278, max=534.278, sum=534.278 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_chemistry&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_chemistry%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.3888888888888889,
          "description": "min=0.389, mean=0.389, max=0.389, sum=0.389 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.3888888888888889,
          "description": "min=0.389, mean=0.389, max=0.389, sum=0.389 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.3888888888888889,
          "description": "min=0.389, mean=0.389, max=0.389, sum=0.389 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 490.27777777777777,
          "description": "min=490.278, mean=490.278, max=490.278, sum=490.278 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20high_school_chemistry&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_chemistry%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Dhigh_school_chemistry%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Dhigh_school_chemistry%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Dhigh_school_chemistry%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Dhigh_school_chemistry%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:high_school_chemistry.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:high_school_chemistry.json"
      }
    ],
    "name": "mmlu_subject:high_school_chemistry"
  },
  {
    "title": "subject: high_school_computer_science",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_computer_science&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_computer_science%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.3888888888888889,
          "description": "min=0.389, mean=0.389, max=0.389, sum=0.389 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.3888888888888889,
          "description": "min=0.389, mean=0.389, max=0.389, sum=0.389 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.3888888888888889,
          "description": "min=0.389, mean=0.389, max=0.389, sum=0.389 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 908.8333333333334,
          "description": "min=908.833, mean=908.833, max=908.833, sum=908.833 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_computer_science&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_computer_science%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.6666666666666666,
          "description": "min=0.667, mean=0.667, max=0.667, sum=0.667 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.6666666666666666,
          "description": "min=0.667, mean=0.667, max=0.667, sum=0.667 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.6666666666666666,
          "description": "min=0.667, mean=0.667, max=0.667, sum=0.667 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 908.8333333333334,
          "description": "min=908.833, mean=908.833, max=908.833, sum=908.833 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_computer_science&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_computer_science%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.16666666666666666,
          "description": "min=0.167, mean=0.167, max=0.167, sum=0.167 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.16666666666666666,
          "description": "min=0.167, mean=0.167, max=0.167, sum=0.167 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.16666666666666666,
          "description": "min=0.167, mean=0.167, max=0.167, sum=0.167 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 908.8333333333334,
          "description": "min=908.833, mean=908.833, max=908.833, sum=908.833 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_computer_science&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_computer_science%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.7222222222222222,
          "description": "min=0.722, mean=0.722, max=0.722, sum=0.722 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.7222222222222222,
          "description": "min=0.722, mean=0.722, max=0.722, sum=0.722 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.7222222222222222,
          "description": "min=0.722, mean=0.722, max=0.722, sum=0.722 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 908.8333333333334,
          "description": "min=908.833, mean=908.833, max=908.833, sum=908.833 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_computer_science&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_computer_science%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.7222222222222222,
          "description": "min=0.722, mean=0.722, max=0.722, sum=0.722 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.7222222222222222,
          "description": "min=0.722, mean=0.722, max=0.722, sum=0.722 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.7222222222222222,
          "description": "min=0.722, mean=0.722, max=0.722, sum=0.722 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 864.8333333333334,
          "description": "min=864.833, mean=864.833, max=864.833, sum=864.833 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20high_school_computer_science&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_computer_science%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Dhigh_school_computer_science%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Dhigh_school_computer_science%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Dhigh_school_computer_science%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Dhigh_school_computer_science%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:high_school_computer_science.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:high_school_computer_science.json"
      }
    ],
    "name": "mmlu_subject:high_school_computer_science"
  },
  {
    "title": "subject: high_school_european_history",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_european_history&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_european_history%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.8333333333333334,
          "description": "min=0.833, mean=0.833, max=0.833, sum=0.833 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.8333333333333334,
          "description": "min=0.833, mean=0.833, max=0.833, sum=0.833 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.8333333333333334,
          "description": "min=0.833, mean=0.833, max=0.833, sum=0.833 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 2.9444444444444446,
          "description": "min=2.944, mean=2.944, max=2.944, sum=2.944 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1769.4444444444443,
          "description": "min=1769.444, mean=1769.444, max=1769.444, sum=1769.444 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_european_history&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_european_history%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.8888888888888888,
          "description": "min=0.889, mean=0.889, max=0.889, sum=0.889 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.8888888888888888,
          "description": "min=0.889, mean=0.889, max=0.889, sum=0.889 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.8888888888888888,
          "description": "min=0.889, mean=0.889, max=0.889, sum=0.889 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 2.9444444444444446,
          "description": "min=2.944, mean=2.944, max=2.944, sum=2.944 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1769.4444444444443,
          "description": "min=1769.444, mean=1769.444, max=1769.444, sum=1769.444 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_european_history&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_european_history%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.5,
          "description": "min=0.5, mean=0.5, max=0.5, sum=0.5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.5,
          "description": "min=0.5, mean=0.5, max=0.5, sum=0.5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.5,
          "description": "min=0.5, mean=0.5, max=0.5, sum=0.5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 2784.222222222222,
          "description": "min=2784.222, mean=2784.222, max=2784.222, sum=2784.222 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.3888888888888888,
          "description": "min=1.389, mean=1.389, max=1.389, sum=1.389 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_european_history&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_european_history%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.7777777777777778,
          "description": "min=0.778, mean=0.778, max=0.778, sum=0.778 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.7777777777777778,
          "description": "min=0.778, mean=0.778, max=0.778, sum=0.778 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.7777777777777778,
          "description": "min=0.778, mean=0.778, max=0.778, sum=0.778 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 3.0,
          "description": "min=3, mean=3, max=3, sum=3 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1750.2222222222222,
          "description": "min=1750.222, mean=1750.222, max=1750.222, sum=1750.222 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20high_school_european_history&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_european_history%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Dhigh_school_european_history%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Dhigh_school_european_history%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Dhigh_school_european_history%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:high_school_european_history.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:high_school_european_history.json"
      }
    ],
    "name": "mmlu_subject:high_school_european_history"
  },
  {
    "title": "subject: high_school_geography",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_geography&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_geography%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.7894736842105263,
          "description": "min=0.789, mean=0.789, max=0.789, sum=0.789 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.7894736842105263,
          "description": "min=0.789, mean=0.789, max=0.789, sum=0.789 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.7894736842105263,
          "description": "min=0.789, mean=0.789, max=0.789, sum=0.789 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 410.10526315789474,
          "description": "min=410.105, mean=410.105, max=410.105, sum=410.105 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_geography&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_geography%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.8421052631578947,
          "description": "min=0.842, mean=0.842, max=0.842, sum=0.842 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.8421052631578947,
          "description": "min=0.842, mean=0.842, max=0.842, sum=0.842 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.8421052631578947,
          "description": "min=0.842, mean=0.842, max=0.842, sum=0.842 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 410.10526315789474,
          "description": "min=410.105, mean=410.105, max=410.105, sum=410.105 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_geography&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_geography%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.631578947368421,
          "description": "min=0.632, mean=0.632, max=0.632, sum=0.632 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.631578947368421,
          "description": "min=0.632, mean=0.632, max=0.632, sum=0.632 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.631578947368421,
          "description": "min=0.632, mean=0.632, max=0.632, sum=0.632 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 410.10526315789474,
          "description": "min=410.105, mean=410.105, max=410.105, sum=410.105 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.4210526315789473,
          "description": "min=1.421, mean=1.421, max=1.421, sum=1.421 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_geography&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_geography%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.8421052631578947,
          "description": "min=0.842, mean=0.842, max=0.842, sum=0.842 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.8421052631578947,
          "description": "min=0.842, mean=0.842, max=0.842, sum=0.842 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.8421052631578947,
          "description": "min=0.842, mean=0.842, max=0.842, sum=0.842 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 410.10526315789474,
          "description": "min=410.105, mean=410.105, max=410.105, sum=410.105 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_geography&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_geography%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_geography,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.8421052631578947,
          "description": "min=0.842, mean=0.842, max=0.842, sum=0.842 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.8421052631578947,
          "description": "min=0.842, mean=0.842, max=0.842, sum=0.842 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.8421052631578947,
          "description": "min=0.842, mean=0.842, max=0.842, sum=0.842 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 366.10526315789474,
          "description": "min=366.105, mean=366.105, max=366.105, sum=366.105 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20high_school_geography&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_geography%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Dhigh_school_geography%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Dhigh_school_geography%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Dhigh_school_geography%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Dhigh_school_geography%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:high_school_geography.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:high_school_geography.json"
      }
    ],
    "name": "mmlu_subject:high_school_geography"
  },
  {
    "title": "subject: high_school_government_and_politics",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_government_and_politics&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_government_and_politics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.8333333333333334,
          "description": "min=0.833, mean=0.833, max=0.833, sum=0.833 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.8333333333333334,
          "description": "min=0.833, mean=0.833, max=0.833, sum=0.833 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.8333333333333334,
          "description": "min=0.833, mean=0.833, max=0.833, sum=0.833 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 506.6666666666667,
          "description": "min=506.667, mean=506.667, max=506.667, sum=506.667 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_government_and_politics&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_government_and_politics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.9444444444444444,
          "description": "min=0.944, mean=0.944, max=0.944, sum=0.944 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.9444444444444444,
          "description": "min=0.944, mean=0.944, max=0.944, sum=0.944 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.9444444444444444,
          "description": "min=0.944, mean=0.944, max=0.944, sum=0.944 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 506.6666666666667,
          "description": "min=506.667, mean=506.667, max=506.667, sum=506.667 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_government_and_politics&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_government_and_politics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.6666666666666666,
          "description": "min=0.667, mean=0.667, max=0.667, sum=0.667 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.6666666666666666,
          "description": "min=0.667, mean=0.667, max=0.667, sum=0.667 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.6666666666666666,
          "description": "min=0.667, mean=0.667, max=0.667, sum=0.667 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 506.6666666666667,
          "description": "min=506.667, mean=506.667, max=506.667, sum=506.667 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.2222222222222223,
          "description": "min=1.222, mean=1.222, max=1.222, sum=1.222 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_government_and_politics&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_government_and_politics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.9444444444444444,
          "description": "min=0.944, mean=0.944, max=0.944, sum=0.944 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.9444444444444444,
          "description": "min=0.944, mean=0.944, max=0.944, sum=0.944 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.9444444444444444,
          "description": "min=0.944, mean=0.944, max=0.944, sum=0.944 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 506.6666666666667,
          "description": "min=506.667, mean=506.667, max=506.667, sum=506.667 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_government_and_politics&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_government_and_politics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.8888888888888888,
          "description": "min=0.889, mean=0.889, max=0.889, sum=0.889 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.8888888888888888,
          "description": "min=0.889, mean=0.889, max=0.889, sum=0.889 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.8888888888888888,
          "description": "min=0.889, mean=0.889, max=0.889, sum=0.889 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 462.6666666666667,
          "description": "min=462.667, mean=462.667, max=462.667, sum=462.667 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20high_school_government_and_politics&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_government_and_politics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Dhigh_school_government_and_politics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Dhigh_school_government_and_politics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Dhigh_school_government_and_politics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Dhigh_school_government_and_politics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:high_school_government_and_politics.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:high_school_government_and_politics.json"
      }
    ],
    "name": "mmlu_subject:high_school_government_and_politics"
  },
  {
    "title": "subject: high_school_macroeconomics",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_macroeconomics&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_macroeconomics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.42105263157894735,
          "description": "min=0.421, mean=0.421, max=0.421, sum=0.421 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.42105263157894735,
          "description": "min=0.421, mean=0.421, max=0.421, sum=0.421 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.42105263157894735,
          "description": "min=0.421, mean=0.421, max=0.421, sum=0.421 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 405.7894736842105,
          "description": "min=405.789, mean=405.789, max=405.789, sum=405.789 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_macroeconomics&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_macroeconomics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.9473684210526315,
          "description": "min=0.947, mean=0.947, max=0.947, sum=0.947 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.9473684210526315,
          "description": "min=0.947, mean=0.947, max=0.947, sum=0.947 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.9473684210526315,
          "description": "min=0.947, mean=0.947, max=0.947, sum=0.947 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 405.7894736842105,
          "description": "min=405.789, mean=405.789, max=405.789, sum=405.789 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_macroeconomics&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_macroeconomics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.631578947368421,
          "description": "min=0.632, mean=0.632, max=0.632, sum=0.632 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.631578947368421,
          "description": "min=0.632, mean=0.632, max=0.632, sum=0.632 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.631578947368421,
          "description": "min=0.632, mean=0.632, max=0.632, sum=0.632 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 405.7894736842105,
          "description": "min=405.789, mean=405.789, max=405.789, sum=405.789 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 2.0,
          "description": "min=2, mean=2, max=2, sum=2 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_macroeconomics&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_macroeconomics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.8947368421052632,
          "description": "min=0.895, mean=0.895, max=0.895, sum=0.895 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.8947368421052632,
          "description": "min=0.895, mean=0.895, max=0.895, sum=0.895 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.8947368421052632,
          "description": "min=0.895, mean=0.895, max=0.895, sum=0.895 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 405.7894736842105,
          "description": "min=405.789, mean=405.789, max=405.789, sum=405.789 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_macroeconomics&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_macroeconomics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.631578947368421,
          "description": "min=0.632, mean=0.632, max=0.632, sum=0.632 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.631578947368421,
          "description": "min=0.632, mean=0.632, max=0.632, sum=0.632 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.631578947368421,
          "description": "min=0.632, mean=0.632, max=0.632, sum=0.632 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 361.7894736842105,
          "description": "min=361.789, mean=361.789, max=361.789, sum=361.789 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20high_school_macroeconomics&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_macroeconomics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Dhigh_school_macroeconomics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Dhigh_school_macroeconomics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Dhigh_school_macroeconomics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Dhigh_school_macroeconomics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:high_school_macroeconomics.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:high_school_macroeconomics.json"
      }
    ],
    "name": "mmlu_subject:high_school_macroeconomics"
  },
  {
    "title": "subject: high_school_mathematics",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_mathematics&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_mathematics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.29411764705882354,
          "description": "min=0.294, mean=0.294, max=0.294, sum=0.294 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.29411764705882354,
          "description": "min=0.294, mean=0.294, max=0.294, sum=0.294 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.29411764705882354,
          "description": "min=0.294, mean=0.294, max=0.294, sum=0.294 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 576.3529411764706,
          "description": "min=576.353, mean=576.353, max=576.353, sum=576.353 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_mathematics&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_mathematics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.29411764705882354,
          "description": "min=0.294, mean=0.294, max=0.294, sum=0.294 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.29411764705882354,
          "description": "min=0.294, mean=0.294, max=0.294, sum=0.294 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.29411764705882354,
          "description": "min=0.294, mean=0.294, max=0.294, sum=0.294 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 576.3529411764706,
          "description": "min=576.353, mean=576.353, max=576.353, sum=576.353 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_mathematics&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_mathematics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.29411764705882354,
          "description": "min=0.294, mean=0.294, max=0.294, sum=0.294 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.29411764705882354,
          "description": "min=0.294, mean=0.294, max=0.294, sum=0.294 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.29411764705882354,
          "description": "min=0.294, mean=0.294, max=0.294, sum=0.294 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 576.3529411764706,
          "description": "min=576.353, mean=576.353, max=576.353, sum=576.353 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.2352941176470589,
          "description": "min=1.235, mean=1.235, max=1.235, sum=1.235 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_mathematics&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_mathematics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.29411764705882354,
          "description": "min=0.294, mean=0.294, max=0.294, sum=0.294 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.29411764705882354,
          "description": "min=0.294, mean=0.294, max=0.294, sum=0.294 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.29411764705882354,
          "description": "min=0.294, mean=0.294, max=0.294, sum=0.294 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 576.3529411764706,
          "description": "min=576.353, mean=576.353, max=576.353, sum=576.353 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_mathematics&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_mathematics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.6470588235294118,
          "description": "min=0.647, mean=0.647, max=0.647, sum=0.647 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.6470588235294118,
          "description": "min=0.647, mean=0.647, max=0.647, sum=0.647 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.6470588235294118,
          "description": "min=0.647, mean=0.647, max=0.647, sum=0.647 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 532.3529411764706,
          "description": "min=532.353, mean=532.353, max=532.353, sum=532.353 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20high_school_mathematics&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_mathematics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Dhigh_school_mathematics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Dhigh_school_mathematics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Dhigh_school_mathematics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Dhigh_school_mathematics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:high_school_mathematics.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:high_school_mathematics.json"
      }
    ],
    "name": "mmlu_subject:high_school_mathematics"
  },
  {
    "title": "subject: high_school_microeconomics",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_microeconomics&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_microeconomics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.47368421052631576,
          "description": "min=0.474, mean=0.474, max=0.474, sum=0.474 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.47368421052631576,
          "description": "min=0.474, mean=0.474, max=0.474, sum=0.474 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.47368421052631576,
          "description": "min=0.474, mean=0.474, max=0.474, sum=0.474 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 435.0,
          "description": "min=435, mean=435, max=435, sum=435 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_microeconomics&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_microeconomics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 435.0,
          "description": "min=435, mean=435, max=435, sum=435 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_microeconomics&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_microeconomics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.2631578947368421,
          "description": "min=0.263, mean=0.263, max=0.263, sum=0.263 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.2631578947368421,
          "description": "min=0.263, mean=0.263, max=0.263, sum=0.263 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.2631578947368421,
          "description": "min=0.263, mean=0.263, max=0.263, sum=0.263 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 435.0,
          "description": "min=435, mean=435, max=435, sum=435 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.7894736842105263,
          "description": "min=1.789, mean=1.789, max=1.789, sum=1.789 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_microeconomics&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_microeconomics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.8421052631578947,
          "description": "min=0.842, mean=0.842, max=0.842, sum=0.842 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.8421052631578947,
          "description": "min=0.842, mean=0.842, max=0.842, sum=0.842 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.8421052631578947,
          "description": "min=0.842, mean=0.842, max=0.842, sum=0.842 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 435.0,
          "description": "min=435, mean=435, max=435, sum=435 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_microeconomics&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_microeconomics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.7368421052631579,
          "description": "min=0.737, mean=0.737, max=0.737, sum=0.737 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.7368421052631579,
          "description": "min=0.737, mean=0.737, max=0.737, sum=0.737 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.7368421052631579,
          "description": "min=0.737, mean=0.737, max=0.737, sum=0.737 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 391.0,
          "description": "min=391, mean=391, max=391, sum=391 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20high_school_microeconomics&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_microeconomics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Dhigh_school_microeconomics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Dhigh_school_microeconomics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Dhigh_school_microeconomics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Dhigh_school_microeconomics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:high_school_microeconomics.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:high_school_microeconomics.json"
      }
    ],
    "name": "mmlu_subject:high_school_microeconomics"
  },
  {
    "title": "subject: high_school_physics",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_physics&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_physics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.2222222222222222,
          "description": "min=0.222, mean=0.222, max=0.222, sum=0.222 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.2222222222222222,
          "description": "min=0.222, mean=0.222, max=0.222, sum=0.222 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.2222222222222222,
          "description": "min=0.222, mean=0.222, max=0.222, sum=0.222 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 599.4444444444445,
          "description": "min=599.444, mean=599.444, max=599.444, sum=599.444 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_physics&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_physics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.3888888888888889,
          "description": "min=0.389, mean=0.389, max=0.389, sum=0.389 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.3888888888888889,
          "description": "min=0.389, mean=0.389, max=0.389, sum=0.389 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.3888888888888889,
          "description": "min=0.389, mean=0.389, max=0.389, sum=0.389 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 599.4444444444445,
          "description": "min=599.444, mean=599.444, max=599.444, sum=599.444 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_physics&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_physics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.2777777777777778,
          "description": "min=0.278, mean=0.278, max=0.278, sum=0.278 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.2777777777777778,
          "description": "min=0.278, mean=0.278, max=0.278, sum=0.278 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.2777777777777778,
          "description": "min=0.278, mean=0.278, max=0.278, sum=0.278 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 599.4444444444445,
          "description": "min=599.444, mean=599.444, max=599.444, sum=599.444 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.5555555555555556,
          "description": "min=1.556, mean=1.556, max=1.556, sum=1.556 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_physics&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_physics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.3333333333333333,
          "description": "min=0.333, mean=0.333, max=0.333, sum=0.333 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.3333333333333333,
          "description": "min=0.333, mean=0.333, max=0.333, sum=0.333 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.3333333333333333,
          "description": "min=0.333, mean=0.333, max=0.333, sum=0.333 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 599.4444444444445,
          "description": "min=599.444, mean=599.444, max=599.444, sum=599.444 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_physics&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_physics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_physics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.2777777777777778,
          "description": "min=0.278, mean=0.278, max=0.278, sum=0.278 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.2777777777777778,
          "description": "min=0.278, mean=0.278, max=0.278, sum=0.278 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.2777777777777778,
          "description": "min=0.278, mean=0.278, max=0.278, sum=0.278 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 555.4444444444445,
          "description": "min=555.444, mean=555.444, max=555.444, sum=555.444 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20high_school_physics&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_physics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Dhigh_school_physics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Dhigh_school_physics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Dhigh_school_physics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Dhigh_school_physics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:high_school_physics.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:high_school_physics.json"
      }
    ],
    "name": "mmlu_subject:high_school_physics"
  },
  {
    "title": "subject: high_school_psychology",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_psychology&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_psychology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.9444444444444444,
          "description": "min=0.944, mean=0.944, max=0.944, sum=0.944 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.9444444444444444,
          "description": "min=0.944, mean=0.944, max=0.944, sum=0.944 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.9444444444444444,
          "description": "min=0.944, mean=0.944, max=0.944, sum=0.944 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 532.2222222222222,
          "description": "min=532.222, mean=532.222, max=532.222, sum=532.222 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_psychology&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_psychology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 532.2222222222222,
          "description": "min=532.222, mean=532.222, max=532.222, sum=532.222 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_psychology&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_psychology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.5555555555555556,
          "description": "min=0.556, mean=0.556, max=0.556, sum=0.556 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.5555555555555556,
          "description": "min=0.556, mean=0.556, max=0.556, sum=0.556 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.5555555555555556,
          "description": "min=0.556, mean=0.556, max=0.556, sum=0.556 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 532.2222222222222,
          "description": "min=532.222, mean=532.222, max=532.222, sum=532.222 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.2222222222222223,
          "description": "min=1.222, mean=1.222, max=1.222, sum=1.222 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_psychology&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_psychology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.9444444444444444,
          "description": "min=0.944, mean=0.944, max=0.944, sum=0.944 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.9444444444444444,
          "description": "min=0.944, mean=0.944, max=0.944, sum=0.944 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.9444444444444444,
          "description": "min=0.944, mean=0.944, max=0.944, sum=0.944 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 532.2222222222222,
          "description": "min=532.222, mean=532.222, max=532.222, sum=532.222 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_psychology&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_psychology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.8888888888888888,
          "description": "min=0.889, mean=0.889, max=0.889, sum=0.889 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.8888888888888888,
          "description": "min=0.889, mean=0.889, max=0.889, sum=0.889 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.8888888888888888,
          "description": "min=0.889, mean=0.889, max=0.889, sum=0.889 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 488.22222222222223,
          "description": "min=488.222, mean=488.222, max=488.222, sum=488.222 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20high_school_psychology&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_psychology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Dhigh_school_psychology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Dhigh_school_psychology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Dhigh_school_psychology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Dhigh_school_psychology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:high_school_psychology.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:high_school_psychology.json"
      }
    ],
    "name": "mmlu_subject:high_school_psychology"
  },
  {
    "title": "subject: high_school_statistics",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_statistics&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_statistics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.3157894736842105,
          "description": "min=0.316, mean=0.316, max=0.316, sum=0.316 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.3157894736842105,
          "description": "min=0.316, mean=0.316, max=0.316, sum=0.316 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.3157894736842105,
          "description": "min=0.316, mean=0.316, max=0.316, sum=0.316 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 841.7368421052631,
          "description": "min=841.737, mean=841.737, max=841.737, sum=841.737 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_statistics&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_statistics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.7894736842105263,
          "description": "min=0.789, mean=0.789, max=0.789, sum=0.789 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.7894736842105263,
          "description": "min=0.789, mean=0.789, max=0.789, sum=0.789 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.7894736842105263,
          "description": "min=0.789, mean=0.789, max=0.789, sum=0.789 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 841.7368421052631,
          "description": "min=841.737, mean=841.737, max=841.737, sum=841.737 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_statistics&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_statistics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.3157894736842105,
          "description": "min=0.316, mean=0.316, max=0.316, sum=0.316 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.3157894736842105,
          "description": "min=0.316, mean=0.316, max=0.316, sum=0.316 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.3157894736842105,
          "description": "min=0.316, mean=0.316, max=0.316, sum=0.316 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 841.7368421052631,
          "description": "min=841.737, mean=841.737, max=841.737, sum=841.737 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_statistics&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_statistics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.631578947368421,
          "description": "min=0.632, mean=0.632, max=0.632, sum=0.632 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.631578947368421,
          "description": "min=0.632, mean=0.632, max=0.632, sum=0.632 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.631578947368421,
          "description": "min=0.632, mean=0.632, max=0.632, sum=0.632 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 841.7368421052631,
          "description": "min=841.737, mean=841.737, max=841.737, sum=841.737 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_statistics&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_statistics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.47368421052631576,
          "description": "min=0.474, mean=0.474, max=0.474, sum=0.474 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.47368421052631576,
          "description": "min=0.474, mean=0.474, max=0.474, sum=0.474 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.47368421052631576,
          "description": "min=0.474, mean=0.474, max=0.474, sum=0.474 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 797.7368421052631,
          "description": "min=797.737, mean=797.737, max=797.737, sum=797.737 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20high_school_statistics&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_statistics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Dhigh_school_statistics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Dhigh_school_statistics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Dhigh_school_statistics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Dhigh_school_statistics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:high_school_statistics.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:high_school_statistics.json"
      }
    ],
    "name": "mmlu_subject:high_school_statistics"
  },
  {
    "title": "subject: high_school_us_history",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_us_history&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_us_history%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.7777777777777778,
          "description": "min=0.778, mean=0.778, max=0.778, sum=0.778 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.7777777777777778,
          "description": "min=0.778, mean=0.778, max=0.778, sum=0.778 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.7777777777777778,
          "description": "min=0.778, mean=0.778, max=0.778, sum=0.778 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 4.0,
          "description": "min=4, mean=4, max=4, sum=4 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1733.2777777777778,
          "description": "min=1733.278, mean=1733.278, max=1733.278, sum=1733.278 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_us_history&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_us_history%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 4.0,
          "description": "min=4, mean=4, max=4, sum=4 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1733.2777777777778,
          "description": "min=1733.278, mean=1733.278, max=1733.278, sum=1733.278 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_us_history&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_us_history%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.2222222222222222,
          "description": "min=0.222, mean=0.222, max=0.222, sum=0.222 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.2222222222222222,
          "description": "min=0.222, mean=0.222, max=0.222, sum=0.222 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.2222222222222222,
          "description": "min=0.222, mean=0.222, max=0.222, sum=0.222 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 2209.277777777778,
          "description": "min=2209.278, mean=2209.278, max=2209.278, sum=2209.278 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.222222222222222,
          "description": "min=5.222, mean=5.222, max=5.222, sum=5.222 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_us_history&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_us_history%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.9444444444444444,
          "description": "min=0.944, mean=0.944, max=0.944, sum=0.944 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.9444444444444444,
          "description": "min=0.944, mean=0.944, max=0.944, sum=0.944 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.9444444444444444,
          "description": "min=0.944, mean=0.944, max=0.944, sum=0.944 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 2209.277777777778,
          "description": "min=2209.278, mean=2209.278, max=2209.278, sum=2209.278 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0555555555555556,
          "description": "min=1.056, mean=1.056, max=1.056, sum=1.056 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_us_history&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_us_history%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.8888888888888888,
          "description": "min=0.889, mean=0.889, max=0.889, sum=0.889 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.8888888888888888,
          "description": "min=0.889, mean=0.889, max=0.889, sum=0.889 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.8888888888888888,
          "description": "min=0.889, mean=0.889, max=0.889, sum=0.889 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 4.055555555555555,
          "description": "min=4.056, mean=4.056, max=4.056, sum=4.056 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1715.7222222222222,
          "description": "min=1715.722, mean=1715.722, max=1715.722, sum=1715.722 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20high_school_us_history&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_us_history%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Dhigh_school_us_history%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Dhigh_school_us_history%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Dhigh_school_us_history%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Dhigh_school_us_history%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:high_school_us_history.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:high_school_us_history.json"
      }
    ],
    "name": "mmlu_subject:high_school_us_history"
  },
  {
    "title": "subject: high_school_world_history",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_world_history&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_world_history%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.631578947368421,
          "description": "min=0.632, mean=0.632, max=0.632, sum=0.632 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.631578947368421,
          "description": "min=0.632, mean=0.632, max=0.632, sum=0.632 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.631578947368421,
          "description": "min=0.632, mean=0.632, max=0.632, sum=0.632 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1477.3684210526317,
          "description": "min=1477.368, mean=1477.368, max=1477.368, sum=1477.368 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_world_history&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_world_history%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.8421052631578947,
          "description": "min=0.842, mean=0.842, max=0.842, sum=0.842 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.8421052631578947,
          "description": "min=0.842, mean=0.842, max=0.842, sum=0.842 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.8421052631578947,
          "description": "min=0.842, mean=0.842, max=0.842, sum=0.842 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1477.3684210526317,
          "description": "min=1477.368, mean=1477.368, max=1477.368, sum=1477.368 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_world_history&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_world_history%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.42105263157894735,
          "description": "min=0.421, mean=0.421, max=0.421, sum=0.421 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.42105263157894735,
          "description": "min=0.421, mean=0.421, max=0.421, sum=0.421 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.42105263157894735,
          "description": "min=0.421, mean=0.421, max=0.421, sum=0.421 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1477.3684210526317,
          "description": "min=1477.368, mean=1477.368, max=1477.368, sum=1477.368 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20high_school_world_history&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_world_history%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.7894736842105263,
          "description": "min=0.789, mean=0.789, max=0.789, sum=0.789 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.7894736842105263,
          "description": "min=0.789, mean=0.789, max=0.789, sum=0.789 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.7894736842105263,
          "description": "min=0.789, mean=0.789, max=0.789, sum=0.789 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1433.3684210526317,
          "description": "min=1433.368, mean=1433.368, max=1433.368, sum=1433.368 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20high_school_world_history&runSpecs=%5B%22mmlu%3Asubject%3Dhigh_school_world_history%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Dhigh_school_world_history%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Dhigh_school_world_history%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Dhigh_school_world_history%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:high_school_world_history.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:high_school_world_history.json"
      }
    ],
    "name": "mmlu_subject:high_school_world_history"
  },
  {
    "title": "subject: human_aging",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20human_aging&runSpecs=%5B%22mmlu%3Asubject%3Dhuman_aging%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=human_aging,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.65,
          "description": "min=0.65, mean=0.65, max=0.65, sum=0.65 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.65,
          "description": "min=0.65, mean=0.65, max=0.65, sum=0.65 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.65,
          "description": "min=0.65, mean=0.65, max=0.65, sum=0.65 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 20.0,
          "description": "min=20, mean=20, max=20, sum=20 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 356.0,
          "description": "min=356, mean=356, max=356, sum=356 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20human_aging&runSpecs=%5B%22mmlu%3Asubject%3Dhuman_aging%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=human_aging,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.8,
          "description": "min=0.8, mean=0.8, max=0.8, sum=0.8 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.8,
          "description": "min=0.8, mean=0.8, max=0.8, sum=0.8 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.8,
          "description": "min=0.8, mean=0.8, max=0.8, sum=0.8 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 20.0,
          "description": "min=20, mean=20, max=20, sum=20 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 356.0,
          "description": "min=356, mean=356, max=356, sum=356 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20human_aging&runSpecs=%5B%22mmlu%3Asubject%3Dhuman_aging%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=human_aging,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.45,
          "description": "min=0.45, mean=0.45, max=0.45, sum=0.45 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.45,
          "description": "min=0.45, mean=0.45, max=0.45, sum=0.45 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.45,
          "description": "min=0.45, mean=0.45, max=0.45, sum=0.45 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 20.0,
          "description": "min=20, mean=20, max=20, sum=20 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 356.0,
          "description": "min=356, mean=356, max=356, sum=356 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20human_aging&runSpecs=%5B%22mmlu%3Asubject%3Dhuman_aging%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=human_aging,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.8,
          "description": "min=0.8, mean=0.8, max=0.8, sum=0.8 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.8,
          "description": "min=0.8, mean=0.8, max=0.8, sum=0.8 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.8,
          "description": "min=0.8, mean=0.8, max=0.8, sum=0.8 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 20.0,
          "description": "min=20, mean=20, max=20, sum=20 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 356.0,
          "description": "min=356, mean=356, max=356, sum=356 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20human_aging&runSpecs=%5B%22mmlu%3Asubject%3Dhuman_aging%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=human_aging,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.75,
          "description": "min=0.75, mean=0.75, max=0.75, sum=0.75 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.75,
          "description": "min=0.75, mean=0.75, max=0.75, sum=0.75 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.75,
          "description": "min=0.75, mean=0.75, max=0.75, sum=0.75 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 20.0,
          "description": "min=20, mean=20, max=20, sum=20 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 312.0,
          "description": "min=312, mean=312, max=312, sum=312 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20human_aging&runSpecs=%5B%22mmlu%3Asubject%3Dhuman_aging%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Dhuman_aging%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Dhuman_aging%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Dhuman_aging%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Dhuman_aging%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:human_aging.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:human_aging.json"
      }
    ],
    "name": "mmlu_subject:human_aging"
  },
  {
    "title": "subject: human_sexuality",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20human_sexuality&runSpecs=%5B%22mmlu%3Asubject%3Dhuman_sexuality%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.6666666666666666,
          "description": "min=0.667, mean=0.667, max=0.667, sum=0.667 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.6666666666666666,
          "description": "min=0.667, mean=0.667, max=0.667, sum=0.667 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.6666666666666666,
          "description": "min=0.667, mean=0.667, max=0.667, sum=0.667 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 382.27777777777777,
          "description": "min=382.278, mean=382.278, max=382.278, sum=382.278 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20human_sexuality&runSpecs=%5B%22mmlu%3Asubject%3Dhuman_sexuality%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.7222222222222222,
          "description": "min=0.722, mean=0.722, max=0.722, sum=0.722 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.7222222222222222,
          "description": "min=0.722, mean=0.722, max=0.722, sum=0.722 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.7222222222222222,
          "description": "min=0.722, mean=0.722, max=0.722, sum=0.722 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 382.27777777777777,
          "description": "min=382.278, mean=382.278, max=382.278, sum=382.278 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20human_sexuality&runSpecs=%5B%22mmlu%3Asubject%3Dhuman_sexuality%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.3888888888888889,
          "description": "min=0.389, mean=0.389, max=0.389, sum=0.389 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.3888888888888889,
          "description": "min=0.389, mean=0.389, max=0.389, sum=0.389 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.3888888888888889,
          "description": "min=0.389, mean=0.389, max=0.389, sum=0.389 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 382.27777777777777,
          "description": "min=382.278, mean=382.278, max=382.278, sum=382.278 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 2.0555555555555554,
          "description": "min=2.056, mean=2.056, max=2.056, sum=2.056 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20human_sexuality&runSpecs=%5B%22mmlu%3Asubject%3Dhuman_sexuality%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.8333333333333334,
          "description": "min=0.833, mean=0.833, max=0.833, sum=0.833 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.8333333333333334,
          "description": "min=0.833, mean=0.833, max=0.833, sum=0.833 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.8333333333333334,
          "description": "min=0.833, mean=0.833, max=0.833, sum=0.833 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 382.27777777777777,
          "description": "min=382.278, mean=382.278, max=382.278, sum=382.278 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20human_sexuality&runSpecs=%5B%22mmlu%3Asubject%3Dhuman_sexuality%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=human_sexuality,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.6111111111111112,
          "description": "min=0.611, mean=0.611, max=0.611, sum=0.611 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.6111111111111112,
          "description": "min=0.611, mean=0.611, max=0.611, sum=0.611 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.6111111111111112,
          "description": "min=0.611, mean=0.611, max=0.611, sum=0.611 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 338.27777777777777,
          "description": "min=338.278, mean=338.278, max=338.278, sum=338.278 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20human_sexuality&runSpecs=%5B%22mmlu%3Asubject%3Dhuman_sexuality%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Dhuman_sexuality%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Dhuman_sexuality%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Dhuman_sexuality%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Dhuman_sexuality%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:human_sexuality.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:human_sexuality.json"
      }
    ],
    "name": "mmlu_subject:human_sexuality"
  },
  {
    "title": "subject: international_law",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20international_law&runSpecs=%5B%22mmlu%3Asubject%3Dinternational_law%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=international_law,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.6111111111111112,
          "description": "min=0.611, mean=0.611, max=0.611, sum=0.611 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.6111111111111112,
          "description": "min=0.611, mean=0.611, max=0.611, sum=0.611 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.6111111111111112,
          "description": "min=0.611, mean=0.611, max=0.611, sum=0.611 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 670.5,
          "description": "min=670.5, mean=670.5, max=670.5, sum=670.5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20international_law&runSpecs=%5B%22mmlu%3Asubject%3Dinternational_law%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=international_law,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.9444444444444444,
          "description": "min=0.944, mean=0.944, max=0.944, sum=0.944 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.9444444444444444,
          "description": "min=0.944, mean=0.944, max=0.944, sum=0.944 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.9444444444444444,
          "description": "min=0.944, mean=0.944, max=0.944, sum=0.944 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 670.5,
          "description": "min=670.5, mean=670.5, max=670.5, sum=670.5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20international_law&runSpecs=%5B%22mmlu%3Asubject%3Dinternational_law%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=international_law,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.6111111111111112,
          "description": "min=0.611, mean=0.611, max=0.611, sum=0.611 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.6111111111111112,
          "description": "min=0.611, mean=0.611, max=0.611, sum=0.611 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.6111111111111112,
          "description": "min=0.611, mean=0.611, max=0.611, sum=0.611 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 670.5,
          "description": "min=670.5, mean=670.5, max=670.5, sum=670.5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20international_law&runSpecs=%5B%22mmlu%3Asubject%3Dinternational_law%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=international_law,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.8333333333333334,
          "description": "min=0.833, mean=0.833, max=0.833, sum=0.833 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.8333333333333334,
          "description": "min=0.833, mean=0.833, max=0.833, sum=0.833 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.8333333333333334,
          "description": "min=0.833, mean=0.833, max=0.833, sum=0.833 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 670.5,
          "description": "min=670.5, mean=670.5, max=670.5, sum=670.5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20international_law&runSpecs=%5B%22mmlu%3Asubject%3Dinternational_law%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=international_law,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.8333333333333334,
          "description": "min=0.833, mean=0.833, max=0.833, sum=0.833 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.8333333333333334,
          "description": "min=0.833, mean=0.833, max=0.833, sum=0.833 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.8333333333333334,
          "description": "min=0.833, mean=0.833, max=0.833, sum=0.833 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 626.5,
          "description": "min=626.5, mean=626.5, max=626.5, sum=626.5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20international_law&runSpecs=%5B%22mmlu%3Asubject%3Dinternational_law%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Dinternational_law%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Dinternational_law%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Dinternational_law%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Dinternational_law%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:international_law.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:international_law.json"
      }
    ],
    "name": "mmlu_subject:international_law"
  },
  {
    "title": "subject: jurisprudence",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20jurisprudence&runSpecs=%5B%22mmlu%3Asubject%3Djurisprudence%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.5,
          "description": "min=0.5, mean=0.5, max=0.5, sum=0.5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.5,
          "description": "min=0.5, mean=0.5, max=0.5, sum=0.5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.5,
          "description": "min=0.5, mean=0.5, max=0.5, sum=0.5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 16.0,
          "description": "min=16, mean=16, max=16, sum=16 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 430.3125,
          "description": "min=430.312, mean=430.312, max=430.312, sum=430.312 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20jurisprudence&runSpecs=%5B%22mmlu%3Asubject%3Djurisprudence%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.625,
          "description": "min=0.625, mean=0.625, max=0.625, sum=0.625 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.625,
          "description": "min=0.625, mean=0.625, max=0.625, sum=0.625 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.625,
          "description": "min=0.625, mean=0.625, max=0.625, sum=0.625 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 16.0,
          "description": "min=16, mean=16, max=16, sum=16 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 430.3125,
          "description": "min=430.312, mean=430.312, max=430.312, sum=430.312 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20jurisprudence&runSpecs=%5B%22mmlu%3Asubject%3Djurisprudence%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.25,
          "description": "min=0.25, mean=0.25, max=0.25, sum=0.25 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.25,
          "description": "min=0.25, mean=0.25, max=0.25, sum=0.25 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.25,
          "description": "min=0.25, mean=0.25, max=0.25, sum=0.25 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 16.0,
          "description": "min=16, mean=16, max=16, sum=16 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 430.3125,
          "description": "min=430.312, mean=430.312, max=430.312, sum=430.312 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20jurisprudence&runSpecs=%5B%22mmlu%3Asubject%3Djurisprudence%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.75,
          "description": "min=0.75, mean=0.75, max=0.75, sum=0.75 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.75,
          "description": "min=0.75, mean=0.75, max=0.75, sum=0.75 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.75,
          "description": "min=0.75, mean=0.75, max=0.75, sum=0.75 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 16.0,
          "description": "min=16, mean=16, max=16, sum=16 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 430.3125,
          "description": "min=430.312, mean=430.312, max=430.312, sum=430.312 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20jurisprudence&runSpecs=%5B%22mmlu%3Asubject%3Djurisprudence%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=jurisprudence,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.625,
          "description": "min=0.625, mean=0.625, max=0.625, sum=0.625 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.625,
          "description": "min=0.625, mean=0.625, max=0.625, sum=0.625 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.625,
          "description": "min=0.625, mean=0.625, max=0.625, sum=0.625 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 16.0,
          "description": "min=16, mean=16, max=16, sum=16 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 386.3125,
          "description": "min=386.312, mean=386.312, max=386.312, sum=386.312 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20jurisprudence&runSpecs=%5B%22mmlu%3Asubject%3Djurisprudence%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Djurisprudence%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Djurisprudence%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Djurisprudence%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Djurisprudence%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:jurisprudence.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:jurisprudence.json"
      }
    ],
    "name": "mmlu_subject:jurisprudence"
  },
  {
    "title": "subject: logical_fallacies",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20logical_fallacies&runSpecs=%5B%22mmlu%3Asubject%3Dlogical_fallacies%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.5,
          "description": "min=0.5, mean=0.5, max=0.5, sum=0.5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.5,
          "description": "min=0.5, mean=0.5, max=0.5, sum=0.5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.5,
          "description": "min=0.5, mean=0.5, max=0.5, sum=0.5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 488.5,
          "description": "min=488.5, mean=488.5, max=488.5, sum=488.5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20logical_fallacies&runSpecs=%5B%22mmlu%3Asubject%3Dlogical_fallacies%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.7777777777777778,
          "description": "min=0.778, mean=0.778, max=0.778, sum=0.778 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.7777777777777778,
          "description": "min=0.778, mean=0.778, max=0.778, sum=0.778 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.7777777777777778,
          "description": "min=0.778, mean=0.778, max=0.778, sum=0.778 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 488.5,
          "description": "min=488.5, mean=488.5, max=488.5, sum=488.5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20logical_fallacies&runSpecs=%5B%22mmlu%3Asubject%3Dlogical_fallacies%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.3333333333333333,
          "description": "min=0.333, mean=0.333, max=0.333, sum=0.333 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.3333333333333333,
          "description": "min=0.333, mean=0.333, max=0.333, sum=0.333 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.3333333333333333,
          "description": "min=0.333, mean=0.333, max=0.333, sum=0.333 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 488.5,
          "description": "min=488.5, mean=488.5, max=488.5, sum=488.5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20logical_fallacies&runSpecs=%5B%22mmlu%3Asubject%3Dlogical_fallacies%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.7777777777777778,
          "description": "min=0.778, mean=0.778, max=0.778, sum=0.778 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.7777777777777778,
          "description": "min=0.778, mean=0.778, max=0.778, sum=0.778 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.7777777777777778,
          "description": "min=0.778, mean=0.778, max=0.778, sum=0.778 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 488.5,
          "description": "min=488.5, mean=488.5, max=488.5, sum=488.5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20logical_fallacies&runSpecs=%5B%22mmlu%3Asubject%3Dlogical_fallacies%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.7777777777777778,
          "description": "min=0.778, mean=0.778, max=0.778, sum=0.778 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.7777777777777778,
          "description": "min=0.778, mean=0.778, max=0.778, sum=0.778 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.7777777777777778,
          "description": "min=0.778, mean=0.778, max=0.778, sum=0.778 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 444.5,
          "description": "min=444.5, mean=444.5, max=444.5, sum=444.5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20logical_fallacies&runSpecs=%5B%22mmlu%3Asubject%3Dlogical_fallacies%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Dlogical_fallacies%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Dlogical_fallacies%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Dlogical_fallacies%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Dlogical_fallacies%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:logical_fallacies.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:logical_fallacies.json"
      }
    ],
    "name": "mmlu_subject:logical_fallacies"
  },
  {
    "title": "subject: machine_learning",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20machine_learning&runSpecs=%5B%22mmlu%3Asubject%3Dmachine_learning%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=machine_learning,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.3125,
          "description": "min=0.312, mean=0.312, max=0.312, sum=0.312 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.3125,
          "description": "min=0.312, mean=0.312, max=0.312, sum=0.312 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.3125,
          "description": "min=0.312, mean=0.312, max=0.312, sum=0.312 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 16.0,
          "description": "min=16, mean=16, max=16, sum=16 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 708.0625,
          "description": "min=708.062, mean=708.062, max=708.062, sum=708.062 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20machine_learning&runSpecs=%5B%22mmlu%3Asubject%3Dmachine_learning%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=machine_learning,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.75,
          "description": "min=0.75, mean=0.75, max=0.75, sum=0.75 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.75,
          "description": "min=0.75, mean=0.75, max=0.75, sum=0.75 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.75,
          "description": "min=0.75, mean=0.75, max=0.75, sum=0.75 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 16.0,
          "description": "min=16, mean=16, max=16, sum=16 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 708.0625,
          "description": "min=708.062, mean=708.062, max=708.062, sum=708.062 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20machine_learning&runSpecs=%5B%22mmlu%3Asubject%3Dmachine_learning%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=machine_learning,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.25,
          "description": "min=0.25, mean=0.25, max=0.25, sum=0.25 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.25,
          "description": "min=0.25, mean=0.25, max=0.25, sum=0.25 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.25,
          "description": "min=0.25, mean=0.25, max=0.25, sum=0.25 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 16.0,
          "description": "min=16, mean=16, max=16, sum=16 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 708.0625,
          "description": "min=708.062, mean=708.062, max=708.062, sum=708.062 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20machine_learning&runSpecs=%5B%22mmlu%3Asubject%3Dmachine_learning%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=machine_learning,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.625,
          "description": "min=0.625, mean=0.625, max=0.625, sum=0.625 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.625,
          "description": "min=0.625, mean=0.625, max=0.625, sum=0.625 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.625,
          "description": "min=0.625, mean=0.625, max=0.625, sum=0.625 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 16.0,
          "description": "min=16, mean=16, max=16, sum=16 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 708.0625,
          "description": "min=708.062, mean=708.062, max=708.062, sum=708.062 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20machine_learning&runSpecs=%5B%22mmlu%3Asubject%3Dmachine_learning%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=machine_learning,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.4375,
          "description": "min=0.438, mean=0.438, max=0.438, sum=0.438 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.4375,
          "description": "min=0.438, mean=0.438, max=0.438, sum=0.438 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.4375,
          "description": "min=0.438, mean=0.438, max=0.438, sum=0.438 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 16.0,
          "description": "min=16, mean=16, max=16, sum=16 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 664.0625,
          "description": "min=664.062, mean=664.062, max=664.062, sum=664.062 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20machine_learning&runSpecs=%5B%22mmlu%3Asubject%3Dmachine_learning%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Dmachine_learning%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Dmachine_learning%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Dmachine_learning%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Dmachine_learning%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:machine_learning.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:machine_learning.json"
      }
    ],
    "name": "mmlu_subject:machine_learning"
  },
  {
    "title": "subject: management",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20management&runSpecs=%5B%22mmlu%3Asubject%3Dmanagement%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=management,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.6470588235294118,
          "description": "min=0.647, mean=0.647, max=0.647, sum=0.647 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.6470588235294118,
          "description": "min=0.647, mean=0.647, max=0.647, sum=0.647 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.6470588235294118,
          "description": "min=0.647, mean=0.647, max=0.647, sum=0.647 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 319.47058823529414,
          "description": "min=319.471, mean=319.471, max=319.471, sum=319.471 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20management&runSpecs=%5B%22mmlu%3Asubject%3Dmanagement%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=management,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.8823529411764706,
          "description": "min=0.882, mean=0.882, max=0.882, sum=0.882 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.8823529411764706,
          "description": "min=0.882, mean=0.882, max=0.882, sum=0.882 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.8823529411764706,
          "description": "min=0.882, mean=0.882, max=0.882, sum=0.882 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 319.47058823529414,
          "description": "min=319.471, mean=319.471, max=319.471, sum=319.471 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20management&runSpecs=%5B%22mmlu%3Asubject%3Dmanagement%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=management,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.47058823529411764,
          "description": "min=0.471, mean=0.471, max=0.471, sum=0.471 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.47058823529411764,
          "description": "min=0.471, mean=0.471, max=0.471, sum=0.471 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.47058823529411764,
          "description": "min=0.471, mean=0.471, max=0.471, sum=0.471 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 319.47058823529414,
          "description": "min=319.471, mean=319.471, max=319.471, sum=319.471 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20management&runSpecs=%5B%22mmlu%3Asubject%3Dmanagement%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=management,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.8235294117647058,
          "description": "min=0.824, mean=0.824, max=0.824, sum=0.824 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.8235294117647058,
          "description": "min=0.824, mean=0.824, max=0.824, sum=0.824 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.8235294117647058,
          "description": "min=0.824, mean=0.824, max=0.824, sum=0.824 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 319.47058823529414,
          "description": "min=319.471, mean=319.471, max=319.471, sum=319.471 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20management&runSpecs=%5B%22mmlu%3Asubject%3Dmanagement%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=management,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.8235294117647058,
          "description": "min=0.824, mean=0.824, max=0.824, sum=0.824 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.8235294117647058,
          "description": "min=0.824, mean=0.824, max=0.824, sum=0.824 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.8235294117647058,
          "description": "min=0.824, mean=0.824, max=0.824, sum=0.824 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 275.47058823529414,
          "description": "min=275.471, mean=275.471, max=275.471, sum=275.471 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20management&runSpecs=%5B%22mmlu%3Asubject%3Dmanagement%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Dmanagement%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Dmanagement%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Dmanagement%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Dmanagement%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:management.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:management.json"
      }
    ],
    "name": "mmlu_subject:management"
  },
  {
    "title": "subject: marketing",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20marketing&runSpecs=%5B%22mmlu%3Asubject%3Dmarketing%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=marketing,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.8421052631578947,
          "description": "min=0.842, mean=0.842, max=0.842, sum=0.842 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.8421052631578947,
          "description": "min=0.842, mean=0.842, max=0.842, sum=0.842 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.8421052631578947,
          "description": "min=0.842, mean=0.842, max=0.842, sum=0.842 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 440.2105263157895,
          "description": "min=440.211, mean=440.211, max=440.211, sum=440.211 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20marketing&runSpecs=%5B%22mmlu%3Asubject%3Dmarketing%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=marketing,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.8947368421052632,
          "description": "min=0.895, mean=0.895, max=0.895, sum=0.895 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.8947368421052632,
          "description": "min=0.895, mean=0.895, max=0.895, sum=0.895 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.8947368421052632,
          "description": "min=0.895, mean=0.895, max=0.895, sum=0.895 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 440.2105263157895,
          "description": "min=440.211, mean=440.211, max=440.211, sum=440.211 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20marketing&runSpecs=%5B%22mmlu%3Asubject%3Dmarketing%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=marketing,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.47368421052631576,
          "description": "min=0.474, mean=0.474, max=0.474, sum=0.474 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.47368421052631576,
          "description": "min=0.474, mean=0.474, max=0.474, sum=0.474 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.47368421052631576,
          "description": "min=0.474, mean=0.474, max=0.474, sum=0.474 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 440.2105263157895,
          "description": "min=440.211, mean=440.211, max=440.211, sum=440.211 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20marketing&runSpecs=%5B%22mmlu%3Asubject%3Dmarketing%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=marketing,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.8947368421052632,
          "description": "min=0.895, mean=0.895, max=0.895, sum=0.895 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.8947368421052632,
          "description": "min=0.895, mean=0.895, max=0.895, sum=0.895 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.8947368421052632,
          "description": "min=0.895, mean=0.895, max=0.895, sum=0.895 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 440.2105263157895,
          "description": "min=440.211, mean=440.211, max=440.211, sum=440.211 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20marketing&runSpecs=%5B%22mmlu%3Asubject%3Dmarketing%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=marketing,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.8947368421052632,
          "description": "min=0.895, mean=0.895, max=0.895, sum=0.895 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.8947368421052632,
          "description": "min=0.895, mean=0.895, max=0.895, sum=0.895 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.8947368421052632,
          "description": "min=0.895, mean=0.895, max=0.895, sum=0.895 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 396.2105263157895,
          "description": "min=396.211, mean=396.211, max=396.211, sum=396.211 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20marketing&runSpecs=%5B%22mmlu%3Asubject%3Dmarketing%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Dmarketing%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Dmarketing%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Dmarketing%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Dmarketing%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:marketing.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:marketing.json"
      }
    ],
    "name": "mmlu_subject:marketing"
  },
  {
    "title": "subject: medical_genetics",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20medical_genetics&runSpecs=%5B%22mmlu%3Asubject%3Dmedical_genetics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.6470588235294118,
          "description": "min=0.647, mean=0.647, max=0.647, sum=0.647 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.6470588235294118,
          "description": "min=0.647, mean=0.647, max=0.647, sum=0.647 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.6470588235294118,
          "description": "min=0.647, mean=0.647, max=0.647, sum=0.647 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 377.0,
          "description": "min=377, mean=377, max=377, sum=377 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20medical_genetics&runSpecs=%5B%22mmlu%3Asubject%3Dmedical_genetics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 377.0,
          "description": "min=377, mean=377, max=377, sum=377 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20medical_genetics&runSpecs=%5B%22mmlu%3Asubject%3Dmedical_genetics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.7058823529411765,
          "description": "min=0.706, mean=0.706, max=0.706, sum=0.706 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.7058823529411765,
          "description": "min=0.706, mean=0.706, max=0.706, sum=0.706 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.7058823529411765,
          "description": "min=0.706, mean=0.706, max=0.706, sum=0.706 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 377.0,
          "description": "min=377, mean=377, max=377, sum=377 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20medical_genetics&runSpecs=%5B%22mmlu%3Asubject%3Dmedical_genetics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.9411764705882353,
          "description": "min=0.941, mean=0.941, max=0.941, sum=0.941 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.9411764705882353,
          "description": "min=0.941, mean=0.941, max=0.941, sum=0.941 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.9411764705882353,
          "description": "min=0.941, mean=0.941, max=0.941, sum=0.941 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 377.0,
          "description": "min=377, mean=377, max=377, sum=377 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20medical_genetics&runSpecs=%5B%22mmlu%3Asubject%3Dmedical_genetics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=medical_genetics,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.8823529411764706,
          "description": "min=0.882, mean=0.882, max=0.882, sum=0.882 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.8823529411764706,
          "description": "min=0.882, mean=0.882, max=0.882, sum=0.882 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.8823529411764706,
          "description": "min=0.882, mean=0.882, max=0.882, sum=0.882 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 333.0,
          "description": "min=333, mean=333, max=333, sum=333 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20medical_genetics&runSpecs=%5B%22mmlu%3Asubject%3Dmedical_genetics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Dmedical_genetics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Dmedical_genetics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Dmedical_genetics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Dmedical_genetics%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:medical_genetics.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:medical_genetics.json"
      }
    ],
    "name": "mmlu_subject:medical_genetics"
  },
  {
    "title": "subject: miscellaneous",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20miscellaneous&runSpecs=%5B%22mmlu%3Asubject%3Dmiscellaneous%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.47368421052631576,
          "description": "min=0.474, mean=0.474, max=0.474, sum=0.474 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.47368421052631576,
          "description": "min=0.474, mean=0.474, max=0.474, sum=0.474 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.47368421052631576,
          "description": "min=0.474, mean=0.474, max=0.474, sum=0.474 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 337.0,
          "description": "min=337, mean=337, max=337, sum=337 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20miscellaneous&runSpecs=%5B%22mmlu%3Asubject%3Dmiscellaneous%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.7368421052631579,
          "description": "min=0.737, mean=0.737, max=0.737, sum=0.737 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.7368421052631579,
          "description": "min=0.737, mean=0.737, max=0.737, sum=0.737 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.7368421052631579,
          "description": "min=0.737, mean=0.737, max=0.737, sum=0.737 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 337.0,
          "description": "min=337, mean=337, max=337, sum=337 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20miscellaneous&runSpecs=%5B%22mmlu%3Asubject%3Dmiscellaneous%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.47368421052631576,
          "description": "min=0.474, mean=0.474, max=0.474, sum=0.474 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.47368421052631576,
          "description": "min=0.474, mean=0.474, max=0.474, sum=0.474 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.47368421052631576,
          "description": "min=0.474, mean=0.474, max=0.474, sum=0.474 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 337.0,
          "description": "min=337, mean=337, max=337, sum=337 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.263157894736842,
          "description": "min=1.263, mean=1.263, max=1.263, sum=1.263 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20miscellaneous&runSpecs=%5B%22mmlu%3Asubject%3Dmiscellaneous%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.7368421052631579,
          "description": "min=0.737, mean=0.737, max=0.737, sum=0.737 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.7368421052631579,
          "description": "min=0.737, mean=0.737, max=0.737, sum=0.737 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.7368421052631579,
          "description": "min=0.737, mean=0.737, max=0.737, sum=0.737 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 337.0,
          "description": "min=337, mean=337, max=337, sum=337 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20miscellaneous&runSpecs=%5B%22mmlu%3Asubject%3Dmiscellaneous%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=miscellaneous,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.7368421052631579,
          "description": "min=0.737, mean=0.737, max=0.737, sum=0.737 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.7368421052631579,
          "description": "min=0.737, mean=0.737, max=0.737, sum=0.737 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.7368421052631579,
          "description": "min=0.737, mean=0.737, max=0.737, sum=0.737 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 19.0,
          "description": "min=19, mean=19, max=19, sum=19 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 293.0,
          "description": "min=293, mean=293, max=293, sum=293 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20miscellaneous&runSpecs=%5B%22mmlu%3Asubject%3Dmiscellaneous%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Dmiscellaneous%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Dmiscellaneous%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Dmiscellaneous%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Dmiscellaneous%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:miscellaneous.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:miscellaneous.json"
      }
    ],
    "name": "mmlu_subject:miscellaneous"
  },
  {
    "title": "subject: moral_disputes",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20moral_disputes&runSpecs=%5B%22mmlu%3Asubject%3Dmoral_disputes%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.35,
          "description": "min=0.35, mean=0.35, max=0.35, sum=0.35 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.35,
          "description": "min=0.35, mean=0.35, max=0.35, sum=0.35 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.35,
          "description": "min=0.35, mean=0.35, max=0.35, sum=0.35 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 20.0,
          "description": "min=20, mean=20, max=20, sum=20 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 514.15,
          "description": "min=514.15, mean=514.15, max=514.15, sum=514.15 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20moral_disputes&runSpecs=%5B%22mmlu%3Asubject%3Dmoral_disputes%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.65,
          "description": "min=0.65, mean=0.65, max=0.65, sum=0.65 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.65,
          "description": "min=0.65, mean=0.65, max=0.65, sum=0.65 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.65,
          "description": "min=0.65, mean=0.65, max=0.65, sum=0.65 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 20.0,
          "description": "min=20, mean=20, max=20, sum=20 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 514.15,
          "description": "min=514.15, mean=514.15, max=514.15, sum=514.15 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20moral_disputes&runSpecs=%5B%22mmlu%3Asubject%3Dmoral_disputes%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.2,
          "description": "min=0.2, mean=0.2, max=0.2, sum=0.2 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.2,
          "description": "min=0.2, mean=0.2, max=0.2, sum=0.2 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.2,
          "description": "min=0.2, mean=0.2, max=0.2, sum=0.2 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 20.0,
          "description": "min=20, mean=20, max=20, sum=20 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 514.15,
          "description": "min=514.15, mean=514.15, max=514.15, sum=514.15 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20moral_disputes&runSpecs=%5B%22mmlu%3Asubject%3Dmoral_disputes%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.6,
          "description": "min=0.6, mean=0.6, max=0.6, sum=0.6 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.6,
          "description": "min=0.6, mean=0.6, max=0.6, sum=0.6 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.6,
          "description": "min=0.6, mean=0.6, max=0.6, sum=0.6 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 20.0,
          "description": "min=20, mean=20, max=20, sum=20 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 514.15,
          "description": "min=514.15, mean=514.15, max=514.15, sum=514.15 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20moral_disputes&runSpecs=%5B%22mmlu%3Asubject%3Dmoral_disputes%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=moral_disputes,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.55,
          "description": "min=0.55, mean=0.55, max=0.55, sum=0.55 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.55,
          "description": "min=0.55, mean=0.55, max=0.55, sum=0.55 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.55,
          "description": "min=0.55, mean=0.55, max=0.55, sum=0.55 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 20.0,
          "description": "min=20, mean=20, max=20, sum=20 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 470.15,
          "description": "min=470.15, mean=470.15, max=470.15, sum=470.15 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20moral_disputes&runSpecs=%5B%22mmlu%3Asubject%3Dmoral_disputes%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Dmoral_disputes%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Dmoral_disputes%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Dmoral_disputes%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Dmoral_disputes%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:moral_disputes.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:moral_disputes.json"
      }
    ],
    "name": "mmlu_subject:moral_disputes"
  },
  {
    "title": "subject: moral_scenarios",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20moral_scenarios&runSpecs=%5B%22mmlu%3Asubject%3Dmoral_scenarios%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.2777777777777778,
          "description": "min=0.278, mean=0.278, max=0.278, sum=0.278 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.2777777777777778,
          "description": "min=0.278, mean=0.278, max=0.278, sum=0.278 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.2777777777777778,
          "description": "min=0.278, mean=0.278, max=0.278, sum=0.278 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 691.0555555555555,
          "description": "min=691.056, mean=691.056, max=691.056, sum=691.056 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20moral_scenarios&runSpecs=%5B%22mmlu%3Asubject%3Dmoral_scenarios%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.2777777777777778,
          "description": "min=0.278, mean=0.278, max=0.278, sum=0.278 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.2777777777777778,
          "description": "min=0.278, mean=0.278, max=0.278, sum=0.278 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.2777777777777778,
          "description": "min=0.278, mean=0.278, max=0.278, sum=0.278 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 691.0555555555555,
          "description": "min=691.056, mean=691.056, max=691.056, sum=691.056 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20moral_scenarios&runSpecs=%5B%22mmlu%3Asubject%3Dmoral_scenarios%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.1111111111111111,
          "description": "min=0.111, mean=0.111, max=0.111, sum=0.111 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.1111111111111111,
          "description": "min=0.111, mean=0.111, max=0.111, sum=0.111 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.1111111111111111,
          "description": "min=0.111, mean=0.111, max=0.111, sum=0.111 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 691.0555555555555,
          "description": "min=691.056, mean=691.056, max=691.056, sum=691.056 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20moral_scenarios&runSpecs=%5B%22mmlu%3Asubject%3Dmoral_scenarios%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.3333333333333333,
          "description": "min=0.333, mean=0.333, max=0.333, sum=0.333 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.3333333333333333,
          "description": "min=0.333, mean=0.333, max=0.333, sum=0.333 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.3333333333333333,
          "description": "min=0.333, mean=0.333, max=0.333, sum=0.333 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 691.0555555555555,
          "description": "min=691.056, mean=691.056, max=691.056, sum=691.056 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20moral_scenarios&runSpecs=%5B%22mmlu%3Asubject%3Dmoral_scenarios%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.4444444444444444,
          "description": "min=0.444, mean=0.444, max=0.444, sum=0.444 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.4444444444444444,
          "description": "min=0.444, mean=0.444, max=0.444, sum=0.444 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.4444444444444444,
          "description": "min=0.444, mean=0.444, max=0.444, sum=0.444 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 647.0555555555555,
          "description": "min=647.056, mean=647.056, max=647.056, sum=647.056 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20moral_scenarios&runSpecs=%5B%22mmlu%3Asubject%3Dmoral_scenarios%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Dmoral_scenarios%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Dmoral_scenarios%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Dmoral_scenarios%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Dmoral_scenarios%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:moral_scenarios.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:moral_scenarios.json"
      }
    ],
    "name": "mmlu_subject:moral_scenarios"
  },
  {
    "title": "subject: nutrition",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20nutrition&runSpecs=%5B%22mmlu%3Asubject%3Dnutrition%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=nutrition,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.7777777777777778,
          "description": "min=0.778, mean=0.778, max=0.778, sum=0.778 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.7777777777777778,
          "description": "min=0.778, mean=0.778, max=0.778, sum=0.778 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.7777777777777778,
          "description": "min=0.778, mean=0.778, max=0.778, sum=0.778 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 619.8333333333334,
          "description": "min=619.833, mean=619.833, max=619.833, sum=619.833 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20nutrition&runSpecs=%5B%22mmlu%3Asubject%3Dnutrition%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=nutrition,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.7777777777777778,
          "description": "min=0.778, mean=0.778, max=0.778, sum=0.778 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.7777777777777778,
          "description": "min=0.778, mean=0.778, max=0.778, sum=0.778 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.7777777777777778,
          "description": "min=0.778, mean=0.778, max=0.778, sum=0.778 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 619.8333333333334,
          "description": "min=619.833, mean=619.833, max=619.833, sum=619.833 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20nutrition&runSpecs=%5B%22mmlu%3Asubject%3Dnutrition%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=nutrition,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.3333333333333333,
          "description": "min=0.333, mean=0.333, max=0.333, sum=0.333 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.3333333333333333,
          "description": "min=0.333, mean=0.333, max=0.333, sum=0.333 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.3333333333333333,
          "description": "min=0.333, mean=0.333, max=0.333, sum=0.333 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 619.8333333333334,
          "description": "min=619.833, mean=619.833, max=619.833, sum=619.833 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20nutrition&runSpecs=%5B%22mmlu%3Asubject%3Dnutrition%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=nutrition,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.7777777777777778,
          "description": "min=0.778, mean=0.778, max=0.778, sum=0.778 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.7777777777777778,
          "description": "min=0.778, mean=0.778, max=0.778, sum=0.778 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.7777777777777778,
          "description": "min=0.778, mean=0.778, max=0.778, sum=0.778 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 619.8333333333334,
          "description": "min=619.833, mean=619.833, max=619.833, sum=619.833 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20nutrition&runSpecs=%5B%22mmlu%3Asubject%3Dnutrition%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=nutrition,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.8333333333333334,
          "description": "min=0.833, mean=0.833, max=0.833, sum=0.833 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.8333333333333334,
          "description": "min=0.833, mean=0.833, max=0.833, sum=0.833 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.8333333333333334,
          "description": "min=0.833, mean=0.833, max=0.833, sum=0.833 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 575.8333333333334,
          "description": "min=575.833, mean=575.833, max=575.833, sum=575.833 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20nutrition&runSpecs=%5B%22mmlu%3Asubject%3Dnutrition%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Dnutrition%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Dnutrition%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Dnutrition%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Dnutrition%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:nutrition.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:nutrition.json"
      }
    ],
    "name": "mmlu_subject:nutrition"
  },
  {
    "title": "subject: philosophy",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20philosophy&runSpecs=%5B%22mmlu%3Asubject%3Dphilosophy%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=philosophy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.5,
          "description": "min=0.5, mean=0.5, max=0.5, sum=0.5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.5,
          "description": "min=0.5, mean=0.5, max=0.5, sum=0.5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.5,
          "description": "min=0.5, mean=0.5, max=0.5, sum=0.5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 364.8888888888889,
          "description": "min=364.889, mean=364.889, max=364.889, sum=364.889 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20philosophy&runSpecs=%5B%22mmlu%3Asubject%3Dphilosophy%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=philosophy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.6111111111111112,
          "description": "min=0.611, mean=0.611, max=0.611, sum=0.611 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.6111111111111112,
          "description": "min=0.611, mean=0.611, max=0.611, sum=0.611 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.6111111111111112,
          "description": "min=0.611, mean=0.611, max=0.611, sum=0.611 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 364.8888888888889,
          "description": "min=364.889, mean=364.889, max=364.889, sum=364.889 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20philosophy&runSpecs=%5B%22mmlu%3Asubject%3Dphilosophy%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=philosophy,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.4444444444444444,
          "description": "min=0.444, mean=0.444, max=0.444, sum=0.444 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.4444444444444444,
          "description": "min=0.444, mean=0.444, max=0.444, sum=0.444 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.4444444444444444,
          "description": "min=0.444, mean=0.444, max=0.444, sum=0.444 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 364.8888888888889,
          "description": "min=364.889, mean=364.889, max=364.889, sum=364.889 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0555555555555556,
          "description": "min=1.056, mean=1.056, max=1.056, sum=1.056 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20philosophy&runSpecs=%5B%22mmlu%3Asubject%3Dphilosophy%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=philosophy,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.6111111111111112,
          "description": "min=0.611, mean=0.611, max=0.611, sum=0.611 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.6111111111111112,
          "description": "min=0.611, mean=0.611, max=0.611, sum=0.611 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.6111111111111112,
          "description": "min=0.611, mean=0.611, max=0.611, sum=0.611 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 364.8888888888889,
          "description": "min=364.889, mean=364.889, max=364.889, sum=364.889 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20philosophy&runSpecs=%5B%22mmlu%3Asubject%3Dphilosophy%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=philosophy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.6666666666666666,
          "description": "min=0.667, mean=0.667, max=0.667, sum=0.667 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.6666666666666666,
          "description": "min=0.667, mean=0.667, max=0.667, sum=0.667 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.6666666666666666,
          "description": "min=0.667, mean=0.667, max=0.667, sum=0.667 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 320.8888888888889,
          "description": "min=320.889, mean=320.889, max=320.889, sum=320.889 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20philosophy&runSpecs=%5B%22mmlu%3Asubject%3Dphilosophy%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Dphilosophy%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Dphilosophy%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Dphilosophy%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Dphilosophy%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:philosophy.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:philosophy.json"
      }
    ],
    "name": "mmlu_subject:philosophy"
  },
  {
    "title": "subject: prehistory",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20prehistory&runSpecs=%5B%22mmlu%3Asubject%3Dprehistory%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=prehistory,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.6666666666666666,
          "description": "min=0.667, mean=0.667, max=0.667, sum=0.667 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.6666666666666666,
          "description": "min=0.667, mean=0.667, max=0.667, sum=0.667 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.6666666666666666,
          "description": "min=0.667, mean=0.667, max=0.667, sum=0.667 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 558.2222222222222,
          "description": "min=558.222, mean=558.222, max=558.222, sum=558.222 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20prehistory&runSpecs=%5B%22mmlu%3Asubject%3Dprehistory%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=prehistory,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.7777777777777778,
          "description": "min=0.778, mean=0.778, max=0.778, sum=0.778 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.7777777777777778,
          "description": "min=0.778, mean=0.778, max=0.778, sum=0.778 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.7777777777777778,
          "description": "min=0.778, mean=0.778, max=0.778, sum=0.778 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 558.2222222222222,
          "description": "min=558.222, mean=558.222, max=558.222, sum=558.222 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20prehistory&runSpecs=%5B%22mmlu%3Asubject%3Dprehistory%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=prehistory,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.3333333333333333,
          "description": "min=0.333, mean=0.333, max=0.333, sum=0.333 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.3333333333333333,
          "description": "min=0.333, mean=0.333, max=0.333, sum=0.333 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.3333333333333333,
          "description": "min=0.333, mean=0.333, max=0.333, sum=0.333 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 558.2222222222222,
          "description": "min=558.222, mean=558.222, max=558.222, sum=558.222 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20prehistory&runSpecs=%5B%22mmlu%3Asubject%3Dprehistory%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=prehistory,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.7777777777777778,
          "description": "min=0.778, mean=0.778, max=0.778, sum=0.778 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.7777777777777778,
          "description": "min=0.778, mean=0.778, max=0.778, sum=0.778 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.7777777777777778,
          "description": "min=0.778, mean=0.778, max=0.778, sum=0.778 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 558.2222222222222,
          "description": "min=558.222, mean=558.222, max=558.222, sum=558.222 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20prehistory&runSpecs=%5B%22mmlu%3Asubject%3Dprehistory%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=prehistory,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.6666666666666666,
          "description": "min=0.667, mean=0.667, max=0.667, sum=0.667 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.6666666666666666,
          "description": "min=0.667, mean=0.667, max=0.667, sum=0.667 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.6666666666666666,
          "description": "min=0.667, mean=0.667, max=0.667, sum=0.667 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 514.2222222222222,
          "description": "min=514.222, mean=514.222, max=514.222, sum=514.222 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20prehistory&runSpecs=%5B%22mmlu%3Asubject%3Dprehistory%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Dprehistory%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Dprehistory%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Dprehistory%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Dprehistory%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:prehistory.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:prehistory.json"
      }
    ],
    "name": "mmlu_subject:prehistory"
  },
  {
    "title": "subject: professional_accounting",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20professional_accounting&runSpecs=%5B%22mmlu%3Asubject%3Dprofessional_accounting%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.5555555555555556,
          "description": "min=0.556, mean=0.556, max=0.556, sum=0.556 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.5555555555555556,
          "description": "min=0.556, mean=0.556, max=0.556, sum=0.556 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.5555555555555556,
          "description": "min=0.556, mean=0.556, max=0.556, sum=0.556 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 692.9444444444445,
          "description": "min=692.944, mean=692.944, max=692.944, sum=692.944 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20professional_accounting&runSpecs=%5B%22mmlu%3Asubject%3Dprofessional_accounting%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.5,
          "description": "min=0.5, mean=0.5, max=0.5, sum=0.5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.5,
          "description": "min=0.5, mean=0.5, max=0.5, sum=0.5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.5,
          "description": "min=0.5, mean=0.5, max=0.5, sum=0.5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 692.9444444444445,
          "description": "min=692.944, mean=692.944, max=692.944, sum=692.944 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20professional_accounting&runSpecs=%5B%22mmlu%3Asubject%3Dprofessional_accounting%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.4444444444444444,
          "description": "min=0.444, mean=0.444, max=0.444, sum=0.444 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.4444444444444444,
          "description": "min=0.444, mean=0.444, max=0.444, sum=0.444 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.4444444444444444,
          "description": "min=0.444, mean=0.444, max=0.444, sum=0.444 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 692.9444444444445,
          "description": "min=692.944, mean=692.944, max=692.944, sum=692.944 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.3888888888888888,
          "description": "min=1.389, mean=1.389, max=1.389, sum=1.389 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20professional_accounting&runSpecs=%5B%22mmlu%3Asubject%3Dprofessional_accounting%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.7222222222222222,
          "description": "min=0.722, mean=0.722, max=0.722, sum=0.722 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.7222222222222222,
          "description": "min=0.722, mean=0.722, max=0.722, sum=0.722 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.7222222222222222,
          "description": "min=0.722, mean=0.722, max=0.722, sum=0.722 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 692.9444444444445,
          "description": "min=692.944, mean=692.944, max=692.944, sum=692.944 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20professional_accounting&runSpecs=%5B%22mmlu%3Asubject%3Dprofessional_accounting%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=professional_accounting,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.4444444444444444,
          "description": "min=0.444, mean=0.444, max=0.444, sum=0.444 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.4444444444444444,
          "description": "min=0.444, mean=0.444, max=0.444, sum=0.444 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.4444444444444444,
          "description": "min=0.444, mean=0.444, max=0.444, sum=0.444 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 648.9444444444445,
          "description": "min=648.944, mean=648.944, max=648.944, sum=648.944 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20professional_accounting&runSpecs=%5B%22mmlu%3Asubject%3Dprofessional_accounting%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Dprofessional_accounting%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Dprofessional_accounting%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Dprofessional_accounting%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Dprofessional_accounting%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:professional_accounting.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:professional_accounting.json"
      }
    ],
    "name": "mmlu_subject:professional_accounting"
  },
  {
    "title": "subject: professional_law",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20professional_law&runSpecs=%5B%22mmlu%3Asubject%3Dprofessional_law%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=professional_law,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.3333333333333333,
          "description": "min=0.333, mean=0.333, max=0.333, sum=0.333 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.3333333333333333,
          "description": "min=0.333, mean=0.333, max=0.333, sum=0.333 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.3333333333333333,
          "description": "min=0.333, mean=0.333, max=0.333, sum=0.333 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1683.5,
          "description": "min=1683.5, mean=1683.5, max=1683.5, sum=1683.5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20professional_law&runSpecs=%5B%22mmlu%3Asubject%3Dprofessional_law%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=professional_law,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.3888888888888889,
          "description": "min=0.389, mean=0.389, max=0.389, sum=0.389 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.3888888888888889,
          "description": "min=0.389, mean=0.389, max=0.389, sum=0.389 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.3888888888888889,
          "description": "min=0.389, mean=0.389, max=0.389, sum=0.389 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1683.5,
          "description": "min=1683.5, mean=1683.5, max=1683.5, sum=1683.5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20professional_law&runSpecs=%5B%22mmlu%3Asubject%3Dprofessional_law%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=professional_law,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.16666666666666666,
          "description": "min=0.167, mean=0.167, max=0.167, sum=0.167 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.16666666666666666,
          "description": "min=0.167, mean=0.167, max=0.167, sum=0.167 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.16666666666666666,
          "description": "min=0.167, mean=0.167, max=0.167, sum=0.167 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1683.5,
          "description": "min=1683.5, mean=1683.5, max=1683.5, sum=1683.5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20professional_law&runSpecs=%5B%22mmlu%3Asubject%3Dprofessional_law%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=professional_law,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.3888888888888889,
          "description": "min=0.389, mean=0.389, max=0.389, sum=0.389 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.3888888888888889,
          "description": "min=0.389, mean=0.389, max=0.389, sum=0.389 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.3888888888888889,
          "description": "min=0.389, mean=0.389, max=0.389, sum=0.389 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1683.5,
          "description": "min=1683.5, mean=1683.5, max=1683.5, sum=1683.5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20professional_law&runSpecs=%5B%22mmlu%3Asubject%3Dprofessional_law%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=professional_law,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.2777777777777778,
          "description": "min=0.278, mean=0.278, max=0.278, sum=0.278 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.2777777777777778,
          "description": "min=0.278, mean=0.278, max=0.278, sum=0.278 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.2777777777777778,
          "description": "min=0.278, mean=0.278, max=0.278, sum=0.278 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1639.5,
          "description": "min=1639.5, mean=1639.5, max=1639.5, sum=1639.5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20professional_law&runSpecs=%5B%22mmlu%3Asubject%3Dprofessional_law%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Dprofessional_law%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Dprofessional_law%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Dprofessional_law%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Dprofessional_law%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:professional_law.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:professional_law.json"
      }
    ],
    "name": "mmlu_subject:professional_law"
  },
  {
    "title": "subject: professional_medicine",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20professional_medicine&runSpecs=%5B%22mmlu%3Asubject%3Dprofessional_medicine%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.4444444444444444,
          "description": "min=0.444, mean=0.444, max=0.444, sum=0.444 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.4444444444444444,
          "description": "min=0.444, mean=0.444, max=0.444, sum=0.444 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.4444444444444444,
          "description": "min=0.444, mean=0.444, max=0.444, sum=0.444 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1151.8333333333333,
          "description": "min=1151.833, mean=1151.833, max=1151.833, sum=1151.833 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20professional_medicine&runSpecs=%5B%22mmlu%3Asubject%3Dprofessional_medicine%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.8888888888888888,
          "description": "min=0.889, mean=0.889, max=0.889, sum=0.889 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.8888888888888888,
          "description": "min=0.889, mean=0.889, max=0.889, sum=0.889 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.8888888888888888,
          "description": "min=0.889, mean=0.889, max=0.889, sum=0.889 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1151.8333333333333,
          "description": "min=1151.833, mean=1151.833, max=1151.833, sum=1151.833 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20professional_medicine&runSpecs=%5B%22mmlu%3Asubject%3Dprofessional_medicine%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.16666666666666666,
          "description": "min=0.167, mean=0.167, max=0.167, sum=0.167 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.16666666666666666,
          "description": "min=0.167, mean=0.167, max=0.167, sum=0.167 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.16666666666666666,
          "description": "min=0.167, mean=0.167, max=0.167, sum=0.167 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1151.8333333333333,
          "description": "min=1151.833, mean=1151.833, max=1151.833, sum=1151.833 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 8.38888888888889,
          "description": "min=8.389, mean=8.389, max=8.389, sum=8.389 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20professional_medicine&runSpecs=%5B%22mmlu%3Asubject%3Dprofessional_medicine%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.8888888888888888,
          "description": "min=0.889, mean=0.889, max=0.889, sum=0.889 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.8888888888888888,
          "description": "min=0.889, mean=0.889, max=0.889, sum=0.889 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.8888888888888888,
          "description": "min=0.889, mean=0.889, max=0.889, sum=0.889 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1151.8333333333333,
          "description": "min=1151.833, mean=1151.833, max=1151.833, sum=1151.833 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20professional_medicine&runSpecs=%5B%22mmlu%3Asubject%3Dprofessional_medicine%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=professional_medicine,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.6666666666666666,
          "description": "min=0.667, mean=0.667, max=0.667, sum=0.667 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.6666666666666666,
          "description": "min=0.667, mean=0.667, max=0.667, sum=0.667 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.6666666666666666,
          "description": "min=0.667, mean=0.667, max=0.667, sum=0.667 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1107.8333333333333,
          "description": "min=1107.833, mean=1107.833, max=1107.833, sum=1107.833 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20professional_medicine&runSpecs=%5B%22mmlu%3Asubject%3Dprofessional_medicine%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Dprofessional_medicine%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Dprofessional_medicine%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Dprofessional_medicine%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Dprofessional_medicine%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:professional_medicine.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:professional_medicine.json"
      }
    ],
    "name": "mmlu_subject:professional_medicine"
  },
  {
    "title": "subject: professional_psychology",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20professional_psychology&runSpecs=%5B%22mmlu%3Asubject%3Dprofessional_psychology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.3333333333333333,
          "description": "min=0.333, mean=0.333, max=0.333, sum=0.333 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.3333333333333333,
          "description": "min=0.333, mean=0.333, max=0.333, sum=0.333 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.3333333333333333,
          "description": "min=0.333, mean=0.333, max=0.333, sum=0.333 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 606.0555555555555,
          "description": "min=606.056, mean=606.056, max=606.056, sum=606.056 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20professional_psychology&runSpecs=%5B%22mmlu%3Asubject%3Dprofessional_psychology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.7222222222222222,
          "description": "min=0.722, mean=0.722, max=0.722, sum=0.722 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.7222222222222222,
          "description": "min=0.722, mean=0.722, max=0.722, sum=0.722 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.7222222222222222,
          "description": "min=0.722, mean=0.722, max=0.722, sum=0.722 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 606.0555555555555,
          "description": "min=606.056, mean=606.056, max=606.056, sum=606.056 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20professional_psychology&runSpecs=%5B%22mmlu%3Asubject%3Dprofessional_psychology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.05555555555555555,
          "description": "min=0.056, mean=0.056, max=0.056, sum=0.056 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.05555555555555555,
          "description": "min=0.056, mean=0.056, max=0.056, sum=0.056 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.05555555555555555,
          "description": "min=0.056, mean=0.056, max=0.056, sum=0.056 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 606.0555555555555,
          "description": "min=606.056, mean=606.056, max=606.056, sum=606.056 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20professional_psychology&runSpecs=%5B%22mmlu%3Asubject%3Dprofessional_psychology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.7222222222222222,
          "description": "min=0.722, mean=0.722, max=0.722, sum=0.722 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.7222222222222222,
          "description": "min=0.722, mean=0.722, max=0.722, sum=0.722 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.7222222222222222,
          "description": "min=0.722, mean=0.722, max=0.722, sum=0.722 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 606.0555555555555,
          "description": "min=606.056, mean=606.056, max=606.056, sum=606.056 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20professional_psychology&runSpecs=%5B%22mmlu%3Asubject%3Dprofessional_psychology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=professional_psychology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.6666666666666666,
          "description": "min=0.667, mean=0.667, max=0.667, sum=0.667 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.6666666666666666,
          "description": "min=0.667, mean=0.667, max=0.667, sum=0.667 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.6666666666666666,
          "description": "min=0.667, mean=0.667, max=0.667, sum=0.667 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 562.0555555555555,
          "description": "min=562.056, mean=562.056, max=562.056, sum=562.056 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20professional_psychology&runSpecs=%5B%22mmlu%3Asubject%3Dprofessional_psychology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Dprofessional_psychology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Dprofessional_psychology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Dprofessional_psychology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Dprofessional_psychology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:professional_psychology.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:professional_psychology.json"
      }
    ],
    "name": "mmlu_subject:professional_psychology"
  },
  {
    "title": "subject: public_relations",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20public_relations&runSpecs=%5B%22mmlu%3Asubject%3Dpublic_relations%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=public_relations,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.6875,
          "description": "min=0.688, mean=0.688, max=0.688, sum=0.688 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.6875,
          "description": "min=0.688, mean=0.688, max=0.688, sum=0.688 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.6875,
          "description": "min=0.688, mean=0.688, max=0.688, sum=0.688 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 16.0,
          "description": "min=16, mean=16, max=16, sum=16 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 442.6875,
          "description": "min=442.688, mean=442.688, max=442.688, sum=442.688 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20public_relations&runSpecs=%5B%22mmlu%3Asubject%3Dpublic_relations%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=public_relations,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.75,
          "description": "min=0.75, mean=0.75, max=0.75, sum=0.75 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.75,
          "description": "min=0.75, mean=0.75, max=0.75, sum=0.75 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.75,
          "description": "min=0.75, mean=0.75, max=0.75, sum=0.75 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 16.0,
          "description": "min=16, mean=16, max=16, sum=16 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 442.6875,
          "description": "min=442.688, mean=442.688, max=442.688, sum=442.688 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20public_relations&runSpecs=%5B%22mmlu%3Asubject%3Dpublic_relations%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=public_relations,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.5,
          "description": "min=0.5, mean=0.5, max=0.5, sum=0.5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.5,
          "description": "min=0.5, mean=0.5, max=0.5, sum=0.5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.5,
          "description": "min=0.5, mean=0.5, max=0.5, sum=0.5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 16.0,
          "description": "min=16, mean=16, max=16, sum=16 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 442.6875,
          "description": "min=442.688, mean=442.688, max=442.688, sum=442.688 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20public_relations&runSpecs=%5B%22mmlu%3Asubject%3Dpublic_relations%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=public_relations,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.6875,
          "description": "min=0.688, mean=0.688, max=0.688, sum=0.688 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.6875,
          "description": "min=0.688, mean=0.688, max=0.688, sum=0.688 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.6875,
          "description": "min=0.688, mean=0.688, max=0.688, sum=0.688 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 16.0,
          "description": "min=16, mean=16, max=16, sum=16 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 442.6875,
          "description": "min=442.688, mean=442.688, max=442.688, sum=442.688 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20public_relations&runSpecs=%5B%22mmlu%3Asubject%3Dpublic_relations%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=public_relations,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.625,
          "description": "min=0.625, mean=0.625, max=0.625, sum=0.625 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.625,
          "description": "min=0.625, mean=0.625, max=0.625, sum=0.625 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.625,
          "description": "min=0.625, mean=0.625, max=0.625, sum=0.625 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 16.0,
          "description": "min=16, mean=16, max=16, sum=16 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 398.6875,
          "description": "min=398.688, mean=398.688, max=398.688, sum=398.688 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20public_relations&runSpecs=%5B%22mmlu%3Asubject%3Dpublic_relations%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Dpublic_relations%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Dpublic_relations%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Dpublic_relations%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Dpublic_relations%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:public_relations.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:public_relations.json"
      }
    ],
    "name": "mmlu_subject:public_relations"
  },
  {
    "title": "subject: security_studies",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20security_studies&runSpecs=%5B%22mmlu%3Asubject%3Dsecurity_studies%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=security_studies,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.6111111111111112,
          "description": "min=0.611, mean=0.611, max=0.611, sum=0.611 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.6111111111111112,
          "description": "min=0.611, mean=0.611, max=0.611, sum=0.611 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.6111111111111112,
          "description": "min=0.611, mean=0.611, max=0.611, sum=0.611 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1214.388888888889,
          "description": "min=1214.389, mean=1214.389, max=1214.389, sum=1214.389 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20security_studies&runSpecs=%5B%22mmlu%3Asubject%3Dsecurity_studies%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=security_studies,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.8333333333333334,
          "description": "min=0.833, mean=0.833, max=0.833, sum=0.833 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.8333333333333334,
          "description": "min=0.833, mean=0.833, max=0.833, sum=0.833 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.8333333333333334,
          "description": "min=0.833, mean=0.833, max=0.833, sum=0.833 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1214.388888888889,
          "description": "min=1214.389, mean=1214.389, max=1214.389, sum=1214.389 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20security_studies&runSpecs=%5B%22mmlu%3Asubject%3Dsecurity_studies%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=security_studies,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.5,
          "description": "min=0.5, mean=0.5, max=0.5, sum=0.5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.5,
          "description": "min=0.5, mean=0.5, max=0.5, sum=0.5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.5,
          "description": "min=0.5, mean=0.5, max=0.5, sum=0.5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1214.388888888889,
          "description": "min=1214.389, mean=1214.389, max=1214.389, sum=1214.389 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20security_studies&runSpecs=%5B%22mmlu%3Asubject%3Dsecurity_studies%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=security_studies,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.8888888888888888,
          "description": "min=0.889, mean=0.889, max=0.889, sum=0.889 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.8888888888888888,
          "description": "min=0.889, mean=0.889, max=0.889, sum=0.889 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.8888888888888888,
          "description": "min=0.889, mean=0.889, max=0.889, sum=0.889 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1214.388888888889,
          "description": "min=1214.389, mean=1214.389, max=1214.389, sum=1214.389 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20security_studies&runSpecs=%5B%22mmlu%3Asubject%3Dsecurity_studies%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=security_studies,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.8333333333333334,
          "description": "min=0.833, mean=0.833, max=0.833, sum=0.833 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.8333333333333334,
          "description": "min=0.833, mean=0.833, max=0.833, sum=0.833 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.8333333333333334,
          "description": "min=0.833, mean=0.833, max=0.833, sum=0.833 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1170.388888888889,
          "description": "min=1170.389, mean=1170.389, max=1170.389, sum=1170.389 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20security_studies&runSpecs=%5B%22mmlu%3Asubject%3Dsecurity_studies%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Dsecurity_studies%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Dsecurity_studies%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Dsecurity_studies%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Dsecurity_studies%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:security_studies.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:security_studies.json"
      }
    ],
    "name": "mmlu_subject:security_studies"
  },
  {
    "title": "subject: sociology",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20sociology&runSpecs=%5B%22mmlu%3Asubject%3Dsociology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=sociology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.8333333333333334,
          "description": "min=0.833, mean=0.833, max=0.833, sum=0.833 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.8333333333333334,
          "description": "min=0.833, mean=0.833, max=0.833, sum=0.833 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.8333333333333334,
          "description": "min=0.833, mean=0.833, max=0.833, sum=0.833 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 482.6111111111111,
          "description": "min=482.611, mean=482.611, max=482.611, sum=482.611 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20sociology&runSpecs=%5B%22mmlu%3Asubject%3Dsociology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=sociology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.8333333333333334,
          "description": "min=0.833, mean=0.833, max=0.833, sum=0.833 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.8333333333333334,
          "description": "min=0.833, mean=0.833, max=0.833, sum=0.833 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.8333333333333334,
          "description": "min=0.833, mean=0.833, max=0.833, sum=0.833 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 482.6111111111111,
          "description": "min=482.611, mean=482.611, max=482.611, sum=482.611 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20sociology&runSpecs=%5B%22mmlu%3Asubject%3Dsociology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=sociology,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.6111111111111112,
          "description": "min=0.611, mean=0.611, max=0.611, sum=0.611 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.6111111111111112,
          "description": "min=0.611, mean=0.611, max=0.611, sum=0.611 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.6111111111111112,
          "description": "min=0.611, mean=0.611, max=0.611, sum=0.611 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 482.6111111111111,
          "description": "min=482.611, mean=482.611, max=482.611, sum=482.611 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.2222222222222223,
          "description": "min=1.222, mean=1.222, max=1.222, sum=1.222 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20sociology&runSpecs=%5B%22mmlu%3Asubject%3Dsociology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=sociology,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.8333333333333334,
          "description": "min=0.833, mean=0.833, max=0.833, sum=0.833 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.8333333333333334,
          "description": "min=0.833, mean=0.833, max=0.833, sum=0.833 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.8333333333333334,
          "description": "min=0.833, mean=0.833, max=0.833, sum=0.833 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 482.6111111111111,
          "description": "min=482.611, mean=482.611, max=482.611, sum=482.611 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20sociology&runSpecs=%5B%22mmlu%3Asubject%3Dsociology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=sociology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.8333333333333334,
          "description": "min=0.833, mean=0.833, max=0.833, sum=0.833 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.8333333333333334,
          "description": "min=0.833, mean=0.833, max=0.833, sum=0.833 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.8333333333333334,
          "description": "min=0.833, mean=0.833, max=0.833, sum=0.833 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 438.6111111111111,
          "description": "min=438.611, mean=438.611, max=438.611, sum=438.611 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20sociology&runSpecs=%5B%22mmlu%3Asubject%3Dsociology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Dsociology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Dsociology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Dsociology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Dsociology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:sociology.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:sociology.json"
      }
    ],
    "name": "mmlu_subject:sociology"
  },
  {
    "title": "subject: us_foreign_policy",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20us_foreign_policy&runSpecs=%5B%22mmlu%3Asubject%3Dus_foreign_policy%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.9411764705882353,
          "description": "min=0.941, mean=0.941, max=0.941, sum=0.941 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.9411764705882353,
          "description": "min=0.941, mean=0.941, max=0.941, sum=0.941 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.9411764705882353,
          "description": "min=0.941, mean=0.941, max=0.941, sum=0.941 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 457.11764705882354,
          "description": "min=457.118, mean=457.118, max=457.118, sum=457.118 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20us_foreign_policy&runSpecs=%5B%22mmlu%3Asubject%3Dus_foreign_policy%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.8823529411764706,
          "description": "min=0.882, mean=0.882, max=0.882, sum=0.882 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.8823529411764706,
          "description": "min=0.882, mean=0.882, max=0.882, sum=0.882 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.8823529411764706,
          "description": "min=0.882, mean=0.882, max=0.882, sum=0.882 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 457.11764705882354,
          "description": "min=457.118, mean=457.118, max=457.118, sum=457.118 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20us_foreign_policy&runSpecs=%5B%22mmlu%3Asubject%3Dus_foreign_policy%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.5882352941176471,
          "description": "min=0.588, mean=0.588, max=0.588, sum=0.588 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.5882352941176471,
          "description": "min=0.588, mean=0.588, max=0.588, sum=0.588 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.5882352941176471,
          "description": "min=0.588, mean=0.588, max=0.588, sum=0.588 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 457.11764705882354,
          "description": "min=457.118, mean=457.118, max=457.118, sum=457.118 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20us_foreign_policy&runSpecs=%5B%22mmlu%3Asubject%3Dus_foreign_policy%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.8823529411764706,
          "description": "min=0.882, mean=0.882, max=0.882, sum=0.882 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.8823529411764706,
          "description": "min=0.882, mean=0.882, max=0.882, sum=0.882 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.8823529411764706,
          "description": "min=0.882, mean=0.882, max=0.882, sum=0.882 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 457.11764705882354,
          "description": "min=457.118, mean=457.118, max=457.118, sum=457.118 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20us_foreign_policy&runSpecs=%5B%22mmlu%3Asubject%3Dus_foreign_policy%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.8235294117647058,
          "description": "min=0.824, mean=0.824, max=0.824, sum=0.824 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.8235294117647058,
          "description": "min=0.824, mean=0.824, max=0.824, sum=0.824 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.8235294117647058,
          "description": "min=0.824, mean=0.824, max=0.824, sum=0.824 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 413.11764705882354,
          "description": "min=413.118, mean=413.118, max=413.118, sum=413.118 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20us_foreign_policy&runSpecs=%5B%22mmlu%3Asubject%3Dus_foreign_policy%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Dus_foreign_policy%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Dus_foreign_policy%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Dus_foreign_policy%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Dus_foreign_policy%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:us_foreign_policy.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:us_foreign_policy.json"
      }
    ],
    "name": "mmlu_subject:us_foreign_policy"
  },
  {
    "title": "subject: virology",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20virology&runSpecs=%5B%22mmlu%3Asubject%3Dvirology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=virology,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.3888888888888889,
          "description": "min=0.389, mean=0.389, max=0.389, sum=0.389 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.3888888888888889,
          "description": "min=0.389, mean=0.389, max=0.389, sum=0.389 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.3888888888888889,
          "description": "min=0.389, mean=0.389, max=0.389, sum=0.389 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 378.05555555555554,
          "description": "min=378.056, mean=378.056, max=378.056, sum=378.056 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20virology&runSpecs=%5B%22mmlu%3Asubject%3Dvirology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=virology,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.5555555555555556,
          "description": "min=0.556, mean=0.556, max=0.556, sum=0.556 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.5555555555555556,
          "description": "min=0.556, mean=0.556, max=0.556, sum=0.556 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.5555555555555556,
          "description": "min=0.556, mean=0.556, max=0.556, sum=0.556 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 378.05555555555554,
          "description": "min=378.056, mean=378.056, max=378.056, sum=378.056 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20virology&runSpecs=%5B%22mmlu%3Asubject%3Dvirology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=virology,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.4444444444444444,
          "description": "min=0.444, mean=0.444, max=0.444, sum=0.444 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.4444444444444444,
          "description": "min=0.444, mean=0.444, max=0.444, sum=0.444 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.4444444444444444,
          "description": "min=0.444, mean=0.444, max=0.444, sum=0.444 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 378.05555555555554,
          "description": "min=378.056, mean=378.056, max=378.056, sum=378.056 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.4444444444444444,
          "description": "min=1.444, mean=1.444, max=1.444, sum=1.444 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20virology&runSpecs=%5B%22mmlu%3Asubject%3Dvirology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=virology,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.5,
          "description": "min=0.5, mean=0.5, max=0.5, sum=0.5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.5,
          "description": "min=0.5, mean=0.5, max=0.5, sum=0.5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.5,
          "description": "min=0.5, mean=0.5, max=0.5, sum=0.5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 378.05555555555554,
          "description": "min=378.056, mean=378.056, max=378.056, sum=378.056 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20virology&runSpecs=%5B%22mmlu%3Asubject%3Dvirology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=virology,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.5555555555555556,
          "description": "min=0.556, mean=0.556, max=0.556, sum=0.556 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.5555555555555556,
          "description": "min=0.556, mean=0.556, max=0.556, sum=0.556 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.5555555555555556,
          "description": "min=0.556, mean=0.556, max=0.556, sum=0.556 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 18.0,
          "description": "min=18, mean=18, max=18, sum=18 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 334.05555555555554,
          "description": "min=334.056, mean=334.056, max=334.056, sum=334.056 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20virology&runSpecs=%5B%22mmlu%3Asubject%3Dvirology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Dvirology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Dvirology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Dvirology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Dvirology%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:virology.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:virology.json"
      }
    ],
    "name": "mmlu_subject:virology"
  },
  {
    "title": "subject: world_religions",
    "header": [
      {
        "value": "Model/adapter",
        "markdown": false,
        "metadata": {}
      },
      {
        "value": "EM",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU"
        }
      },
      {
        "value": "ECE (10-bin)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n10-bin expected calibration error: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "ECE (10-bin)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "EM (Robustness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Robustness: Computes worst case over different robustness perturbations (misspellings, formatting, contrast sets).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Robustness"
        }
      },
      {
        "value": "EM (Fairness)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nExact match: Fraction of instances that the predicted output matches a correct reference exactly.\n- Perturbation Fairness: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).",
        "markdown": false,
        "lower_is_better": false,
        "metadata": {
          "metric": "EM",
          "run_group": "MMLU",
          "perturbation": "Fairness"
        }
      },
      {
        "value": "Denoised inference time (s)",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\nDenoised inference runtime (s): Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.",
        "markdown": false,
        "lower_is_better": true,
        "metadata": {
          "metric": "Denoised inference time (s)",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# eval",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# eval: Number of evaluation instances.",
        "markdown": false,
        "metadata": {
          "metric": "# eval",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# train",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# train: Number of training instances (e.g., in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "# train",
          "run_group": "MMLU"
        }
      },
      {
        "value": "truncated",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\ntruncated: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).",
        "markdown": false,
        "metadata": {
          "metric": "truncated",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# prompt tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# prompt tokens: Number of tokens in the prompt.",
        "markdown": false,
        "metadata": {
          "metric": "# prompt tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# output tokens",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# output tokens: Actual number of output tokens.",
        "markdown": false,
        "metadata": {
          "metric": "# output tokens",
          "run_group": "MMLU"
        }
      },
      {
        "value": "# trials",
        "description": "The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).\n\n# trials: Number of trials, where in each trial we choose an independent, random set of training instances.",
        "markdown": false,
        "metadata": {
          "metric": "# trials",
          "run_group": "MMLU"
        }
      }
    ],
    "rows": [
      [
        {
          "value": "Together AI Llama 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20world_religions&runSpecs=%5B%22mmlu%3Asubject%3Dworld_religions%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=world_religions,method=multiple_choice_joint,model=together_Llama-3-8b-chat-hf"
          ]
        },
        {
          "value": 0.7647058823529411,
          "description": "min=0.765, mean=0.765, max=0.765, sum=0.765 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.7647058823529411,
          "description": "min=0.765, mean=0.765, max=0.765, sum=0.765 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.7647058823529411,
          "description": "min=0.765, mean=0.765, max=0.765, sum=0.765 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 309.05882352941177,
          "description": "min=309.059, mean=309.059, max=309.059, sum=309.059 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Together AI Llama 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20world_religions&runSpecs=%5B%22mmlu%3Asubject%3Dworld_religions%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=world_religions,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf"
          ]
        },
        {
          "value": 0.7058823529411765,
          "description": "min=0.706, mean=0.706, max=0.706, sum=0.706 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.7058823529411765,
          "description": "min=0.706, mean=0.706, max=0.706, sum=0.706 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.7058823529411765,
          "description": "min=0.706, mean=0.706, max=0.706, sum=0.706 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 309.05882352941177,
          "description": "min=309.059, mean=309.059, max=309.059, sum=309.059 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (8B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20world_religions&runSpecs=%5B%22mmlu%3Asubject%3Dworld_religions%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=world_religions,method=multiple_choice_joint,model=groq_llama3-8b-8192"
          ]
        },
        {
          "value": 0.6470588235294118,
          "description": "min=0.647, mean=0.647, max=0.647, sum=0.647 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.6470588235294118,
          "description": "min=0.647, mean=0.647, max=0.647, sum=0.647 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.6470588235294118,
          "description": "min=0.647, mean=0.647, max=0.647, sum=0.647 (1)",
          "style": {},
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 309.05882352941177,
          "description": "min=309.059, mean=309.059, max=309.059, sum=309.059 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.2352941176470589,
          "description": "min=1.235, mean=1.235, max=1.235, sum=1.235 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Groq LLaMA 3 (70B)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20world_religions&runSpecs=%5B%22mmlu%3Asubject%3Dworld_religions%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=world_religions,method=multiple_choice_joint,model=groq_llama3-70b-8192"
          ]
        },
        {
          "value": 0.7647058823529411,
          "description": "min=0.765, mean=0.765, max=0.765, sum=0.765 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.7647058823529411,
          "description": "min=0.765, mean=0.765, max=0.765, sum=0.765 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.7647058823529411,
          "description": "min=0.765, mean=0.765, max=0.765, sum=0.765 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 309.05882352941177,
          "description": "min=309.059, mean=309.059, max=309.059, sum=309.059 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ],
      [
        {
          "value": "Hugging face LLaMA 3 (8B)(original)",
          "description": "",
          "href": "?group=mmlu&subgroup=subject%3A%20world_religions&runSpecs=%5B%22mmlu%3Asubject%3Dworld_religions%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%5D",
          "markdown": false,
          "run_spec_names": [
            "mmlu:subject=world_religions,method=multiple_choice_joint,model=huggingface_Meta-Llama-3-8B"
          ]
        },
        {
          "value": 0.7647058823529411,
          "description": "min=0.765, mean=0.765, max=0.765, sum=0.765 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 0.7647058823529411,
          "description": "min=0.765, mean=0.765, max=0.765, sum=0.765 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "value": 0.7647058823529411,
          "description": "min=0.765, mean=0.765, max=0.765, sum=0.765 (1)",
          "style": {
            "font-weight": "bold"
          },
          "markdown": false
        },
        {
          "description": "1 matching runs, but no matching metrics",
          "markdown": false
        },
        {
          "value": 17.0,
          "description": "min=17, mean=17, max=17, sum=17 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 5.0,
          "description": "min=5, mean=5, max=5, sum=5 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 0.0,
          "description": "min=0, mean=0, max=0, sum=0 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 265.05882352941177,
          "description": "min=265.059, mean=265.059, max=265.059, sum=265.059 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        },
        {
          "value": 1.0,
          "description": "min=1, mean=1, max=1, sum=1 (1)",
          "style": {},
          "markdown": false
        }
      ]
    ],
    "links": [
      {
        "text": "compare all",
        "href": "?group=mmlu&subgroup=subject%3A%20world_religions&runSpecs=%5B%22mmlu%3Asubject%3Dworld_religions%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-70b-8192%22%2C%20%22mmlu%3Asubject%3Dworld_religions%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dgroq_llama3-8b-8192%22%2C%20%22mmlu%3Asubject%3Dworld_religions%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dhuggingface_Meta-Llama-3-8B%22%2C%20%22mmlu%3Asubject%3Dworld_religions%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-70b-chat-hf%22%2C%20%22mmlu%3Asubject%3Dworld_religions%2Cmethod%3Dmultiple_choice_joint%2Cmodel%3Dtogether_Llama-3-8b-chat-hf%22%5D"
      },
      {
        "text": "LaTeX",
        "href": "benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:world_religions.tex"
      },
      {
        "text": "JSON",
        "href": "benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:world_religions.json"
      }
    ],
    "name": "mmlu_subject:world_religions"
  }
]