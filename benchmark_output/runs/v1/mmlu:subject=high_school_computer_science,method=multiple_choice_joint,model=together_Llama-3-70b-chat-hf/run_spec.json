{
  "name": "mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=together_Llama-3-70b-chat-hf",
  "scenario_spec": {
    "class_name": "helm.benchmark.scenarios.mmlu_scenario.MMLUScenario",
    "args": {
      "subject": "high_school_computer_science"
    }
  },
  "adapter_spec": {
    "method": "multiple_choice_joint",
    "global_prefix": "",
    "global_suffix": "",
    "instructions": "The following are multiple choice questions (with answers) about high school computer science.\n",
    "input_prefix": "Question: ",
    "input_suffix": "\n",
    "reference_prefix": "A. ",
    "reference_suffix": "\n",
    "output_prefix": "Answer: ",
    "output_suffix": "\n",
    "instance_prefix": "\n",
    "substitutions": [],
    "max_train_instances": 5,
    "max_eval_instances": 20,
    "num_outputs": 5,
    "num_train_trials": 1,
    "num_trials": 1,
    "sample_train": true,
    "model_deployment": "together/Llama-3-70b-chat-hf",
    "model": "together/Llama-3-70b-chat-hf",
    "temperature": 0.0,
    "max_tokens": 20,
    "stop_sequences": [
      "\n"
    ],
    "multi_label": false
  },
  "metric_specs": [
    {
      "class_name": "helm.benchmark.metrics.basic_metrics.BasicGenerationMetric",
      "args": {
        "names": [
          "exact_match",
          "quasi_exact_match",
          "prefix_exact_match",
          "quasi_prefix_exact_match"
        ]
      }
    },
    {
      "class_name": "helm.benchmark.metrics.basic_metrics.BasicReferenceMetric",
      "args": {}
    },
    {
      "class_name": "helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric",
      "args": {}
    }
  ],
  "data_augmenter_spec": {
    "perturbation_specs": [],
    "should_augment_train_instances": false,
    "should_include_original_train": false,
    "should_skip_unchanged_train": false,
    "should_augment_eval_instances": false,
    "should_include_original_eval": false,
    "should_skip_unchanged_eval": false,
    "seeds_per_instance": 1
  },
  "groups": [
    "mmlu"
  ]
}