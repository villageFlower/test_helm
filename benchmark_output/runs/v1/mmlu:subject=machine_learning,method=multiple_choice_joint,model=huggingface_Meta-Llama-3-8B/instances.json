[
  {
    "input": {
      "text": "Existential risks posed by AI are most commonly associated with which of the following professors?"
    },
    "references": [
      {
        "output": {
          "text": "Nando de Frietas"
        },
        "tags": []
      },
      {
        "output": {
          "text": "Yann LeCun"
        },
        "tags": []
      },
      {
        "output": {
          "text": "Stuart Russell"
        },
        "tags": [
          "correct"
        ]
      },
      {
        "output": {
          "text": "Jitendra Malik"
        },
        "tags": []
      }
    ],
    "split": "test",
    "id": "id90"
  },
  {
    "input": {
      "text": "Statement 1| CIFAR-10 classification performance for convolution neural networks can exceed 95%. Statement 2| Ensembles of neural networks do not improve classification accuracy since the representations they learn are highly correlated."
    },
    "references": [
      {
        "output": {
          "text": "True, True"
        },
        "tags": []
      },
      {
        "output": {
          "text": "False, False"
        },
        "tags": []
      },
      {
        "output": {
          "text": "True, False"
        },
        "tags": [
          "correct"
        ]
      },
      {
        "output": {
          "text": "False, True"
        },
        "tags": []
      }
    ],
    "split": "test",
    "id": "id103"
  },
  {
    "input": {
      "text": "Statement 1| If there exists a set of k instances that cannot be shattered by H, then VC(H) < k. Statement 2| If two hypothesis classes H1 and H2 satisfy H1 \u2286 H2, then VC(H1) \u2264 VC(H2)."
    },
    "references": [
      {
        "output": {
          "text": "True, True"
        },
        "tags": []
      },
      {
        "output": {
          "text": "False, False"
        },
        "tags": []
      },
      {
        "output": {
          "text": "True, False"
        },
        "tags": []
      },
      {
        "output": {
          "text": "False, True"
        },
        "tags": [
          "correct"
        ]
      }
    ],
    "split": "valid",
    "id": "id15"
  },
  {
    "input": {
      "text": "Statement 1| PCA and Spectral Clustering (such as Andrew Ng\u2019s) perform eigendecomposition on two different matrices. However, the size of these two matrices are the same. Statement 2| Since classification is a special case of regression, logistic regression is a special case of linear regression."
    },
    "references": [
      {
        "output": {
          "text": "True, True"
        },
        "tags": []
      },
      {
        "output": {
          "text": "False, False"
        },
        "tags": [
          "correct"
        ]
      },
      {
        "output": {
          "text": "True, False"
        },
        "tags": []
      },
      {
        "output": {
          "text": "False, True"
        },
        "tags": []
      }
    ],
    "split": "test",
    "id": "id76"
  },
  {
    "input": {
      "text": "Which of the following best describes the joint probability distribution P(X, Y, Z) for the given Bayes net. X <- Y -> Z?"
    },
    "references": [
      {
        "output": {
          "text": "P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y)"
        },
        "tags": [
          "correct"
        ]
      },
      {
        "output": {
          "text": "P(X, Y, Z) = P(X) * P(Y|X) * P(Z|Y)"
        },
        "tags": []
      },
      {
        "output": {
          "text": "P(X, Y, Z) = P(Z) * P(X|Z) * P(Y|Z)"
        },
        "tags": []
      },
      {
        "output": {
          "text": "P(X, Y, Z) = P(X) * P(Y) * P(Z)"
        },
        "tags": []
      }
    ],
    "split": "valid",
    "id": "id13"
  },
  {
    "input": {
      "text": "Predicting the amount of rainfall in a region based on various cues is a ______ problem."
    },
    "references": [
      {
        "output": {
          "text": "Supervised learning"
        },
        "tags": [
          "correct"
        ]
      },
      {
        "output": {
          "text": "Unsupervised learning"
        },
        "tags": []
      },
      {
        "output": {
          "text": "Clustering"
        },
        "tags": []
      },
      {
        "output": {
          "text": "None of the above"
        },
        "tags": []
      }
    ],
    "split": "test",
    "id": "id50"
  },
  {
    "input": {
      "text": "_ refers to a model that can neither model the training data nor generalize to new data."
    },
    "references": [
      {
        "output": {
          "text": "good fitting"
        },
        "tags": []
      },
      {
        "output": {
          "text": "overfitting"
        },
        "tags": []
      },
      {
        "output": {
          "text": "underfitting"
        },
        "tags": [
          "correct"
        ]
      },
      {
        "output": {
          "text": "all of the above"
        },
        "tags": []
      }
    ],
    "split": "test",
    "id": "id27"
  },
  {
    "input": {
      "text": "Compared to the variance of the Maximum Likelihood Estimate (MLE), the variance of the Maximum A Posteriori (MAP) estimate is ________"
    },
    "references": [
      {
        "output": {
          "text": "higher"
        },
        "tags": []
      },
      {
        "output": {
          "text": "same"
        },
        "tags": []
      },
      {
        "output": {
          "text": "lower"
        },
        "tags": [
          "correct"
        ]
      },
      {
        "output": {
          "text": "it could be any of the above"
        },
        "tags": []
      }
    ],
    "split": "valid",
    "id": "id12"
  },
  {
    "input": {
      "text": "If N is the number of instances in the training dataset, nearest neighbors has a classification run time of"
    },
    "references": [
      {
        "output": {
          "text": "O(1)"
        },
        "tags": []
      },
      {
        "output": {
          "text": "O( N )"
        },
        "tags": [
          "correct"
        ]
      },
      {
        "output": {
          "text": "O(log N )"
        },
        "tags": []
      },
      {
        "output": {
          "text": "O( N^2 )"
        },
        "tags": []
      }
    ],
    "split": "test",
    "id": "id66"
  },
  {
    "input": {
      "text": "Statement 1| Besides EM, gradient descent can be used to perform inference or learning on Gaussian mixture model. Statement 2 | Assuming a fixed number of attributes, a Gaussian-based Bayes optimal classifier can be learned in time linear in the number of records in the dataset."
    },
    "references": [
      {
        "output": {
          "text": "True, True"
        },
        "tags": [
          "correct"
        ]
      },
      {
        "output": {
          "text": "False, False"
        },
        "tags": []
      },
      {
        "output": {
          "text": "True, False"
        },
        "tags": []
      },
      {
        "output": {
          "text": "False, True"
        },
        "tags": []
      }
    ],
    "split": "test",
    "id": "id96"
  },
  {
    "input": {
      "text": "Which PyTorch 1.8 command(s) produce $10\\times 5$ Gaussian matrix with each entry i.i.d. sampled from $\\mathcal{N}(\\mu=5,\\sigma^2=16)$ and a $10\\times 10$ uniform matrix with each entry i.i.d. sampled from $U[-1,1)$?"
    },
    "references": [
      {
        "output": {
          "text": "\\texttt{5 + torch.randn(10,5) * 16} ; \\texttt{torch.rand(10,10,low=-1,high=1)}"
        },
        "tags": []
      },
      {
        "output": {
          "text": "\\texttt{5 + torch.randn(10,5) * 16} ; \\texttt{(torch.rand(10,10) - 0.5) / 0.5}"
        },
        "tags": []
      },
      {
        "output": {
          "text": "\\texttt{5 + torch.randn(10,5) * 4} ; \\texttt{2 * torch.rand(10,10) - 1}"
        },
        "tags": [
          "correct"
        ]
      },
      {
        "output": {
          "text": "\\texttt{torch.normal(torch.ones(10,5)*5,torch.ones(5,5)*16)} ; \\texttt{2 * torch.rand(10,10) - 1}"
        },
        "tags": []
      }
    ],
    "split": "test",
    "id": "id119"
  },
  {
    "input": {
      "text": "After applying a regularization penalty in linear regression, you find that some of the coefficients of w are zeroed out. Which of the following penalties might have been used?"
    },
    "references": [
      {
        "output": {
          "text": "L0 norm"
        },
        "tags": []
      },
      {
        "output": {
          "text": "L1 norm"
        },
        "tags": []
      },
      {
        "output": {
          "text": "L2 norm"
        },
        "tags": []
      },
      {
        "output": {
          "text": "either (a) or (b)"
        },
        "tags": [
          "correct"
        ]
      }
    ],
    "split": "test",
    "id": "id115"
  },
  {
    "input": {
      "text": "What is the rank of the following matrix? A = [[1, 1, 1], [1, 1, 1], [1, 1, 1]]"
    },
    "references": [
      {
        "output": {
          "text": "0"
        },
        "tags": []
      },
      {
        "output": {
          "text": "1"
        },
        "tags": [
          "correct"
        ]
      },
      {
        "output": {
          "text": "2"
        },
        "tags": []
      },
      {
        "output": {
          "text": "3"
        },
        "tags": []
      }
    ],
    "split": "test",
    "id": "id38"
  },
  {
    "input": {
      "text": "Which of the following are the spatial clustering algorithms?"
    },
    "references": [
      {
        "output": {
          "text": "Partitioning based clustering"
        },
        "tags": []
      },
      {
        "output": {
          "text": "K-means clustering"
        },
        "tags": []
      },
      {
        "output": {
          "text": "Grid based clustering"
        },
        "tags": []
      },
      {
        "output": {
          "text": "All of the above"
        },
        "tags": [
          "correct"
        ]
      }
    ],
    "split": "test",
    "id": "id71"
  },
  {
    "input": {
      "text": "Statement 1| The L2 penalty in a ridge regression is equivalent to a Laplace prior on the weights. Statement 2| There is at least one set of 4 points in R^3 that can be shattered by the hypothesis set of all 2D planes in R^3."
    },
    "references": [
      {
        "output": {
          "text": "True, True"
        },
        "tags": []
      },
      {
        "output": {
          "text": "False, False"
        },
        "tags": []
      },
      {
        "output": {
          "text": "True, False"
        },
        "tags": []
      },
      {
        "output": {
          "text": "False, True"
        },
        "tags": [
          "correct"
        ]
      }
    ],
    "split": "valid",
    "id": "id7"
  },
  {
    "input": {
      "text": "Statement 1| Overfitting is more likely when the set of training data is small. Statement 2| Overfitting is more likely when the hypothesis space is small."
    },
    "references": [
      {
        "output": {
          "text": "True, True"
        },
        "tags": []
      },
      {
        "output": {
          "text": "False, False"
        },
        "tags": []
      },
      {
        "output": {
          "text": "True, False"
        },
        "tags": []
      },
      {
        "output": {
          "text": "False, True"
        },
        "tags": [
          "correct"
        ]
      }
    ],
    "split": "test",
    "id": "id95"
  },
  {
    "input": {
      "text": "For a Gaussian Bayes classifier, which one of these structural assumptions is the one that most affects the trade-off between underfitting and overfitting:"
    },
    "references": [
      {
        "output": {
          "text": "Whether we learn the class centers by Maximum Likelihood or Gradient Descent"
        },
        "tags": []
      },
      {
        "output": {
          "text": "Whether we assume full class covariance matrices or diagonal class covariance matrices"
        },
        "tags": [
          "correct"
        ]
      },
      {
        "output": {
          "text": "Whether we have equal class priors or priors estimated from the data."
        },
        "tags": []
      },
      {
        "output": {
          "text": "Whether we allow classes to have different mean vectors or we force them to share the same mean vector"
        },
        "tags": []
      }
    ],
    "split": "test",
    "id": "id94"
  },
  {
    "input": {
      "text": "Statement 1| The back-propagation algorithm learns a globally optimal neural network with hidden layers. Statement 2| The VC dimension of a line should be at most 2, since I can find at least one case of 3 points that cannot be shattered by any line."
    },
    "references": [
      {
        "output": {
          "text": "True, True"
        },
        "tags": []
      },
      {
        "output": {
          "text": "False, False"
        },
        "tags": [
          "correct"
        ]
      },
      {
        "output": {
          "text": "True, False"
        },
        "tags": []
      },
      {
        "output": {
          "text": "False, True"
        },
        "tags": []
      }
    ],
    "split": "test",
    "id": "id29"
  },
  {
    "input": {
      "text": "Statement 1| The derivative of the sigmoid $\\sigma(x)=(1+e^{-x})^{-1}$ with respect to $x$ is equal to $\\text{Var}(B)$ where $B\\sim \\text{Bern}(\\sigma(x))$ is a Bernoulli random variable. Statement 2| Setting the bias parameters in each layer of neural network to 0 changes the bias-variance trade-off such that the model's variance increases and the model's bias decreases"
    },
    "references": [
      {
        "output": {
          "text": "True, True"
        },
        "tags": []
      },
      {
        "output": {
          "text": "False, False"
        },
        "tags": []
      },
      {
        "output": {
          "text": "True, False"
        },
        "tags": [
          "correct"
        ]
      },
      {
        "output": {
          "text": "False, True"
        },
        "tags": []
      }
    ],
    "split": "test",
    "id": "id127"
  },
  {
    "input": {
      "text": "Statement 1| As of 2020, some models attain greater than 98% accuracy on CIFAR-10. Statement 2| The original ResNets were not optimized with the Adam optimizer."
    },
    "references": [
      {
        "output": {
          "text": "True, True"
        },
        "tags": [
          "correct"
        ]
      },
      {
        "output": {
          "text": "False, False"
        },
        "tags": []
      },
      {
        "output": {
          "text": "True, False"
        },
        "tags": []
      },
      {
        "output": {
          "text": "False, True"
        },
        "tags": []
      }
    ],
    "split": "test",
    "id": "id35"
  }
]